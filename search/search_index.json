{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"model-diagnostics","text":"CI/CD Docs Package Meta"},{"location":"#tools-for-diagnostics-and-assessment-of-machine-learning-models","title":"Tools for diagnostics and assessment of (machine learning) models","text":"<p>Highlights:</p> <ul> <li>All common point predictions covered: mean, median, quantiles, expectiles.</li> <li>Assess model calibration with identification functions (generalized residuals), compute_bias and compute_marginal.</li> <li>Assess calibration and bias graphically<ul> <li>reliability diagrams for auto-calibration</li> <li>bias plots for conditional calibration</li> <li>marginal plots for average <code>y_obs</code>, <code>y_pred</code> and partial dependence for one feature</li> </ul> </li> <li>Assess the predictive performance of models<ul> <li>strictly consistent, homogeneous scoring functions</li> <li>score decomposition into miscalibration, discrimination and uncertainty</li> </ul> </li> <li>Choose your plot backend, either matplotlib or plotly, e.g., via set_config.</li> </ul> <p> To our knowledge, this is the first python package to offer reliability diagrams for quantiles and expectiles and a score decomposition, both made available by an internal implementation of isotonic quantile/expectile regression. </p> <p>This package relies on the giant shoulders of, among others, polars, matplotlib, scipy and scikit-learn.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install model-diagnostics\n</code></pre>"},{"location":"#contributions","title":"Contributions","text":"<p>Contributions are warmly welcome! When contributing, you agree that your contributions will be subject to the MIT License.</p>"},{"location":"development/","title":"Development","text":"<p>Applied principles are:</p> <ul> <li>Trunk-based development </li> <li>Test driven development (TDD) (in spirit)</li> <li>Open source, community contributions are warmly welcome</li> <li>Continuous integration (CI) automation</li> </ul>"},{"location":"development/#tools","title":"Tools","text":"<p>We use hatch as project management tool. Environments and scripts are defined in the file <code>pyproject.toml</code>.</p> <p>Apart from the default environment are <code>docs</code>, <code>jupyter</code>, <code>lint</code> and <code>test</code> (for a test matrix). Some useful commands are:</p> <ul> <li><code>hatch run cov</code> or <code>hatch run pytest</code> to run tests</li> <li><code>hatch -e test run pytest</code> to run test matrix for multiple versions of python and packages</li> <li><code>hatch run format:check</code> to check formatting, linting, typing</li> <li><code>hatch run format:fix</code> to apply <code>ruff format</code> and <code>ruff check --fix</code></li> <li><code>hatch run docs:serve</code> to run a local server under localhost:8000    for the website supporting live updates, i.e. you can change the docs and see    updates immediately. Press Control+C to stop.</li> <li><code>hatch run jupyter:lab</code> to start a jupyter lab server</li> </ul> <p>Some more commands concering testing are: - <code>hatch run test:versions</code> prints package versions per test matrix - <code>hatch run cov-erase</code> remove coverage report, i.e. \"clean cache\" - <code>hatch run test:cov --cov-report=xml</code> run test matrix and generate coverage report</p>"},{"location":"development/#hatch-config","title":"Hatch config","text":"<p>If you work with, e.g., VSCode and want to detect the python executable, one way to do it is the following:</p> <ul> <li>Edit the <code>config.toml</code> of hatch. Look at https://hatch.pypa.io/latest/config/hatch/   for its platform dependent path.</li> <li>Add the entry   <pre><code>[dirs.env]\nvirtual = \".hatch\"\n</code></pre></li> </ul> <p>This way, hatch will install the virtual environments specified in <code>pyprojects.toml</code> (look out for <code>tool.hatch.envs.</code>) under a <code>.hatch</code> subdirectory in the project (for us in the <code>model-diagnostics</code> git repository folder). Now, it is a bit easier to select a path for the python interpreter or jupyter kernel, e.g. <code>.hatch/jupyter/bin/python</code>.</p>"},{"location":"development/#release-process","title":"Release Process","text":"<p>See https://packaging.python.org/en/latest/tutorials/packaging-projects/, https://hatch.pypa.io/latest/build/ and https://hatch.pypa.io/latest/publish/. This will require credentials for https://test.pypi.org and https://pypi.org.</p> <ul> <li>Create a new pull request (PR) and set the new <code>version</code> in <code>__about__.py</code>.</li> <li>Merge the PR on github and sync locally</li> <li><code>git checkout main</code></li> <li><code>git pull origin main</code> (<code>origin</code> might be <code>upstream</code>)</li> <li>Create annotated git tag on branch main for the current commit (not needed for release candidates):   <code>git tag -a v1.4.0 -m \"Release v1.4.0\"</code></li> <li>Push the annotated tag: <code>git push origin &lt;tag_name&gt;</code>.   Note that <code>origin</code> might be to be replaced by <code>upstream</code> depending on your setup.</li> <li>Run <code>hatch build</code>.</li> <li>Create a github release with the new tag and upload the build artifacts.   This triggers to build and deploy the website via github pages.   (The github action <code>deploy_docs</code> can also be triggered manually for convenience.)</li> <li>Publish build artifacts on test.pypi: <code>hatch publish -r test</code></li> <li>Verify the release on test.pypi (best done in some virtual environment):   <code>python3 -m pip install --index-url https://test.pypi.org/simple/ --no-deps model-diagnostics</code></li> <li>Publish build artifacts on pypi (the real thing!): <code>hatch publish</code></li> </ul>"},{"location":"gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the code reference pages and navigation.\"\"\"\n</pre> \"\"\"Generate the code reference pages and navigation.\"\"\" In\u00a0[\u00a0]: Copied! <pre>import logging\nfrom pathlib import Path\n</pre> import logging from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import griffe\nimport mkdocs_gen_files\n</pre> import griffe import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>logger = logging.getLogger(\"gen_ref_pages\")\nlogging.basicConfig(level=logging.INFO)\nnav = mkdocs_gen_files.Nav()\n</pre> logger = logging.getLogger(\"gen_ref_pages\") logging.basicConfig(level=logging.INFO) nav = mkdocs_gen_files.Nav() In\u00a0[\u00a0]: Copied! <pre>root = Path(__file__).parent.parent\nsrc = root / \"src\"\n</pre> root = Path(__file__).parent.parent src = root / \"src\" In\u00a0[\u00a0]: Copied! <pre>log_msg = [\"\\n\\tAPI tReference:\"]\nfor path in sorted(\n    set(src.rglob(\"*.py\"))\n    - set(src.rglob(\"*tests/*.py\"))\n    # - set(src.rglob(\"*/_[!_]*.py\"))  # No single underscore file\n    - set(src.rglob(\"*/_[!_]*/*.py\"))  # No single underscore module\n    - set(src.rglob(\"__about__.py\"))\n):\n    module_path = path.relative_to(src).with_suffix(\"\")\n    doc_path = path.relative_to(src).with_suffix(\".md\")\n    full_doc_path = Path(\"reference\", doc_path)\n\n    parts = tuple(module_path.parts)\n\n    if parts[-1] == \"__init__\":\n        parts = parts[:-1]\n        doc_path = doc_path.with_name(\"index.md\")\n        full_doc_path = full_doc_path.with_name(\"index.md\")\n    elif parts[-1] == \"_config\" and parts[-2] == \"model_diagnostics\":\n        # Place _config.py under top level model_diagnostics.\n        doc_path = doc_path.with_name(\"index.md\")\n        full_doc_path = full_doc_path.with_name(\"index.md\")\n        g = griffe.load(\".\".join(parts))\n        with mkdocs_gen_files.open(full_doc_path, \"a\") as fd:\n            for f in g.functions:\n                ident = \".\".join(parts[:-1] + (f,))\n                log_msg += [f\"Add to {full_doc_path}\", f\"::: {ident}\"]\n                fd.write(f\"\\n::: {ident}\")\n        continue\n    elif parts[-1] == \"__main__\":\n        continue\n\n    nav[parts] = doc_path.as_posix()\n\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        ident = \".\".join(parts)\n        log_msg += [f\"Add to {full_doc_path}\", f\"::: {ident}\"]\n        fd.write(f\"::: {ident}\")\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path.relative_to(root))\n</pre> log_msg = [\"\\n\\tAPI tReference:\"] for path in sorted(     set(src.rglob(\"*.py\"))     - set(src.rglob(\"*tests/*.py\"))     # - set(src.rglob(\"*/_[!_]*.py\"))  # No single underscore file     - set(src.rglob(\"*/_[!_]*/*.py\"))  # No single underscore module     - set(src.rglob(\"__about__.py\")) ):     module_path = path.relative_to(src).with_suffix(\"\")     doc_path = path.relative_to(src).with_suffix(\".md\")     full_doc_path = Path(\"reference\", doc_path)      parts = tuple(module_path.parts)      if parts[-1] == \"__init__\":         parts = parts[:-1]         doc_path = doc_path.with_name(\"index.md\")         full_doc_path = full_doc_path.with_name(\"index.md\")     elif parts[-1] == \"_config\" and parts[-2] == \"model_diagnostics\":         # Place _config.py under top level model_diagnostics.         doc_path = doc_path.with_name(\"index.md\")         full_doc_path = full_doc_path.with_name(\"index.md\")         g = griffe.load(\".\".join(parts))         with mkdocs_gen_files.open(full_doc_path, \"a\") as fd:             for f in g.functions:                 ident = \".\".join(parts[:-1] + (f,))                 log_msg += [f\"Add to {full_doc_path}\", f\"::: {ident}\"]                 fd.write(f\"\\n::: {ident}\")         continue     elif parts[-1] == \"__main__\":         continue      nav[parts] = doc_path.as_posix()      with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:         ident = \".\".join(parts)         log_msg += [f\"Add to {full_doc_path}\", f\"::: {ident}\"]         fd.write(f\"::: {ident}\")      mkdocs_gen_files.set_edit_path(full_doc_path, path.relative_to(root)) In\u00a0[\u00a0]: Copied! <pre>logger.info(\"\\n\\t\".join(log_msg))\n</pre> logger.info(\"\\n\\t\".join(log_msg)) In\u00a0[\u00a0]: Copied! <pre>log_msg = [\"\\n\\tNavigation:\"]\nwith mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:\n    log_msg += list(nav.build_literate_nav())\n    nav_file.writelines(nav.build_literate_nav())\n</pre> log_msg = [\"\\n\\tNavigation:\"] with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:     log_msg += list(nav.build_literate_nav())     nav_file.writelines(nav.build_literate_nav()) In\u00a0[\u00a0]: Copied! <pre>logger.info(\"\\n\\t\".join(log_msg))\n</pre> logger.info(\"\\n\\t\".join(log_msg))"},{"location":"examples/classification/","title":"Classification on Nursery Dataset","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n</pre> import numpy as np import pandas as pd from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split In\u00a0[2]: Copied! <pre>X, y = fetch_openml(data_id=43938, as_frame=True, return_X_y=True, parser=\"auto\")\ndf_original = pd.concat([X, y], axis=1)\n</pre> X, y = fetch_openml(data_id=43938, as_frame=True, return_X_y=True, parser=\"auto\") df_original = pd.concat([X, y], axis=1) In\u00a0[3]: Copied! <pre>df_original[\"class\"].value_counts()\n</pre> df_original[\"class\"].value_counts() Out[3]: <pre>class\nnot_recom     4320\npriority      4266\nspec_prior    4044\nvery_recom     328\nrecommend        2\nName: count, dtype: int64</pre> In\u00a0[4]: Copied! <pre>df = df_original.copy()\ndf[\"binary_response\"] = df[\"class\"].isin([\"priority\", \"spec_prior\"])\ny_var = \"binary_response\"\nx_vars = X.columns\nprint(f\"The prepared dataset contains {df.shape[0]} rows.\")\n</pre> df = df_original.copy() df[\"binary_response\"] = df[\"class\"].isin([\"priority\", \"spec_prior\"]) y_var = \"binary_response\" x_vars = X.columns print(f\"The prepared dataset contains {df.shape[0]} rows.\") <pre>The prepared dataset contains 12960 rows.\n</pre> In\u00a0[5]: Copied! <pre>df.head()\n</pre> df.head() Out[5]: parents has_nurs form children housing finance social health class binary_response 0 usual proper complete 1 convenient convenient nonprob recommended recommend False 1 usual proper complete 1 convenient convenient nonprob priority priority True 2 usual proper complete 1 convenient convenient nonprob not_recom not_recom False 3 usual proper complete 1 convenient convenient slightly_prob recommended recommend False 4 usual proper complete 1 convenient convenient slightly_prob priority priority True <p>4320 + 4320 = 8640 observations can be perfectly predicted using the feature health, as we can see in the contingency table below:</p> In\u00a0[6]: Copied! <pre>contingency_table = pd.crosstab(df[\"health\"], df['binary_response'])    \nprint(\"Contingency Table:\")\ncontingency_table\n</pre> contingency_table = pd.crosstab(df[\"health\"], df['binary_response'])     print(\"Contingency Table:\") contingency_table <pre>Contingency Table:\n</pre> Out[6]: binary_response False True health not_recom 4320 0 priority 0 4320 recommended 330 3990 <p>By further looking into the other features of the observations where $health=\\text{recommended}$ we can see if one can completely predict the response variable using the other covariates as well.</p> <p>We automate this procedure in section 2.3 by fitting a decision tree.</p> In\u00a0[7]: Copied! <pre>df_train, df_test = train_test_split(df, train_size=0.75, random_state=1234321)\ndf = (\n    pd.concat((df_train, df_test), axis=0, keys=(\"train\", \"test\"))\n    .reset_index(level=0)\n    .rename(columns={\"level_0\": \"split\"})\n)\n\ny_train, y_test = df_train[y_var].astype(float), df_test[y_var].astype(float)\nX_train, X_test = df_train[x_vars], df_test[x_vars]\n</pre> df_train, df_test = train_test_split(df, train_size=0.75, random_state=1234321) df = (     pd.concat((df_train, df_test), axis=0, keys=(\"train\", \"test\"))     .reset_index(level=0)     .rename(columns={\"level_0\": \"split\"}) )  y_train, y_test = df_train[y_var].astype(float), df_test[y_var].astype(float) X_train, X_test = df_train[x_vars], df_test[x_vars] <p>We check whether the split results in two approximately identically distributed samples.</p> In\u00a0[8]: Copied! <pre>df.groupby(\"split\")[[\"binary_response\"]].mean()\n</pre> df.groupby(\"split\")[[\"binary_response\"]].mean() Out[8]: binary_response split test 0.636420 train 0.642798 In\u00a0[9]: Copied! <pre>from sklearn.dummy import DummyClassifier\n\n\nm_trivial = DummyClassifier(strategy=\"prior\").fit(X_train, y_train)\n</pre> from sklearn.dummy import DummyClassifier   m_trivial = DummyClassifier(strategy=\"prior\").fit(X_train, y_train) In\u00a0[10]: Copied! <pre>from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n\n# All variables are discrete.\nx_discrete = x_vars\n# ColumnTransformer for linear models\ncol_trans_cat = ColumnTransformer(\n    [\n        (\n            \"cat_features\",\n            OneHotEncoder(drop=\"first\", sparse_output=False),\n            x_discrete,\n        ),\n    ]\n)\nm_logistic = Pipeline(\n    [\n        (\"column_transformer\", col_trans_cat),\n        (\n            \"model\",\n            LogisticRegression(\n                penalty=None,\n                solver=\"newton-cholesky\",\n            )\n        ),\n    ]\n).fit(X_train, y_train)\n</pre> from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression   # All variables are discrete. x_discrete = x_vars # ColumnTransformer for linear models col_trans_cat = ColumnTransformer(     [         (             \"cat_features\",             OneHotEncoder(drop=\"first\", sparse_output=False),             x_discrete,         ),     ] ) m_logistic = Pipeline(     [         (\"column_transformer\", col_trans_cat),         (             \"model\",             LogisticRegression(                 penalty=None,                 solver=\"newton-cholesky\",             )         ),     ] ).fit(X_train, y_train) <p>We can have a look at the model coefficients.</p> In\u00a0[11]: Copied! <pre>feature_names = m_logistic[:-1].get_feature_names_out()\nmodel_coefs = m_logistic.steps[-1][1].coef_.squeeze()\nmodel_coefs = pd.DataFrame(\n    {\n        \"coefficient\": [x.removeprefix(\"cat_features__\") for x in feature_names],\n         \"value\": model_coefs,\n    }\n).sort_values(by=\"value\")\n\nwith pd.option_context(\"display.max_rows\", 20):\n    display(model_coefs)\n</pre> feature_names = m_logistic[:-1].get_feature_names_out() model_coefs = m_logistic.steps[-1][1].coef_.squeeze() model_coefs = pd.DataFrame(     {         \"coefficient\": [x.removeprefix(\"cat_features__\") for x in feature_names],          \"value\": model_coefs,     } ).sort_values(by=\"value\")  with pd.option_context(\"display.max_rows\", 20):     display(model_coefs) coefficient value 3 has_nurs_less_proper -11.857906 4 has_nurs_proper -11.849024 1 parents_usual -11.532055 0 parents_pretentious -10.116730 2 has_nurs_improper -9.870463 5 has_nurs_very_crit -0.097957 16 social_slightly_prob 0.000125 6 form_completed 0.829653 8 form_incomplete 1.767603 9 children_2 1.860888 14 finance_inconv 2.152042 13 housing_less_conv 2.414219 7 form_foster 3.181892 10 children_3 3.955277 11 children_more 4.120487 12 housing_critical 5.707804 15 social_problematic 11.130701 18 health_recommended 47.231852 17 health_priority 59.699965 In\u00a0[12]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier, plot_tree\n\n\n# We can reuse the categorical one-hot encoding of the linear model.\nm_tree = Pipeline(\n    [\n        (\"column_transformer\", col_trans_cat),\n        (\n            \"model\", DecisionTreeClassifier(\n                criterion=\"log_loss\", min_samples_leaf=2,\n            )\n        ),\n    ]\n).fit(X_train, y_train)\n\nplot_tree(\n    m_tree.steps[-1][1],\n    feature_names=list(m_tree[:-1].get_feature_names_out()),\n    max_depth=3,\n);\n</pre> from sklearn.tree import DecisionTreeClassifier, plot_tree   # We can reuse the categorical one-hot encoding of the linear model. m_tree = Pipeline(     [         (\"column_transformer\", col_trans_cat),         (             \"model\", DecisionTreeClassifier(                 criterion=\"log_loss\", min_samples_leaf=2,             )         ),     ] ).fit(X_train, y_train)  plot_tree(     m_tree.steps[-1][1],     feature_names=list(m_tree[:-1].get_feature_names_out()),     max_depth=3, ); <p>The reading of the tree structure is difficult due to the one-hot encoding. For instance, <code>cat_features__health_priority &lt;= 0.5</code> means <code>health == priority</code> goes to the right, else to the left. We can see that the first two splits have been performed as we previously did \"by hand\" on the feature health.</p> <p>We restricted the tree plot to a depth of 3 for ease of readability. The tree has in fact a depth of 17.</p> In\u00a0[13]: Copied! <pre>m_tree.steps[-1][1].get_depth()\n</pre> m_tree.steps[-1][1].get_depth() Out[13]: <pre>17</pre> <p>As the fitted decision tree has a large depth, we check how many observations are in each leaf.</p> In\u00a0[14]: Copied! <pre># Get the leaf node assignments for each sample\nleaf_ids = m_tree.steps[-1][1].apply(col_trans_cat.fit_transform(X_train))\n\n# Count the samples in each leaf node\nunique_leaf_ids = np.unique(leaf_ids)\nleaf_samples = [np.sum(leaf_ids == node_id) for node_id in unique_leaf_ids]\n\nnp.sort(leaf_samples)[:10]\n</pre> # Get the leaf node assignments for each sample leaf_ids = m_tree.steps[-1][1].apply(col_trans_cat.fit_transform(X_train))  # Count the samples in each leaf node unique_leaf_ids = np.unique(leaf_ids) leaf_samples = [np.sum(leaf_ids == node_id) for node_id in unique_leaf_ids]  np.sort(leaf_samples)[:10] Out[14]: <pre>array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</pre> <p>Some leaf nodes contain only 2 observations, the minimum we set (which is quite low).</p> In\u00a0[15]: Copied! <pre>%%time\n# Note that this cell might take a few seconds.\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\n# ColumnTransformer for boosted trees\ncol_trans_bt = ColumnTransformer(\n    [\n        (\"categorical_features\", OrdinalEncoder(), x_discrete),\n    ]\n)\n\nm_hgbt_base = Pipeline(\n    [\n        (\"column_transformer\", col_trans_bt),\n        (\n            \"model\", HistGradientBoostingClassifier(\n                loss=\"log_loss\",\n                categorical_features=list(range(len(x_discrete))),\n                max_iter=100,\n                early_stopping=True,\n                random_state=33,\n                validation_fraction=0.2,\n            )\n        ),\n    ]\n)\n\nparam_grid = {\n    \"model__learning_rate\": [0.6, 0.7, 0.8],  # quite large\n    \"model__l2_regularization\": [0, 1e-6, 1e-3],\n    \"model__max_depth\": [10, 15, 20],\n    \"model__min_samples_leaf\": [2, 3, 4],\n}\n\n\n# successive halfing grid search (CV) on the training data\nshgs = HalvingGridSearchCV(\n    m_hgbt_base,\n    param_grid=param_grid,\n    cv=2,\n    scoring=\"neg_log_loss\",\n    random_state=321,\n).fit(X_train, y_train)\n\nm_hgbt = shgs.best_estimator_\n</pre> %%time # Note that this cell might take a few seconds. from sklearn.experimental import enable_halving_search_cv from sklearn.model_selection import HalvingGridSearchCV from sklearn.ensemble import HistGradientBoostingClassifier from sklearn.preprocessing import OrdinalEncoder   # ColumnTransformer for boosted trees col_trans_bt = ColumnTransformer(     [         (\"categorical_features\", OrdinalEncoder(), x_discrete),     ] )  m_hgbt_base = Pipeline(     [         (\"column_transformer\", col_trans_bt),         (             \"model\", HistGradientBoostingClassifier(                 loss=\"log_loss\",                 categorical_features=list(range(len(x_discrete))),                 max_iter=100,                 early_stopping=True,                 random_state=33,                 validation_fraction=0.2,             )         ),     ] )  param_grid = {     \"model__learning_rate\": [0.6, 0.7, 0.8],  # quite large     \"model__l2_regularization\": [0, 1e-6, 1e-3],     \"model__max_depth\": [10, 15, 20],     \"model__min_samples_leaf\": [2, 3, 4], }   # successive halfing grid search (CV) on the training data shgs = HalvingGridSearchCV(     m_hgbt_base,     param_grid=param_grid,     cv=2,     scoring=\"neg_log_loss\",     random_state=321, ).fit(X_train, y_train)  m_hgbt = shgs.best_estimator_ <pre>CPU times: user 33.2 s, sys: 569 ms, total: 33.8 s\nWall time: 8.81 s\n</pre> In\u00a0[16]: Copied! <pre>shgs.best_params_, shgs.best_score_\n</pre> shgs.best_params_, shgs.best_score_ Out[16]: <pre>({'model__l2_regularization': 1e-06,\n  'model__learning_rate': 0.7,\n  'model__max_depth': 15,\n  'model__min_samples_leaf': 2},\n -6.40295807008405e-05)</pre> In\u00a0[17]: Copied! <pre>m_hgbt[-1].n_iter_\n</pre> m_hgbt[-1].n_iter_ Out[17]: <pre>27</pre> In\u00a0[18]: Copied! <pre>models = [m_trivial, m_logistic, m_tree, m_hgbt]\nmodel_names = [\"Trivial\", \"Logistic\", \"Tree\", \"HGBT\"]\n\ndf_pred_test = pd.DataFrame(\n    {name: m.predict_proba(X_test)[:, 1] for m, name in zip(models, model_names)}\n)\n</pre> models = [m_trivial, m_logistic, m_tree, m_hgbt] model_names = [\"Trivial\", \"Logistic\", \"Tree\", \"HGBT\"]  df_pred_test = pd.DataFrame(     {name: m.predict_proba(X_test)[:, 1] for m, name in zip(models, model_names)} ) In\u00a0[19]: Copied! <pre>import matplotlib.pyplot as plt\nfrom model_diagnostics.calibration import compute_bias, plot_bias, plot_reliability_diagram\n</pre> import matplotlib.pyplot as plt from model_diagnostics.calibration import compute_bias, plot_bias, plot_reliability_diagram In\u00a0[20]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 8))\nplot_reliability_diagram(\n    y_obs=y_test,\n    y_pred=df_pred_test[[\"Logistic\", \"Tree\", \"HGBT\"]],\n    n_bootstrap=100,\n    ax=ax,\n);\n</pre> fig, ax = plt.subplots(figsize=(8, 8)) plot_reliability_diagram(     y_obs=y_test,     y_pred=df_pred_test[[\"Logistic\", \"Tree\", \"HGBT\"]],     n_bootstrap=100,     ax=ax, ); <p>The HGBT model is very close to be auto-calibrated. A comment regarding the strong performance of the HGBT model is in section 4.1.</p> In\u00a0[21]: Copied! <pre>compute_bias(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    feature=None,\n)\n</pre> compute_bias(     y_obs=y_test,     y_pred=df_pred_test,     feature=None, ) Out[21]: shape: (4, 6)modelbias_meanbias_countbias_weightsbias_stderrp_valuestrf64u32f64f64f64\"Trivial\"0.00637932403240.00.0084520.4505\"Logistic\"-0.00137432403240.00.0014930.357317\"Tree\"0.00077232403240.00.0008780.379312\"HGBT\"0.00000832403240.00.0000070.298421 In\u00a0[22]: Copied! <pre>compute_bias(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    feature=X_test[\"parents\"],\n)\n</pre> compute_bias(     y_obs=y_test,     y_pred=df_pred_test,     feature=X_test[\"parents\"], ) Out[22]: shape: (12, 7)modelparentsbias_meanbias_countbias_weightsbias_stderrp_valuestrcatf64u32f64f64f64\"Trivial\"\"great_pret\"-0.00937611041104.00.0143410.513399\"Trivial\"\"pretentious\"-0.0018310891089.00.014510.899677\"Trivial\"\"usual\"0.03152810471047.00.0150720.036696\"Logistic\"\"great_pret\"-0.00000611041104.00.0000170.726384\"Logistic\"\"pretentious\"0.00013910891089.00.0031140.964296\"Logistic\"\"usual\"-0.00439210471047.00.0032950.1828\"Tree\"\"great_pret\"0.011041104.00.00.0\"Tree\"\"pretentious\"0.00229610891089.00.0024150.342047\"Tree\"\"usual\"-1.0604e-1910471047.00.0010321.0\"HGBT\"\"great_pret\"-7.7474e-811041104.03.6859e-80.035785\"HGBT\"\"pretentious\"7.5675e-710891089.04.5198e-70.094364\"HGBT\"\"usual\"0.00002310471047.00.0000230.312915 In\u00a0[23]: Copied! <pre>fig, ax = plt.subplots(figsize=(8, 8))\nplot_bias(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    feature=X_test[\"parents\"],\n    confidence_level=0.8\n);\n</pre> fig, ax = plt.subplots(figsize=(8, 8)) plot_bias(     y_obs=y_test,     y_pred=df_pred_test,     feature=X_test[\"parents\"],     confidence_level=0.8 ); In\u00a0[24]: Copied! <pre>from model_diagnostics.scoring import LogLoss, decompose\n\n\nlog_loss = LogLoss()\n\ndf = decompose(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    scoring_function=log_loss,\n)\ndf.sort([\"score\"])\n</pre> from model_diagnostics.scoring import LogLoss, decompose   log_loss = LogLoss()  df = decompose(     y_obs=y_test,     y_pred=df_pred_test,     scoring_function=log_loss, ) df.sort([\"score\"]) Out[24]: shape: (4, 5)modelmiscalibrationdiscriminationuncertaintyscorestrf64f64f64f64\"HGBT\"0.0000090.655450.655450.000009\"Logistic\"0.0035090.6359950.655450.022964\"Trivial\"0.0000880.00.655450.655539\"Tree\"inf0.6413660.65545inf <p>First, the HGBT model has the best (smallest) log loss in column score. We also see that the uncertainty term is model independent as it is a property of the data and the scoring function only.</p> <p>The HGBT model is superior compared to the logistic and to the tree model both in terms of the miscalibration (lower) and of the discrimination (higher). HGBT has a score close to zero, so it seems to almost perfectly predict the target variable.</p> <p>The tree model, however, has an infinite score and infinite miscalibration due to few predictions being equal to 1 when $Y=0$ (4 observations, see below) or vice versa (2 observations, see below). This can be seen as a weekness of the log loss: it penalizes false certainty (predicting probabilities of 0 or 1, contradicting to the data) with infinity.</p> <p>The good performance of the tree-based models might suggest that the data was \"generated\" following a decision-tree-like approach, whose logic the models can then learn given that all the required covariates are available.</p> In\u00a0[25]: Copied! <pre>np.sum((df_pred_test[\"Tree\"] == 1) &amp; (y_test.values == 0))\n</pre> np.sum((df_pred_test[\"Tree\"] == 1) &amp; (y_test.values == 0)) Out[25]: <pre>4</pre> In\u00a0[26]: Copied! <pre>np.sum((df_pred_test[\"Tree\"] == 0) &amp; (y_test.values == 1))\n</pre> np.sum((df_pred_test[\"Tree\"] == 0) &amp; (y_test.values == 1)) Out[26]: <pre>1</pre> In\u00a0[27]: Copied! <pre>from model_diagnostics.scoring import SquaredError, plot_murphy_diagram\n\n\nplot_murphy_diagram(\n    y_obs=y_test,\n    y_pred=df_pred_test.loc[:, [\"Logistic\", \"Tree\", \"HGBT\"]],\n    etas=200,\n);\n</pre> from model_diagnostics.scoring import SquaredError, plot_murphy_diagram   plot_murphy_diagram(     y_obs=y_test,     y_pred=df_pred_test.loc[:, [\"Logistic\", \"Tree\", \"HGBT\"]],     etas=200, ); <p>The HGBT model seems to have almost perfect score (0 is the minimal attainable value) over the whole range of etas and therefore dominates all other models.</p> <p>We show this on the score decomposition of the squared error, which is called Brier score in the context of classification.</p> In\u00a0[28]: Copied! <pre>decompose(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    scoring_function=SquaredError(),\n).sort([\"score\"])\n</pre> decompose(     y_obs=y_test,     y_pred=df_pred_test,     scoring_function=SquaredError(), ).sort([\"score\"]) Out[28]: shape: (4, 5)modelmiscalibrationdiscriminationuncertaintyscorestrf64f64f64f64\"HGBT\"1.7994e-70.231390.231391.7994e-7\"Tree\"0.0000140.2289080.231390.002495\"Logistic\"0.0008620.225030.231390.007221\"Trivial\"0.0000410.00.231390.23143"},{"location":"examples/classification/#classification-on-nursery-dataset","title":"Classification on Nursery Dataset\u00b6","text":"<p>This notebook demonstrates model diagnostics on the nursery dataset https://www.openml.org/d/43938, originally https://doi.org/10.24432/C5P88W.</p> <p>The response variable class has five different values: not_recom, recommend, very_recom, priority, spec_prior.</p> <p>Our modelling goal is to estimate the conditional probability of $Y=1$, i.e. $P(Y=1|X)$, where $X$ are the features and $Y$ is defined as $$ Y =  \\begin{cases} 1 \\text{ if class} \\in \\text{\\{priority, special priority\\}}\\\\ 0 \\text{ if otherwise } \\end{cases} $$</p> <p>Note is made that for binary classification $E(Y|X) = P(Y=1|X)$, that is, the conditional expected value equals the conditional probability of $Y$ being equal to one. Therefore, all the theory for the expectation functional applies to binary classification, probabilistic predictions are point predictions and scoring rules coincide with scoring functions.</p> <p>The impatient reader can directly jump to chapters 3-4 where the capability of <code>model-diagnostics</code> are shown. Chapters 1-2 are needed for data preparation and model training.</p>"},{"location":"examples/classification/#1-data","title":"1. Data\u00b6","text":""},{"location":"examples/classification/#11-data-load-and-preprocessing","title":"1.1 Data Load and Preprocessing\u00b6","text":""},{"location":"examples/classification/#12-contingency-tables","title":"1.2 Contingency tables\u00b6","text":""},{"location":"examples/classification/#13-data-split","title":"1.3 Data Split\u00b6","text":""},{"location":"examples/classification/#2-models","title":"2. Models\u00b6","text":"<p>Our modelling goal is to estimate the conditional probability of $Y=1$, i.e. $P(Y=1|X)$, and we will finally evaluate the models with the log loss $$S(y, z) = -y\\log(z) - (1 - y) \\log (1 - z)$$</p>"},{"location":"examples/classification/#21-the-trivial-model","title":"2.1 The trivial model\u00b6","text":""},{"location":"examples/classification/#22-logistic","title":"2.2 Logistic\u00b6","text":"<p>Here, we train a Logistic Regression model.</p>"},{"location":"examples/classification/#23-decision-tree","title":"2.3 Decision Tree\u00b6","text":"<p>We will now train a <code>DecisionTreeClassifier</code> with default parameters, so no hyperparameter optimization.</p>"},{"location":"examples/classification/#24-gradient-boosted-decision-trees","title":"2.4 Gradient Boosted Decision Trees\u00b6","text":"<p>We will now train a <code>HistGradientBoostingClassifier</code>.</p>"},{"location":"examples/classification/#3-calibration-assessment","title":"3. Calibration Assessment\u00b6","text":"<p>To make the code easier, we put together the predictions of all of our models on the test set in a single dataframe.</p>"},{"location":"examples/classification/#31-reliability-diagrams","title":"3.1. Reliability Diagrams\u00b6","text":"<p>A reliability diagram plots an estimation of $E(Y|m(X))$ versus $m(X)$ and therefore assesses auto-calibration. Good auto-calibration is seen by closeness to the diagonal depicted as dotted line.</p> <p>A good way to estimate $E(Y|m(X))$ and thereby avoiding manual binning is by isotonic regression. This is implemented in <code>plot_reliability_diagram</code>.</p>"},{"location":"examples/classification/#32-conditional-bias","title":"3.2 Conditional Bias\u00b6","text":""},{"location":"examples/classification/#unconditional-bias","title":"Unconditional Bias\u00b6","text":""},{"location":"examples/classification/#bias-conditional-on-parents","title":"Bias Conditional on parents\u00b6","text":""},{"location":"examples/classification/#4-predictive-model-performance","title":"4. Predictive Model Performance\u00b6","text":""},{"location":"examples/classification/#41-score-decomposition-of-the-log-loss","title":"4.1 Score Decomposition of the Log Loss\u00b6","text":"<p>We use the log loss and evaluate on the test set. On top, we additively decompose the scores in terms of miscalibration (smaller is better), discrimination (larger is better) and uncertainty (property of the data, same for all models).</p>"},{"location":"examples/classification/#42-murphy-diagram","title":"4.2 Murphy Diagram\u00b6","text":"<p>How does the ranking of the model performances change with other scoring functions? Is one model dominating the others for a wide range of scoring functions? Such questions can be handily answered by a Murphy diagram. It plots the mean score for a wide range of elementary scoring functions, parametrized by <code>eta</code> on the x-axis. We just specify the number of <code>etas</code>, the plot finds out meaningfull min and max values.</p>"},{"location":"examples/quantile_regression/","title":"Quantile Regression on Synthetic Data","text":"In\u00a0[1]: Copied! <pre>from cycler import cycler\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\ncmap = matplotlib.colormaps.get_cmap(\"tab10\")\n# Swap 3rd with 7th position to make 3rd gray.\ncolor_cy = cycler(\"color\", cmap([0, 1, 7, 2, 3, 4, 5, 6, 8, 9]))\n# This can be checked by running\n# from matplotlib.colors import ListedColormap\n# display(cmap)\n# display(ListedColormap(color_cy.by_key()[\"color\"]))\n\nnew_cycler = color_cy + cycler(linestyle=[\"-\"] * 2 + ['--'] + ['-'] * 7)\nplt.rc('axes', prop_cycle=new_cycler)\n</pre> from cycler import cycler import matplotlib import matplotlib.pyplot as plt   cmap = matplotlib.colormaps.get_cmap(\"tab10\") # Swap 3rd with 7th position to make 3rd gray. color_cy = cycler(\"color\", cmap([0, 1, 7, 2, 3, 4, 5, 6, 8, 9])) # This can be checked by running # from matplotlib.colors import ListedColormap # display(cmap) # display(ListedColormap(color_cy.by_key()[\"color\"]))  new_cycler = color_cy + cycler(linestyle=[\"-\"] * 2 + ['--'] + ['-'] * 7) plt.rc('axes', prop_cycle=new_cycler) In\u00a0[2]: Copied! <pre>import numpy as np\nimport polars as pl\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.linear_model import QuantileRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import SplineTransformer\n\n\nquantile_level = 0.75\n\ndef create_data(n, rng):    \n    x = np.linspace(start=0, stop=10, num=200)\n    y_true_mean = 10 + 0.5 * x * np.sin(x)\n    a = 5\n    # Note that numpy uses the Lomax version of pareto with mean 1/(a-1).\n    y = y_true_mean + 10 * (rng.pareto(a, size=x.shape[0]) - 1 / (a - 1))\n    y_true_quantile = y_true_mean + 10 * (\n        np.power(1 - quantile_level, -1/a) - 1 - 1 / (a - 1)\n    )\n    return y, x, y_true_quantile\n\nrng = np.random.RandomState(42)\ny_train, x_train, y_quantile_train = create_data(200, rng)\ny_test, x_test, y_quantile_test = create_data(100, rng)\nX_train, X_test = x_train[:, np.newaxis], x_test[:, np.newaxis]\n\nm_linear = GridSearchCV(\n    estimator=make_pipeline(\n        SplineTransformer(degree=3),\n        QuantileRegressor(quantile=quantile_level, solver=\"highs\"),\n    ),\n    param_grid={\n        \"splinetransformer__n_knots\": [3, 5, 8, 10],\n        \"quantileregressor__alpha\": np.logspace(-4, 0, 20)\n    },\n    cv=5,\n).fit(X_train, y_train)\nprint(\n    \"best (alpha, n_knots) for linear quantile regression = \"\n    f\"({m_linear.best_params_['quantileregressor__alpha']}, \"\n    f\"{m_linear.best_params_['splinetransformer__n_knots']})\"\n)\n\nm_hgbt = HistGradientBoostingRegressor(\n    loss=\"quantile\", quantile=quantile_level\n).fit(X_train, y_train)\n\n\nfig, ax = plt.subplots()\nax.scatter(x_train, y_train, color=\"black\", alpha=0.5)\nax.plot(x_train, m_linear.predict(X_train), label=f\"LinReg Quantile: {quantile_level}\")\nax.plot(x_train, m_hgbt.predict(X_train), label=f\"HGBT Quantile: {quantile_level}\")\nax.plot(\n    x_train, y_quantile_train, label=f\"True {quantile_level}-Quantile\",\n)\nax.legend()\n</pre> import numpy as np import polars as pl from sklearn.ensemble import HistGradientBoostingRegressor from sklearn.linear_model import QuantileRegressor from sklearn.model_selection import GridSearchCV from sklearn.pipeline import make_pipeline from sklearn.preprocessing import SplineTransformer   quantile_level = 0.75  def create_data(n, rng):         x = np.linspace(start=0, stop=10, num=200)     y_true_mean = 10 + 0.5 * x * np.sin(x)     a = 5     # Note that numpy uses the Lomax version of pareto with mean 1/(a-1).     y = y_true_mean + 10 * (rng.pareto(a, size=x.shape[0]) - 1 / (a - 1))     y_true_quantile = y_true_mean + 10 * (         np.power(1 - quantile_level, -1/a) - 1 - 1 / (a - 1)     )     return y, x, y_true_quantile  rng = np.random.RandomState(42) y_train, x_train, y_quantile_train = create_data(200, rng) y_test, x_test, y_quantile_test = create_data(100, rng) X_train, X_test = x_train[:, np.newaxis], x_test[:, np.newaxis]  m_linear = GridSearchCV(     estimator=make_pipeline(         SplineTransformer(degree=3),         QuantileRegressor(quantile=quantile_level, solver=\"highs\"),     ),     param_grid={         \"splinetransformer__n_knots\": [3, 5, 8, 10],         \"quantileregressor__alpha\": np.logspace(-4, 0, 20)     },     cv=5, ).fit(X_train, y_train) print(     \"best (alpha, n_knots) for linear quantile regression = \"     f\"({m_linear.best_params_['quantileregressor__alpha']}, \"     f\"{m_linear.best_params_['splinetransformer__n_knots']})\" )  m_hgbt = HistGradientBoostingRegressor(     loss=\"quantile\", quantile=quantile_level ).fit(X_train, y_train)   fig, ax = plt.subplots() ax.scatter(x_train, y_train, color=\"black\", alpha=0.5) ax.plot(x_train, m_linear.predict(X_train), label=f\"LinReg Quantile: {quantile_level}\") ax.plot(x_train, m_hgbt.predict(X_train), label=f\"HGBT Quantile: {quantile_level}\") ax.plot(     x_train, y_quantile_train, label=f\"True {quantile_level}-Quantile\", ) ax.legend() <pre>best (alpha, n_knots) for linear quantile regression = (0.00026366508987303583, 5)\n</pre> Out[2]: <pre>&lt;matplotlib.legend.Legend at 0x131115a00&gt;</pre> In\u00a0[3]: Copied! <pre>from model_diagnostics.calibration import compute_bias, plot_bias, plot_reliability_diagram\n\ndf = pl.DataFrame({\n    \"linear_quant_reg\": m_linear.predict(X_train),\n    \"hgbt_quant_reg\": m_hgbt.predict(X_train),\n    \"true_quantile\": y_quantile_train,\n})\n\ncompute_bias(y_obs=y_train, y_pred=df, functional=\"quantile\", level=quantile_level)\n</pre> from model_diagnostics.calibration import compute_bias, plot_bias, plot_reliability_diagram  df = pl.DataFrame({     \"linear_quant_reg\": m_linear.predict(X_train),     \"hgbt_quant_reg\": m_hgbt.predict(X_train),     \"true_quantile\": y_quantile_train, })  compute_bias(y_obs=y_train, y_pred=df, functional=\"quantile\", level=quantile_level) Out[3]: shape: (3, 6)modelbias_meanbias_countbias_weightsbias_stderrp_valuestrf64u32f64f64f64\"linear_quant_reg\"0.005200200.00.0304880.869899\"hgbt_quant_reg\"0.0200200.00.0306951.0\"true_quantile\"-0.005200200.00.0308970.871607 In\u00a0[4]: Copied! <pre>ax = plot_bias(\n    y_obs=y_train,\n    y_pred=df,\n    functional=\"quantile\",\n    level=quantile_level,\n    confidence_level=1 - 0.9,\n)\nax.set_title(\"Bias Plot on Training Set\")\n</pre> ax = plot_bias(     y_obs=y_train,     y_pred=df,     functional=\"quantile\",     level=quantile_level,     confidence_level=1 - 0.9, ) ax.set_title(\"Bias Plot on Training Set\") Out[4]: <pre>Text(0.5, 1.0, 'Bias Plot on Training Set')</pre> <p>By setting a very small confidence level $\\alpha$, such that $1-\\alpha$ is in between the p-values, we see that the confidence interval of the linear model does not reach the 0-line while the one of the HGBT model does. Please note that this is a poor choice of a confidence level for this sample size because even the true quantile does not cover the 0-line due to sample uncertainty.</p> <p>We go on by investigating auto-calibration with the classical tool: a reliability diagram.</p> In\u00a0[5]: Copied! <pre>%%time\nax = plot_reliability_diagram(\n    y_train,\n    df,\n    functional=\"quantile\",\n    level=quantile_level,\n    n_bootstrap=100,\n)\nax.set_ylim(None, 20)\nax.set_title(\"Reliability Diagram on Training Set\")\n</pre> %%time ax = plot_reliability_diagram(     y_train,     df,     functional=\"quantile\",     level=quantile_level,     n_bootstrap=100, ) ax.set_ylim(None, 20) ax.set_title(\"Reliability Diagram on Training Set\") <pre>CPU times: user 3.71 s, sys: 162 ms, total: 3.87 s\nWall time: 3.84 s\n</pre> Out[5]: <pre>Text(0.5, 1.0, 'Reliability Diagram on Training Set')</pre> <p>The reliability diagram shows reasonable auto-calibration for both models. For large predicted values, the linear model seems a bit off.</p> <p>We conclude the assessment of calibration by a bias plot conditional on our single feature.</p> In\u00a0[6]: Copied! <pre>ax = plot_bias(y_obs=y_train, y_pred=df, feature=x_train, confidence_level=0.9)\nax.set_title(\"Bias Plot conditional on prediction on Training Set\")\n</pre> ax = plot_bias(y_obs=y_train, y_pred=df, feature=x_train, confidence_level=0.9) ax.set_title(\"Bias Plot conditional on prediction on Training Set\") Out[6]: <pre>Text(0.5, 1.0, 'Bias Plot conditional on prediction on Training Set')</pre> <p>Here again, the HGBT model seems a bit better calibrated.</p> In\u00a0[7]: Copied! <pre>from model_diagnostics.scoring import decompose, plot_murphy_diagram, PinballLoss\n\n\npbl = PinballLoss(level=quantile_level)\ndf = pl.DataFrame({\n    \"linear_quant_reg\": m_linear.predict(X_test),\n    \"hgbt_quant_reg\": m_hgbt.predict(X_test),\n    \"true_quantile\": y_quantile_test,\n})\ndecompose(y_obs=y_test, y_pred=df, scoring_function=pbl).sort(\"score\")\n</pre> from model_diagnostics.scoring import decompose, plot_murphy_diagram, PinballLoss   pbl = PinballLoss(level=quantile_level) df = pl.DataFrame({     \"linear_quant_reg\": m_linear.predict(X_test),     \"hgbt_quant_reg\": m_hgbt.predict(X_test),     \"true_quantile\": y_quantile_test, }) decompose(y_obs=y_test, y_pred=df, scoring_function=pbl).sort(\"score\") Out[7]: shape: (3, 5)modelmiscalibrationdiscriminationuncertaintyscorestrf64f64f64f64\"linear_quant_reg\"0.0657290.240921.0751950.900004\"true_quantile\"0.0824750.2575581.0751950.900112\"hgbt_quant_reg\"0.0560780.2068181.0751950.924455 <p>The linear model has a better (lower) out-of-sample score (or loss). While the HGBT has a better miscalibration term (smaller is better), the dominating term is the discrimination (larger is better) where the linear model is clearly superior.</p> <p>Again, note that there is sample uncertainty. It causes the true quantile to have a slightly worse score than the linear model. A simple t-test will show if the score differences are significant or related to sample uncertainty.</p> In\u00a0[8]: Copied! <pre>from scipy.stats import ttest_1samp\n\n\nttest_1samp(\n    pbl.score_per_obs(y_obs=y_test, y_pred=m_linear.predict(X_test))\n    - pbl.score_per_obs(y_obs=y_test, y_pred=y_quantile_test),\n    popmean=0,\n)\n</pre> from scipy.stats import ttest_1samp   ttest_1samp(     pbl.score_per_obs(y_obs=y_test, y_pred=m_linear.predict(X_test))     - pbl.score_per_obs(y_obs=y_test, y_pred=y_quantile_test),     popmean=0, ) Out[8]: <pre>TtestResult(statistic=-0.005514228985366034, pvalue=0.9956058279989302, df=199)</pre> <p>The test statistic is negative which means that the score of the linear model is better than the one of the true quantile. With the very large p-value, however, we cannot reject the null hypothesis of equal mean values, i.e. equal scores. It means that we do not have statistical evidence to say that any one of the linear and the true model performs better.</p> <p>We play the same game and perform a t-test for the score difference of the linear and the HGBT models.</p> In\u00a0[9]: Copied! <pre>ttest_1samp(\n    pbl.score_per_obs(y_obs=y_test, y_pred=m_linear.predict(X_test))\n    - pbl.score_per_obs(y_obs=y_test, y_pred=m_hgbt.predict(X_test)),\n    popmean=0,\n)\n</pre> ttest_1samp(     pbl.score_per_obs(y_obs=y_test, y_pred=m_linear.predict(X_test))     - pbl.score_per_obs(y_obs=y_test, y_pred=m_hgbt.predict(X_test)),     popmean=0, ) Out[9]: <pre>TtestResult(statistic=-1.1262034739249915, pvalue=0.2614356992337325, df=199)</pre> <p>This time, the p-value is 26%: Assuming the null hypothesis of equal scores is true, there is a 26% chance of the test statistic to be more extreme as our obtained value of -1.126. This is not splendid, but, given our small sample size of 100, it is small enough to reject the null hypothesis and say that the linear model has a significantly better score than the HGBT model.</p> <p>All of the above score analysis was based on the pinball loss, but there are other possible choices. Does the ranking of the models change with the choice of a scoring function? This can be analysed by means of the Murphy diagram. The x-axis <code>eta</code> specifies different scoring/loss functions, all consistent for the 75%-quantile.</p> In\u00a0[10]: Copied! <pre>plot_murphy_diagram(y_obs=y_test, y_pred=df, functional=\"quantile\", level=quantile_level)\n</pre> plot_murphy_diagram(y_obs=y_test, y_pred=df, functional=\"quantile\", level=quantile_level) Out[10]: <pre>&lt;Axes: title={'center': 'Murphy Diagram'}, xlabel='eta', ylabel='score'&gt;</pre> <p>The linear model seems indeed to be superior over a range of <code>eta</code> values.</p>"},{"location":"examples/quantile_regression/#quantile-regression-on-synthetic-data","title":"Quantile Regression on Synthetic Data\u00b6","text":"<p>This notebook shows the diagnostic tools applied to quantile regression. For details, see https://arxiv.org/abs/2202.12780.</p> <p>First, we prepare some plotting settings.</p>"},{"location":"examples/quantile_regression/#1-data-and-models","title":"1. Data and Models\u00b6","text":"<p>We start by creating an artificial dataset with an asymmetric distribution such that mean and median are very different. Then, we fit a linear model and gradient boosted trees for the 75%-quantile.</p>"},{"location":"examples/quantile_regression/#2-calibration-assessment","title":"2. Calibration Assessment\u00b6","text":"<p>First, we look at the calibration in total, i.e. neither conditioning on a feature nor on the predictions. We want to predict the 75%-quantile, so we expect that 25% of the data lie above and 75% below the model predictions. This is what the identification function for quantiles actually calculates such that a value of 0 is ideal. Note that the identification function is implicitly called in <code>compute_bias</code> as well as <code>plot_bias</code>.</p>"},{"location":"examples/quantile_regression/#3-comparison-of-model-performance","title":"3. Comparison of Model Performance\u00b6","text":"<p>We use the standard loss function for quantile regression: the pinball loss. Furthermore, as is standard, we do this analysis out-of-sample, i.e. using the test data set.</p> <p>We report not only the pinball loss or score, but also the additive score decomposition: <code>score = miscalibration - discrimination + uncertainty</code>.</p>"},{"location":"examples/regression_on_workers_compensation/","title":"Regression on Workers' Compensation Dataset","text":"In\u00a0[1]: Copied! <pre>import calendar\nimport numpy as np\nimport pandas as pd\nfrom sklearn import set_config\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\nset_config(transform_output=\"pandas\")  # enable the set_output API\n</pre> import calendar import numpy as np import pandas as pd from sklearn import set_config from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split   set_config(transform_output=\"pandas\")  # enable the set_output API In\u00a0[2]: Copied! <pre>df_original = fetch_openml(data_id=42876, parser=\"auto\").frame\n</pre> df_original = fetch_openml(data_id=42876, parser=\"auto\").frame In\u00a0[3]: Copied! <pre>df = df_original.query(\"WeeklyPay &gt;= 200 and HoursWorkedPerWeek &gt;= 20\")\ndf = df.assign(\n    DateTimeOfAccident=lambda x: pd.to_datetime(x[\"DateTimeOfAccident\"]),\n    DateOfAccident=lambda x: x.DateTimeOfAccident.dt.date,\n    DateReported=lambda x: pd.to_datetime(x.DateReported).dt.date,\n    LogDelay=lambda x: np.log1p(pd.to_timedelta(x.DateReported - x.DateOfAccident).dt.days),\n    HourOfAccident=lambda x: x.DateTimeOfAccident.dt.hour,\n    WeekDayOfAccident=lambda x: pd.Categorical.from_codes(\n        codes=x.DateTimeOfAccident.dt.weekday,\n        dtype=pd.CategoricalDtype(list(calendar.day_name), ordered=True),\n    ),\n    LogWeeklyPay=lambda x: np.log1p(x.WeeklyPay),\n    LogInitial=lambda x: np.log(x.InitialCaseEstimate),\n    DependentChildren=lambda x: np.fmin(4, x.DependentChildren),\n    HoursWorkedPerWeek=lambda x: np.fmin(60, x.HoursWorkedPerWeek),\n    Gender=lambda x: x.Gender.astype(\"category\"),\n    MaritalStatus=lambda x: x.MaritalStatus.astype(\"category\"),\n    PartTimeFullTime=lambda x: x.PartTimeFullTime.astype(\"category\"),\n).rename(columns={\"HoursWorkedPerWeek\": \"HoursPerWeek\"})\n\nx_continuous = [\n    \"Age\",\n    \"LogWeeklyPay\",\n    \"LogInitial\",\n    \"HourOfAccident\",\n    \"HoursPerWeek\",\n    \"LogDelay\",\n]\nx_discrete = [\n    \"Gender\",\n    \"MaritalStatus\",\n    \"PartTimeFullTime\",\n    \"DependentChildren\",\n    \"DaysWorkedPerWeek\",\n    \"WeekDayOfAccident\",\n]\nx_vars = x_continuous + x_discrete\ny_var = \"UltimateIncurredClaimCost\"\n\nprint(f\"The prepared dataset contains {df.shape[0]} rows.\")\n</pre> df = df_original.query(\"WeeklyPay &gt;= 200 and HoursWorkedPerWeek &gt;= 20\") df = df.assign(     DateTimeOfAccident=lambda x: pd.to_datetime(x[\"DateTimeOfAccident\"]),     DateOfAccident=lambda x: x.DateTimeOfAccident.dt.date,     DateReported=lambda x: pd.to_datetime(x.DateReported).dt.date,     LogDelay=lambda x: np.log1p(pd.to_timedelta(x.DateReported - x.DateOfAccident).dt.days),     HourOfAccident=lambda x: x.DateTimeOfAccident.dt.hour,     WeekDayOfAccident=lambda x: pd.Categorical.from_codes(         codes=x.DateTimeOfAccident.dt.weekday,         dtype=pd.CategoricalDtype(list(calendar.day_name), ordered=True),     ),     LogWeeklyPay=lambda x: np.log1p(x.WeeklyPay),     LogInitial=lambda x: np.log(x.InitialCaseEstimate),     DependentChildren=lambda x: np.fmin(4, x.DependentChildren),     HoursWorkedPerWeek=lambda x: np.fmin(60, x.HoursWorkedPerWeek),     Gender=lambda x: x.Gender.astype(\"category\"),     MaritalStatus=lambda x: x.MaritalStatus.astype(\"category\"),     PartTimeFullTime=lambda x: x.PartTimeFullTime.astype(\"category\"), ).rename(columns={\"HoursWorkedPerWeek\": \"HoursPerWeek\"})  x_continuous = [     \"Age\",     \"LogWeeklyPay\",     \"LogInitial\",     \"HourOfAccident\",     \"HoursPerWeek\",     \"LogDelay\", ] x_discrete = [     \"Gender\",     \"MaritalStatus\",     \"PartTimeFullTime\",     \"DependentChildren\",     \"DaysWorkedPerWeek\",     \"WeekDayOfAccident\", ] x_vars = x_continuous + x_discrete y_var = \"UltimateIncurredClaimCost\"  print(f\"The prepared dataset contains {df.shape[0]} rows.\") <pre>The prepared dataset contains 82017 rows.\n</pre> In\u00a0[4]: Copied! <pre>df.head()\n</pre> df.head() Out[4]: DateTimeOfAccident DateReported Age Gender MaritalStatus DependentChildren DependentsOther WeeklyPay PartTimeFullTime HoursPerWeek DaysWorkedPerWeek ClaimDescription InitialCaseEstimate UltimateIncurredClaimCost DateOfAccident LogDelay HourOfAccident WeekDayOfAccident LogWeeklyPay LogInitial 0 2005-01-13 09:00:00 2005-01-24 45 M S 0 0 500 F 38 5 MOVING DISC STRAINED RIGHT SHOULDER 9500.0 102.39 2005-01-13 2.484907 9 Thursday 6.216606 9.159047 1 1994-09-28 15:00:00 1994-10-17 40 M M 0 0 283 F 38 5 BOILING WATER CAME FROM TRUCK STRAIN RIGHT WRIST 3000.0 1451.00 1994-09-28 2.995732 15 Wednesday 5.648974 8.006368 5 2002-02-28 07:00:00 2002-04-08 50 F S 0 0 517 F 38 5 SPILLED CHEMICAL WELDING IRRITATION LEFT CORNEA 1000.0 320.28 2002-02-28 3.688879 7 Thursday 6.249975 6.907755 7 1995-04-20 14:00:00 1995-05-08 19 M S 0 0 200 F 38 5 ENTERED GRINDER FOREIGN BODY RIGHT EYE 110.0 108.00 1995-04-20 2.944439 14 Thursday 5.303305 4.700480 8 2005-01-07 14:00:00 2005-01-31 19 M S 0 0 767 F 40 5 LIFTING STRAIN LOWER BACK AND 9700.0 7110.90 2005-01-07 3.218876 14 Friday 6.643790 9.179881 In\u00a0[5]: Copied! <pre>df_train, df_test = train_test_split(df, train_size=0.75, random_state=1234321)\ndf = (\n    pd.concat((df_train, df_test), axis=0, keys=(\"train\", \"test\"))\n    .reset_index(level=0)\n    .rename(columns={\"level_0\": \"split\"})\n)\n\ny_train, y_test = df_train[y_var], df_test[y_var]\nX_train, X_test = df_train[x_vars], df_test[x_vars]\n</pre> df_train, df_test = train_test_split(df, train_size=0.75, random_state=1234321) df = (     pd.concat((df_train, df_test), axis=0, keys=(\"train\", \"test\"))     .reset_index(level=0)     .rename(columns={\"level_0\": \"split\"}) )  y_train, y_test = df_train[y_var], df_test[y_var] X_train, X_test = df_train[x_vars], df_test[x_vars] <p>We check whether the split results in two approximately identically distributed samples.</p> In\u00a0[6]: Copied! <pre>df.groupby(\"split\").agg(\n    mean=pd.NamedAgg(column=y_var, aggfunc=\"mean\"),\n    q20=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.2)),\n    q40=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.4)),\n    q50=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.5)),\n    q60=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.6)),\n    q80=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.8)),\n    q90=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.9)),\n)\n</pre> df.groupby(\"split\").agg(     mean=pd.NamedAgg(column=y_var, aggfunc=\"mean\"),     q20=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.2)),     q40=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.4)),     q50=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.5)),     q60=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.6)),     q80=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.8)),     q90=pd.NamedAgg(column=y_var, aggfunc=lambda x: np.quantile(x, 0.9)), ) Out[6]: mean q20 q40 q50 q60 q80 q90 split test 13300.958856 205.0 464.714 707.500 1135.984 4716.426 18979.432 train 12909.500160 203.0 459.000 691.185 1096.230 4596.730 19030.384 In\u00a0[7]: Copied! <pre>from sklearn.dummy import DummyRegressor\n\n\nm_trivial = DummyRegressor(strategy=\"mean\").fit(X_train, y_train)\n</pre> from sklearn.dummy import DummyRegressor   m_trivial = DummyRegressor(strategy=\"mean\").fit(X_train, y_train) In\u00a0[\u00a0]: Copied! <pre>from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n# ColumnTransformer for linear models\ncol_trans_linear = ColumnTransformer(\n    [\n        (\"numeric_features\", \"passthrough\", x_continuous),\n        (\n            \"categorical_features\",\n            OneHotEncoder(drop=\"first\", sparse_output=False),\n            x_discrete\n        ),\n    ]\n)\n\nm_ols = Pipeline(\n    [\n        (\"column_transformer\", col_trans_linear),\n        (\n            \"model\",\n            TransformedTargetRegressor(\n                regressor=LinearRegression(),\n                func=np.log,\n                inverse_func=np.exp,\n            )\n        ),\n    ]\n).fit(X_train, y_train)\n</pre> from sklearn.compose import ColumnTransformer, TransformedTargetRegressor from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder   # ColumnTransformer for linear models col_trans_linear = ColumnTransformer(     [         (\"numeric_features\", \"passthrough\", x_continuous),         (             \"categorical_features\",             OneHotEncoder(drop=\"first\", sparse_output=False),             x_discrete         ),     ] )  m_ols = Pipeline(     [         (\"column_transformer\", col_trans_linear),         (             \"model\",             TransformedTargetRegressor(                 regressor=LinearRegression(),                 func=np.log,                 inverse_func=np.exp,             )         ),     ] ).fit(X_train, y_train) <p>We expect a high bias due to the log transformation. We correct for this bias by a multiplicative constant such that on the training set: $\\sum_i m(x_i) = \\sum_i y_i$.</p> In\u00a0[9]: Copied! <pre>ols_corr_factor = np.sum(y_train) / np.sum(m_ols.predict(X_train))\nprint(f\"The correction factor is {ols_corr_factor}\")\nm_ols[-1].regressor_.intercept_ += np.log(ols_corr_factor)\n</pre> ols_corr_factor = np.sum(y_train) / np.sum(m_ols.predict(X_train)) print(f\"The correction factor is {ols_corr_factor}\") m_ols[-1].regressor_.intercept_ += np.log(ols_corr_factor) <pre>The correction factor is 6.6446580674943165\n</pre> In\u00a0[10]: Copied! <pre>np.sum(y_train) / np.sum(m_ols.predict(X_train))\n</pre> np.sum(y_train) / np.sum(m_ols.predict(X_train)) Out[10]: <pre>1.0000000000000002</pre> In\u00a0[11]: Copied! <pre>from sklearn.linear_model import GammaRegressor, PoissonRegressor\n\n\nm_glm_poisson = Pipeline(\n    [\n        (\"column_transformer\", col_trans_linear),\n        (\"model\", PoissonRegressor(alpha=1e-15, solver=\"newton-cholesky\")),\n    ]\n).fit(X_train, y_train)\n\nm_glm_gamma = Pipeline(\n    [\n        (\"column_transformer\", col_trans_linear),\n        (\"model\", GammaRegressor(alpha=1e-15, solver=\"newton-cholesky\")),\n    ]\n).fit(X_train, y_train)\n</pre> from sklearn.linear_model import GammaRegressor, PoissonRegressor   m_glm_poisson = Pipeline(     [         (\"column_transformer\", col_trans_linear),         (\"model\", PoissonRegressor(alpha=1e-15, solver=\"newton-cholesky\")),     ] ).fit(X_train, y_train)  m_glm_gamma = Pipeline(     [         (\"column_transformer\", col_trans_linear),         (\"model\", GammaRegressor(alpha=1e-15, solver=\"newton-cholesky\")),     ] ).fit(X_train, y_train) In\u00a0[12]: Copied! <pre>%%time\n# Note that this cell might take a little while ~ 2 minutes on my laptop.\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\n# ColumnTransformer for boosted trees\ncol_trans_bt = ColumnTransformer(\n    [\n        (\"categorical_features\", OrdinalEncoder(), x_discrete),\n        (\"numeric_features\", \"passthrough\", x_continuous),\n    ]\n)\nm_hgbt_poisson_base = Pipeline(\n    [\n        (\"column_transformer\", col_trans_bt),\n        (\n            \"model\", HistGradientBoostingRegressor(\n                loss=\"poisson\",\n                categorical_features=list(range(len(x_discrete))),\n                monotonic_cst={\"numeric_features__LogWeeklyPay\": 1}, # set_output API, YES!\n                max_iter=200,\n                random_state=33,\n            )\n        ),\n    ]\n)\n\nparam_grid = {\n    \"model__learning_rate\": [0.02, 0.05],\n    \"model__min_samples_leaf\": [30, 40, 50],\n    \"model__l2_regularization\": [0.1, 1],\n    \"model__max_depth\": [None, 3],\n}\n\n\n# successive halfing grid search (CV) on the training data\nshgs = HalvingGridSearchCV(\n    m_hgbt_poisson_base,\n    param_grid=param_grid,\n    cv=5,\n    scoring=\"neg_mean_gamma_deviance\",\n    random_state=321,\n).fit(X_train, y_train)\nm_hgbt_poisson = shgs.best_estimator_\n</pre> %%time # Note that this cell might take a little while ~ 2 minutes on my laptop. from sklearn.ensemble import HistGradientBoostingRegressor from sklearn.experimental import enable_halving_search_cv from sklearn.model_selection import HalvingGridSearchCV from sklearn.preprocessing import OrdinalEncoder   # ColumnTransformer for boosted trees col_trans_bt = ColumnTransformer(     [         (\"categorical_features\", OrdinalEncoder(), x_discrete),         (\"numeric_features\", \"passthrough\", x_continuous),     ] ) m_hgbt_poisson_base = Pipeline(     [         (\"column_transformer\", col_trans_bt),         (             \"model\", HistGradientBoostingRegressor(                 loss=\"poisson\",                 categorical_features=list(range(len(x_discrete))),                 monotonic_cst={\"numeric_features__LogWeeklyPay\": 1}, # set_output API, YES!                 max_iter=200,                 random_state=33,             )         ),     ] )  param_grid = {     \"model__learning_rate\": [0.02, 0.05],     \"model__min_samples_leaf\": [30, 40, 50],     \"model__l2_regularization\": [0.1, 1],     \"model__max_depth\": [None, 3], }   # successive halfing grid search (CV) on the training data shgs = HalvingGridSearchCV(     m_hgbt_poisson_base,     param_grid=param_grid,     cv=5,     scoring=\"neg_mean_gamma_deviance\",     random_state=321, ).fit(X_train, y_train) m_hgbt_poisson = shgs.best_estimator_ <pre>CPU times: user 4min 49s, sys: 5.1 s, total: 4min 55s\nWall time: 1min 17s\n</pre> In\u00a0[13]: Copied! <pre>shgs.best_params_, shgs.best_score_\n</pre> shgs.best_params_, shgs.best_score_ Out[13]: <pre>({'model__l2_regularization': 1,\n  'model__learning_rate': 0.05,\n  'model__max_depth': 3,\n  'model__min_samples_leaf': 40},\n -3.547907637012485)</pre> In\u00a0[14]: Copied! <pre>m_hgbt_poisson[-1].n_iter_\n</pre> m_hgbt_poisson[-1].n_iter_ Out[14]: <pre>130</pre> In\u00a0[15]: Copied! <pre>df_pred_test = pd.DataFrame(\n    {\n        \"Trivial\": m_trivial.predict(X_test),\n        \"OLS\": m_ols.predict(X_test),\n        \"GLM_Gamma\": m_glm_gamma.predict(X_test),\n        \"GLM_Poisson\": m_glm_poisson.predict(X_test),\n        \"HGBT_Poisson\": m_hgbt_poisson.predict(X_test),\n    }\n)\n</pre> df_pred_test = pd.DataFrame(     {         \"Trivial\": m_trivial.predict(X_test),         \"OLS\": m_ols.predict(X_test),         \"GLM_Gamma\": m_glm_gamma.predict(X_test),         \"GLM_Poisson\": m_glm_poisson.predict(X_test),         \"HGBT_Poisson\": m_hgbt_poisson.predict(X_test),     } ) In\u00a0[16]: Copied! <pre>import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MultipleLocator\nfrom model_diagnostics.calibration import compute_bias, plot_bias, plot_reliability_diagram\n</pre> import matplotlib.pyplot as plt from matplotlib.ticker import MultipleLocator from model_diagnostics.calibration import compute_bias, plot_bias, plot_reliability_diagram In\u00a0[17]: Copied! <pre>fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 8), sharex=True, sharey=True)\n\nfor ax, model in zip(axes.flat, [\"OLS\", \"GLM_Gamma\", \"GLM_Poisson\", \"HGBT_Poisson\"]):\n    plot_reliability_diagram(\n        y_obs=y_test,\n        y_pred=df_pred_test[model],\n        n_bootstrap=100,\n        ax=ax,\n    )\n    ax.xaxis.set_major_locator(MultipleLocator(100_000))\n\nfig.set_layout_engine(layout=\"tight\")\n</pre> fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 8), sharex=True, sharey=True)  for ax, model in zip(axes.flat, [\"OLS\", \"GLM_Gamma\", \"GLM_Poisson\", \"HGBT_Poisson\"]):     plot_reliability_diagram(         y_obs=y_test,         y_pred=df_pred_test[model],         n_bootstrap=100,         ax=ax,     )     ax.xaxis.set_major_locator(MultipleLocator(100_000))  fig.set_layout_engine(layout=\"tight\") <p>For a better visual comparison, we use the <code>diagram_type=\"bias\"</code> option to rotate the plots by 45 degrees (and mirror on the horizontal line) and plot all models in one go. This means, closer to y=0 is better. On top, we set the x-axis to a logarithmic scale and zoom in a little.</p> In\u00a0[18]: Copied! <pre>ax = plot_reliability_diagram(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    n_bootstrap=100,\n    diagram_type=\"bias\",\n)\nax.set_xlim(left=1e4, right=3e5)\nax.set_ylim(bottom=-1e5, top=1e5)\nax.set_xscale(\"log\")\n</pre> ax = plot_reliability_diagram(     y_obs=y_test,     y_pred=df_pred_test,     n_bootstrap=100,     diagram_type=\"bias\", ) ax.set_xlim(left=1e4, right=3e5) ax.set_ylim(bottom=-1e5, top=1e5) ax.set_xscale(\"log\") In\u00a0[19]: Copied! <pre>compute_bias(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    feature=None,\n)\n</pre> compute_bias(     y_obs=y_test,     y_pred=df_pred_test,     feature=None, ) Out[19]: shape: (5, 6)modelbias_meanbias_countbias_weightsbias_stderrp_valuestrf64u32f64f64f64\"Trivial\"-391.4586962050520505.0394.4279090.320979\"OLS\"-158.2262772050520505.0370.4703430.669314\"GLM_Gamma\"-1525.262422050520505.0373.6537790.000045\"GLM_Poisson\"-142.6723622050520505.0370.4585930.70015\"HGBT_Poisson\"-94.4399052050520505.0370.8233640.798976 In\u00a0[20]: Copied! <pre>compute_bias(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    feature=X_test[\"Gender\"],\n)\n</pre> compute_bias(     y_obs=y_test,     y_pred=df_pred_test,     feature=X_test[\"Gender\"], ) Out[20]: shape: (10, 7)modelGenderbias_meanbias_countbias_weightsbias_stderrp_valuestrcatf64u32f64f64f64\"Trivial\"\"F\"-4693.41476244414441.0925.5110274.1151e-7\"Trivial\"\"M\"797.8457671606416064.0433.1552340.065502\"OLS\"\"F\"-1212.77311744414441.0878.1888220.16735\"OLS\"\"M\"133.3102351606416064.0405.7953010.742526\"GLM_Gamma\"\"F\"-1183.00501644414441.0882.6855870.180238\"GLM_Gamma\"\"M\"-1619.8817631606416064.0409.8125890.000078\"GLM_Poisson\"\"F\"-184.99512244414441.0878.9311760.833305\"GLM_Poisson\"\"M\"-130.9719521606416064.0405.6850450.74682\"HGBT_Poisson\"\"F\"-303.38457544414441.0875.6096710.728996\"HGBT_Poisson\"\"M\"-36.6757561606416064.0406.7743130.928159 In\u00a0[21]: Copied! <pre>plot_bias(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    feature=X_test[\"Gender\"],\n    confidence_level=0.8\n)\n</pre> plot_bias(     y_obs=y_test,     y_pred=df_pred_test,     feature=X_test[\"Gender\"],     confidence_level=0.8 ) Out[21]: <pre>&lt;Axes: title={'center': 'Bias Plot'}, xlabel='Gender', ylabel='bias'&gt;</pre> In\u00a0[22]: Copied! <pre>plot_bias(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    feature=X_test[\"LogWeeklyPay\"],\n    confidence_level=0.8,\n    n_bins=20,\n    bin_method=\"quantile\",\n)\n</pre> plot_bias(     y_obs=y_test,     y_pred=df_pred_test,     feature=X_test[\"LogWeeklyPay\"],     confidence_level=0.8,     n_bins=20,     bin_method=\"quantile\", ) Out[22]: <pre>&lt;Axes: title={'center': 'Bias Plot'}, xlabel='binned LogWeeklyPay', ylabel='bias'&gt;</pre> In\u00a0[23]: Copied! <pre>ax = plot_bias(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    feature=X_test[\"Age\"],\n    n_bins=20,  # X_test[\"Age\"].nunique(),\n    bin_method=\"quantile\",\n    confidence_level=0.8,\n)\n</pre> ax = plot_bias(     y_obs=y_test,     y_pred=df_pred_test,     feature=X_test[\"Age\"],     n_bins=20,  # X_test[\"Age\"].nunique(),     bin_method=\"quantile\",     confidence_level=0.8, ) In\u00a0[24]: Copied! <pre>from model_diagnostics import config_context\nfrom model_diagnostics.calibration import plot_marginal\n\n\nwith config_context(plot_backend=\"plotly\"):\n    fig = plot_marginal(\n        y_obs=y_test,\n        y_pred=df_pred_test[\"HGBT_Poisson\"],\n        X=X_test,\n        feature_name=\"Age\",\n        predict_function=m_hgbt_poisson.predict\n    )\nfig.show(renderer=\"notebook\")\n</pre> from model_diagnostics import config_context from model_diagnostics.calibration import plot_marginal   with config_context(plot_backend=\"plotly\"):     fig = plot_marginal(         y_obs=y_test,         y_pred=df_pred_test[\"HGBT_Poisson\"],         X=X_test,         feature_name=\"Age\",         predict_function=m_hgbt_poisson.predict     ) fig.show(renderer=\"notebook\") <p>It is even possible to plot several features as subplots of one single figure. For the plotly backend, this would amount to a lot of extra work, as <code>plot_marginal</code> is already a subplot consisting of 2 y-axis which are side-flipped, 1st yaxis on the right, 2nd yaxis on the left side. This is necessary to plot the histogram in the background. In order to simplify this for users, there is the function <code>add_marginal_subplot</code> that does all the heavy lifting.</p> In\u00a0[25]: Copied! <pre>from math import ceil\nfrom plotly.subplots import make_subplots\nfrom model_diagnostics.calibration import add_marginal_subplot\n\n\nfeature_list = [\"Gender\", \"MaritalStatus\", \"LogWeeklyPay\", \"Age\"]\nn_cols = 2\nn_rows = ceil(len(feature_list) / n_cols)\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    vertical_spacing=0.4 / n_rows,  # default = 0.3 / n_rows\n    # subplot_titles=feature_list,\n    specs=[[{\"secondary_y\": True}] * n_cols] * n_rows,  # This is important!\n)\nfor row in range(n_rows):\n    for col in range(n_cols):\n        i = n_cols * row + col\n        with config_context(plot_backend=\"plotly\"):\n            subfig = plot_marginal(\n                y_obs=y_test,\n                y_pred=df_pred_test[\"HGBT_Poisson\"],\n                X=X_test,\n                feature_name=feature_list[i],\n                predict_function=m_hgbt_poisson.predict,\n            )\n        add_marginal_subplot(subfig, fig, row, col)\n\nfig.update_xaxes(title_standoff=5)\nfig.update_yaxes(title_standoff=5)\nfig.show(renderer=\"notebook\")\n</pre> from math import ceil from plotly.subplots import make_subplots from model_diagnostics.calibration import add_marginal_subplot   feature_list = [\"Gender\", \"MaritalStatus\", \"LogWeeklyPay\", \"Age\"] n_cols = 2 n_rows = ceil(len(feature_list) / n_cols) fig = make_subplots(     rows=n_rows,     cols=n_cols,     vertical_spacing=0.4 / n_rows,  # default = 0.3 / n_rows     # subplot_titles=feature_list,     specs=[[{\"secondary_y\": True}] * n_cols] * n_rows,  # This is important! ) for row in range(n_rows):     for col in range(n_cols):         i = n_cols * row + col         with config_context(plot_backend=\"plotly\"):             subfig = plot_marginal(                 y_obs=y_test,                 y_pred=df_pred_test[\"HGBT_Poisson\"],                 X=X_test,                 feature_name=feature_list[i],                 predict_function=m_hgbt_poisson.predict,             )         add_marginal_subplot(subfig, fig, row, col)  fig.update_xaxes(title_standoff=5) fig.update_yaxes(title_standoff=5) fig.show(renderer=\"notebook\") In\u00a0[26]: Copied! <pre>import polars as pl\nfrom model_diagnostics.scoring import GammaDeviance, decompose\n\n\ngamma_deviance = GammaDeviance()\n\ndf = decompose(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    scoring_function=gamma_deviance,\n)\ndf.sort([\"score\"])\n</pre> import polars as pl from model_diagnostics.scoring import GammaDeviance, decompose   gamma_deviance = GammaDeviance()  df = decompose(     y_obs=y_test,     y_pred=df_pred_test,     scoring_function=gamma_deviance, ) df.sort([\"score\"]) Out[26]: shape: (5, 5)modelmiscalibrationdiscriminationuncertaintyscorestrf64f64f64f64\"HGBT_Poisson\"0.0881141.6967575.0860023.477359\"GLM_Gamma\"0.190561.5977075.0860023.678855\"OLS\"0.2719941.6056045.0860023.752392\"GLM_Poisson\"0.4709651.6241915.0860023.932776\"Trivial\"0.0009018.8818e-165.0860025.086903 <p>First, the HGBT model has the best (smallest) overall Gamma deviance in column score. We also see that the uncertainty term is model independent as it is a property of the data and the scoring function only. Very typical for insurance claims, it is the largest component. This shows that insurance claims happen to a large part by chance.</p> <p>The deciding difference of the scores then stems from the miscalibration term. Except for the trivial model, the HGBT model has the best (smallest) one. The discrimination term (larger is better) is quite similar among the non-trivial models. Interestingly, the Gamma GLM has a better miscalibration term, but a worse discrimination term compared to the Poisson GLM.</p> In\u00a0[27]: Copied! <pre>from model_diagnostics.scoring import SquaredError, plot_murphy_diagram\n\n\nax = plot_murphy_diagram(\n    y_obs=y_test,\n    y_pred=df_pred_test.loc[:, [\"OLS\", \"GLM_Gamma\", \"GLM_Poisson\", \"HGBT_Poisson\"]],\n    etas=np.linspace(1e4, 0.2e6, 1000),\n)\n</pre> from model_diagnostics.scoring import SquaredError, plot_murphy_diagram   ax = plot_murphy_diagram(     y_obs=y_test,     y_pred=df_pred_test.loc[:, [\"OLS\", \"GLM_Gamma\", \"GLM_Poisson\", \"HGBT_Poisson\"]],     etas=np.linspace(1e4, 0.2e6, 1000), ) <p>We see that there is no clear winner. Indeed, for other scoring functions like the squared error, the HGBT model is not the best performing one.</p> In\u00a0[28]: Copied! <pre>decompose(\n    y_obs=y_test,\n    y_pred=df_pred_test,\n    scoring_function=SquaredError(),\n).sort([\"score\"])\n</pre> decompose(     y_obs=y_test,     y_pred=df_pred_test,     scoring_function=SquaredError(), ).sort([\"score\"]) Out[28]: shape: (5, 5)modelmiscalibrationdiscriminationuncertaintyscorestrf64f64f64f64\"GLM_Poisson\"3.8145e74.1404e83.1899e92.8140e9\"OLS\"6.8568e74.4428e83.1899e92.8142e9\"HGBT_Poisson\"2.7010e73.9737e83.1899e92.8195e9\"GLM_Gamma\"4.1506e73.6635e83.1899e92.8650e9\"Trivial\"153239.9106110.03.1899e93.1900e9"},{"location":"examples/regression_on_workers_compensation/#regression-on-workers-compensation-dataset","title":"Regression on Workers' Compensation Dataset\u00b6","text":"<p>This notebook demonstrates model diagnostics on the workers' compensation dataset https://www.openml.org/d/42876. For details, see https://arxiv.org/abs/2202.12780.</p> <p>The modelling goal is to predict the expectation $E(Y|X)$ of $Y=\\text{UltimateIncurredClaimCost}$ conditional on the features $X$.</p> <p>The impatient can directly jump to chapters 3-4 where the capability of <code>model-diagnostics</code> are shown. Chapters 1-2 are needed for data preparation and model training.</p>"},{"location":"examples/regression_on_workers_compensation/#1-data","title":"1. Data\u00b6","text":""},{"location":"examples/regression_on_workers_compensation/#11-data-load-and-preprocessing","title":"1.1 Data Load and Preprocessing\u00b6","text":""},{"location":"examples/regression_on_workers_compensation/#12-data-split","title":"1.2 Data Split\u00b6","text":""},{"location":"examples/regression_on_workers_compensation/#2-models","title":"2. Models\u00b6","text":"<p>We aim at predicting the conditional expectation $E(Y|X)$ and will finally evaluate the models with the Gamma deviance $S(z, y) = 2 \\left(log(\\frac{z}{y}) + \\frac{y}{z} - 1\\right)$.</p>"},{"location":"examples/regression_on_workers_compensation/#21-the-trivial-model","title":"2.1 The trivial model\u00b6","text":""},{"location":"examples/regression_on_workers_compensation/#22-ols","title":"2.2 OLS\u00b6","text":"<p>Here, we train an Ordinary Least Squares (OLS) model, but on the log transformed target. <code>TransformedTargetRegressor</code> takes care of this transformation and also of the back transformation such that predictions are on the original target UltimateIncurredClaimCost.</p>"},{"location":"examples/regression_on_workers_compensation/#23-gamma-and-poisson-glm","title":"2.3 Gamma and Poisson GLM\u00b6","text":"<p>We use both, a Poisson and a Gamma GLM. The reason for a Gamma GLM is that we measure performance with the Gamma deviance. The reason to try out a Poisson GLM is that is has a canonical loss-link combination that ensures good calibration (given the design matrix is well specified).</p>"},{"location":"examples/regression_on_workers_compensation/#24-gradient-boosted-decision-trees","title":"2.4 Gradient Boosted Decision Trees\u00b6","text":"<p>Note that starting with scikit-learn version 1.3, we could use the Gamma deviance to train boosted trees, e.g. <code>HistGradientBoostingRegressor(loss=\"gamma\")</code> (HGBT)\u2014or use XGBoost or LightGBM for that. Instead, we again use the Poisson deviance for training the HGBT to make this exercise a bit more interesting. Here again, we expect better calibration for the canonical loss-link combination that comes with <code>loss=\"poisson\"</code>.</p>"},{"location":"examples/regression_on_workers_compensation/#3-calibration-assessment","title":"3. Calibration Assessment\u00b6","text":"<p>To make the code easier, we put together the predictions of all of our models on the test set in a single dataframe.</p>"},{"location":"examples/regression_on_workers_compensation/#31-reliability-diagrams","title":"3.1. Reliability Diagrams\u00b6","text":"<p>A reliability diagram plots an estimation of $E(Y|m(X))$ versus $m(X)$ and therefore assesses auto-calibration. Good auto-calibration is seen by closeness to the diagonal depicted as dotted line.</p> <p>A good way to estimate $E(Y|m(X))$ and thereby avoiding manual binning is by isotonic regression. This is implemented in <code>plot_reliability_diagram</code>.</p>"},{"location":"examples/regression_on_workers_compensation/#32-conditional-bias","title":"3.2 Conditional Bias\u00b6","text":""},{"location":"examples/regression_on_workers_compensation/#unconditional-bias","title":"Unconditional Bias\u00b6","text":""},{"location":"examples/regression_on_workers_compensation/#bias-conditional-on-gender","title":"Bias Conditional on Gender\u00b6","text":""},{"location":"examples/regression_on_workers_compensation/#bias-conditional-on-logweeklypay","title":"Bias Conditional on LogWeeklyPay\u00b6","text":""},{"location":"examples/regression_on_workers_compensation/#bias-conditional-on-age","title":"Bias Conditional on Age\u00b6","text":"<p>Age is given as an integer in the data. We can pass <code>n_bins</code> to appropriate number to see all integer ages that occur in the test data.</p>"},{"location":"examples/regression_on_workers_compensation/#33-marginal-plot-a-model-overview-per-feature","title":"3.3 Marginal Plot - a Model overview per feature\u00b6","text":"<p>Here, we focus on a single model, say HGBT_Poisson. We want to get a good overview of calibration, model effects and feature distribution at the same time. This functionality is offered by <code>plot_marginal</code>. It plots:</p> <ul> <li>Mean observed and mean predicted per (binned) feature value</li> <li>Partial dependence of the feature</li> <li>Marginal distribution of the feature</li> </ul> <p>To make this even more interesting, we use the plotly backend.</p>"},{"location":"examples/regression_on_workers_compensation/#4-predictive-model-performance","title":"4. Predictive Model Performance\u00b6","text":""},{"location":"examples/regression_on_workers_compensation/#41-score-decomposition-of-the-gamma-deviance","title":"4.1 Score Decomposition of the Gamma Deviance\u00b6","text":"<p>We use the Gamma deviance and evaluate on the test set. On top, we additively decompose the scores in terms of miscalibration (smaller is better), discrimination (larger is better) and uncertainty (property of the data, same for all models).</p>"},{"location":"examples/regression_on_workers_compensation/#42-murphy-diagram","title":"4.2 Murphy Diagram\u00b6","text":"<p>How does the ranking of the model performances change with other scoring functions? Is one model dominating the others for a wide range of scoring functions? Such questions can be handily answered by a Murphy diagram. It plots the mean score for a wide range of elementary scoring functions, parametrized by <code>eta</code> on the x-axis. We explicitly specify the <code>etas</code> to zoom into the range where differences can be spotted.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>model_diagnostics<ul> <li>calibration<ul> <li>identification</li> <li>plots</li> </ul> </li> <li>scoring<ul> <li>plots</li> <li>scoring</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/model_diagnostics/","title":"model_diagnostics","text":""},{"location":"reference/model_diagnostics/#model_diagnostics","title":"<code>model_diagnostics</code>","text":""},{"location":"reference/model_diagnostics/#model_diagnostics.config_context","title":"<code>config_context(*, plot_backend=None)</code>","text":"<p>Context manager for global model-diagnostics configuration.</p> <p>Parameters:</p> Name Type Description Default <code>plot_backend</code> <code>bool</code> <p>The library used for plotting. Can be \"matplotlib\" or \"plotly\". If None, the existing value won't change. Global default: \"matplotlib\".</p> <code>None</code> <p>Yields:</p> Type Description <code>None.</code> See Also <p>set_config : Set global model-diagnostics configuration. get_config : Retrieve current values of the global configuration.</p> Notes <p>All settings, not just those presently modified, will be returned to their previous values when the context manager is exited.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import model_diagnostics\n&gt;&gt;&gt; from model_diagnostics.calibration import plot_reliability_diagram\n&gt;&gt;&gt; with model_diagnostics.config_context(plot_backend=\"plotly\"):\n...    plot_reliability_diagram(y_obs=[0, 1], y_pred=[0.3, 0.7])\n</code></pre> Source code in <code>src/model_diagnostics/_config.py</code> <pre><code>@contextmanager\ndef config_context(\n    *,\n    plot_backend: Optional[str] = None,\n) -&gt; Iterator[None]:\n    \"\"\"Context manager for global model-diagnostics configuration.\n\n    Parameters\n    ----------\n    plot_backend : bool, default=None\n        The library used for plotting. Can be \"matplotlib\" or \"plotly\".\n        If None, the existing value won't change. Global default: \"matplotlib\".\n\n    Yields\n    ------\n    None.\n\n    See Also\n    --------\n    set_config : Set global model-diagnostics configuration.\n    get_config : Retrieve current values of the global configuration.\n\n    Notes\n    -----\n    All settings, not just those presently modified, will be returned to\n    their previous values when the context manager is exited.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import model_diagnostics\n    &gt;&gt;&gt; from model_diagnostics.calibration import plot_reliability_diagram\n    &gt;&gt;&gt; with model_diagnostics.config_context(plot_backend=\"plotly\"):  # doctest: +SKIP\n    ...    plot_reliability_diagram(y_obs=[0, 1], y_pred=[0.3, 0.7])\n    \"\"\"\n    old_config = get_config()\n    set_config(\n        plot_backend=plot_backend,\n    )\n\n    try:\n        yield\n    finally:\n        set_config(**old_config)\n</code></pre>"},{"location":"reference/model_diagnostics/#model_diagnostics.get_config","title":"<code>get_config()</code>","text":"<p>Retrieve current values for configuration set by :func:<code>set_config</code>.</p> <p>Returns:</p> Name Type Description <code>config</code> <code>dict</code> <p>A copy of the configuration dictionary. Keys are parameter names that can be passed to :func:<code>set_config</code>.</p> See Also <p>config_context : Context manager for global model-diagnostics configuration. set_config : Set global model-diagnostics configuration.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import model_diagnostics\n&gt;&gt;&gt; config = model_diagnostics.get_config()\n&gt;&gt;&gt; config.keys()\ndict_keys([...])\n</code></pre> Source code in <code>src/model_diagnostics/_config.py</code> <pre><code>def get_config() -&gt; dict:\n    \"\"\"Retrieve current values for configuration set by :func:`set_config`.\n\n    Returns\n    -------\n    config : dict\n        A copy of the configuration dictionary. Keys are parameter names that can be\n        passed to :func:`set_config`.\n\n    See Also\n    --------\n    config_context : Context manager for global model-diagnostics configuration.\n    set_config : Set global model-diagnostics configuration.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import model_diagnostics\n    &gt;&gt;&gt; config = model_diagnostics.get_config()\n    &gt;&gt;&gt; config.keys()\n    dict_keys([...])\n    \"\"\"\n    # Return a copy of the global config so that users will\n    # not be able to modify the configuration with the returned dict.\n    return _global_config.copy()\n</code></pre>"},{"location":"reference/model_diagnostics/#model_diagnostics.set_config","title":"<code>set_config(plot_backend=None)</code>","text":"<p>Set global model-diagnostics configuration.</p> <p>Parameters:</p> Name Type Description Default <code>plot_backend</code> <code>bool</code> <p>The library used for plotting. Can be \"matplotlib\" or \"plotly\". If None, the existing value won't change. Global default: \"matplotlib\".</p> <code>None</code> See Also <p>config_context : Context manager for global scikit-learn configuration. get_config : Retrieve current values of the global configuration.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from model_diagnostics import set_config\n&gt;&gt;&gt; set_config(plot_backend=\"plotly\")\n</code></pre> Source code in <code>src/model_diagnostics/_config.py</code> <pre><code>def set_config(\n    plot_backend: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Set global model-diagnostics configuration.\n\n    Parameters\n    ----------\n    plot_backend : bool, default=None\n        The library used for plotting. Can be \"matplotlib\" or \"plotly\".\n        If None, the existing value won't change. Global default: \"matplotlib\".\n\n    See Also\n    --------\n    config_context : Context manager for global scikit-learn configuration.\n    get_config : Retrieve current values of the global configuration.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from model_diagnostics import set_config\n    &gt;&gt;&gt; set_config(plot_backend=\"plotly\")  # doctest: +SKIP\n    \"\"\"\n    if plot_backend not in (None, \"matplotlib\", \"plotly\"):\n        msg = f\"The plot_backend must be matplotlib or plotly, got {plot_backend}.\"\n        raise ValueError(msg)\n    if plot_backend == \"plotly\" and not find_spec(\"plotly\"):\n        msg = (\n            \"In order to set the plot backend to plotly, plotly must be installed, \"\n            \"i.e. via `pip install plotly`.\"\n        )\n        raise ModuleNotFoundError(msg)\n\n    if plot_backend is not None:\n        _global_config[\"plot_backend\"] = plot_backend\n</code></pre>"},{"location":"reference/model_diagnostics/#model_diagnostics.get_config","title":"<code>get_config()</code>","text":"<p>Retrieve current values for configuration set by :func:<code>set_config</code>.</p> <p>Returns:</p> Name Type Description <code>config</code> <code>dict</code> <p>A copy of the configuration dictionary. Keys are parameter names that can be passed to :func:<code>set_config</code>.</p> See Also <p>config_context : Context manager for global model-diagnostics configuration. set_config : Set global model-diagnostics configuration.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import model_diagnostics\n&gt;&gt;&gt; config = model_diagnostics.get_config()\n&gt;&gt;&gt; config.keys()\ndict_keys([...])\n</code></pre> Source code in <code>src/model_diagnostics/_config.py</code> <pre><code>def get_config() -&gt; dict:\n    \"\"\"Retrieve current values for configuration set by :func:`set_config`.\n\n    Returns\n    -------\n    config : dict\n        A copy of the configuration dictionary. Keys are parameter names that can be\n        passed to :func:`set_config`.\n\n    See Also\n    --------\n    config_context : Context manager for global model-diagnostics configuration.\n    set_config : Set global model-diagnostics configuration.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import model_diagnostics\n    &gt;&gt;&gt; config = model_diagnostics.get_config()\n    &gt;&gt;&gt; config.keys()\n    dict_keys([...])\n    \"\"\"\n    # Return a copy of the global config so that users will\n    # not be able to modify the configuration with the returned dict.\n    return _global_config.copy()\n</code></pre>"},{"location":"reference/model_diagnostics/#model_diagnostics.set_config","title":"<code>set_config(plot_backend=None)</code>","text":"<p>Set global model-diagnostics configuration.</p> <p>Parameters:</p> Name Type Description Default <code>plot_backend</code> <code>bool</code> <p>The library used for plotting. Can be \"matplotlib\" or \"plotly\". If None, the existing value won't change. Global default: \"matplotlib\".</p> <code>None</code> See Also <p>config_context : Context manager for global scikit-learn configuration. get_config : Retrieve current values of the global configuration.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from model_diagnostics import set_config\n&gt;&gt;&gt; set_config(plot_backend=\"plotly\")\n</code></pre> Source code in <code>src/model_diagnostics/_config.py</code> <pre><code>def set_config(\n    plot_backend: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Set global model-diagnostics configuration.\n\n    Parameters\n    ----------\n    plot_backend : bool, default=None\n        The library used for plotting. Can be \"matplotlib\" or \"plotly\".\n        If None, the existing value won't change. Global default: \"matplotlib\".\n\n    See Also\n    --------\n    config_context : Context manager for global scikit-learn configuration.\n    get_config : Retrieve current values of the global configuration.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from model_diagnostics import set_config\n    &gt;&gt;&gt; set_config(plot_backend=\"plotly\")  # doctest: +SKIP\n    \"\"\"\n    if plot_backend not in (None, \"matplotlib\", \"plotly\"):\n        msg = f\"The plot_backend must be matplotlib or plotly, got {plot_backend}.\"\n        raise ValueError(msg)\n    if plot_backend == \"plotly\" and not find_spec(\"plotly\"):\n        msg = (\n            \"In order to set the plot backend to plotly, plotly must be installed, \"\n            \"i.e. via `pip install plotly`.\"\n        )\n        raise ModuleNotFoundError(msg)\n\n    if plot_backend is not None:\n        _global_config[\"plot_backend\"] = plot_backend\n</code></pre>"},{"location":"reference/model_diagnostics/#model_diagnostics.config_context","title":"<code>config_context(*, plot_backend=None)</code>","text":"<p>Context manager for global model-diagnostics configuration.</p> <p>Parameters:</p> Name Type Description Default <code>plot_backend</code> <code>bool</code> <p>The library used for plotting. Can be \"matplotlib\" or \"plotly\". If None, the existing value won't change. Global default: \"matplotlib\".</p> <code>None</code> <p>Yields:</p> Type Description <code>None.</code> See Also <p>set_config : Set global model-diagnostics configuration. get_config : Retrieve current values of the global configuration.</p> Notes <p>All settings, not just those presently modified, will be returned to their previous values when the context manager is exited.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import model_diagnostics\n&gt;&gt;&gt; from model_diagnostics.calibration import plot_reliability_diagram\n&gt;&gt;&gt; with model_diagnostics.config_context(plot_backend=\"plotly\"):\n...    plot_reliability_diagram(y_obs=[0, 1], y_pred=[0.3, 0.7])\n</code></pre> Source code in <code>src/model_diagnostics/_config.py</code> <pre><code>@contextmanager\ndef config_context(\n    *,\n    plot_backend: Optional[str] = None,\n) -&gt; Iterator[None]:\n    \"\"\"Context manager for global model-diagnostics configuration.\n\n    Parameters\n    ----------\n    plot_backend : bool, default=None\n        The library used for plotting. Can be \"matplotlib\" or \"plotly\".\n        If None, the existing value won't change. Global default: \"matplotlib\".\n\n    Yields\n    ------\n    None.\n\n    See Also\n    --------\n    set_config : Set global model-diagnostics configuration.\n    get_config : Retrieve current values of the global configuration.\n\n    Notes\n    -----\n    All settings, not just those presently modified, will be returned to\n    their previous values when the context manager is exited.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import model_diagnostics\n    &gt;&gt;&gt; from model_diagnostics.calibration import plot_reliability_diagram\n    &gt;&gt;&gt; with model_diagnostics.config_context(plot_backend=\"plotly\"):  # doctest: +SKIP\n    ...    plot_reliability_diagram(y_obs=[0, 1], y_pred=[0.3, 0.7])\n    \"\"\"\n    old_config = get_config()\n    set_config(\n        plot_backend=plot_backend,\n    )\n\n    try:\n        yield\n    finally:\n        set_config(**old_config)\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/","title":"calibration","text":""},{"location":"reference/model_diagnostics/calibration/#model_diagnostics.calibration","title":"<code>calibration</code>","text":""},{"location":"reference/model_diagnostics/calibration/#model_diagnostics.calibration.add_marginal_subplot","title":"<code>add_marginal_subplot(subfig, fig, row, col)</code>","text":"<p>Add a plotly subplot from plot_marginal to a multi-plot figure.</p> <p>This auxiliary function is accompanies <code>plot_marginal</code> in order to ease plotting with subfigures with the plotly backend.</p> <p>For it to work, you must call plotly's <code>make_subplots</code> with the <code>specs</code> argument and set the appropriate number of <code>{\"secondary_y\": True}</code> in a list of lists. <pre><code>from plotly.subplots import make_subplots\n\nn_rows, n_cols = ...\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    specs=[[{\"secondary_y\": True}] * n_cols] * n_rows,  # This is important!\n)\n</code></pre> The reason is that <code>plot_marginal</code> uses a secondary yaxis (and swapped sides with the primary yaxis).</p> <p>Parameters:</p> Name Type Description Default <code>subfig</code> <code>plotly Figure</code> <p>The subfigure which is added to <code>fig</code>.</p> required <code>fig</code> <code>plotly Figure</code> <p>The multi-plot figure to which <code>subfig</code> is added at positions <code>row</code> and <code>col</code>.</p> required <code>row</code> <code>int</code> <p>The (0-based) row index of <code>fig</code> at which <code>subfig</code> is added.</p> required <code>col</code> <code>int</code> <p>The (0-based) column index of <code>fig</code> at which <code>subfig</code> is added.</p> required <p>Returns:</p> Type Description <code>fig</code> <p>The plotly figure <code>fig</code>.</p> Source code in <code>src/model_diagnostics/calibration/plots.py</code> <pre><code>def add_marginal_subplot(subfig, fig, row: int, col: int):\n    \"\"\"Add a plotly subplot from plot_marginal to a multi-plot figure.\n\n    This auxiliary function is accompanies\n    [`plot_marginal`][model_diagnostics.calibration.plot_marginal] in order to ease\n    plotting with subfigures with the plotly backend.\n\n    For it to work, you must call plotly's `make_subplots` with the `specs` argument\n    and set the appropriate number of `{\"secondary_y\": True}` in a list of lists.\n    ```py hl_lines=\"7\"\n    from plotly.subplots import make_subplots\n\n    n_rows, n_cols = ...\n    fig = make_subplots(\n        rows=n_rows,\n        cols=n_cols,\n        specs=[[{\"secondary_y\": True}] * n_cols] * n_rows,  # This is important!\n    )\n    ```\n    The reason is that `plot_marginal` uses a secondary yaxis (and swapped sides with\n    the primary yaxis).\n\n    Parameters\n    ----------\n    subfig : plotly Figure\n        The subfigure which is added to `fig`.\n    fig : plotly Figure\n        The multi-plot figure to which `subfig` is added at positions `row` and `col`.\n    row : int\n        The (0-based) row index of `fig` at which `subfig` is added.\n    col : int\n        The (0-based) column index of `fig` at which `subfig` is added.\n\n    Returns\n    -------\n    fig\n        The plotly figure `fig`.\n    \"\"\"\n    # It returns a tuple of `range`s starting at 1.\n    plotly_rows, plotly_cols = fig._get_subplot_rows_columns()  # noqa: SLF001\n    n_rows = len(plotly_rows)\n    n_cols = len(plotly_cols)\n    if row &gt;= n_rows or col &gt;= n_cols:\n        msg = (\n            f\"The `fig` only has {n_rows} rows and {n_cols} columns. You specified \"\n            f\"(0-based) {row=} and {col=}.\"\n        )\n        raise ValueError(msg)\n    i = n_cols * row + col\n    # Plotly uses 1-based indices:\n    row += 1\n    col += 1\n    # Transfer the x-axis titles of the subfig to fig.\n    xaxis = \"xaxis\" if i == 0 else f\"xaxis{i + 1}\"\n    fig[\"layout\"][xaxis][\"title\"] = subfig[\"layout\"][\"xaxis\"][\"title\"]\n    # Change sides of y-axis.\n    yaxis = \"yaxis\" if i == 0 else f\"yaxis{2 * i + 1}\"\n    yaxis2 = f\"yaxis{2 * (i + 1)}\"\n    fig.update_layout(\n        **{\n            yaxis: {\"side\": \"right\", \"showgrid\": False},\n            yaxis2: {\"side\": \"left\", \"title\": \"y\"},\n        }\n    )\n    # Only the last added subfig should show the legends, but all the ones before\n    # should not.\n    # So don't show legends for row-1 and col-1.\n    if row &gt; 1:\n        fig.update_traces(patch={\"showlegend\": False}, row=row - 1, col=col)\n    if col &gt; 1:\n        fig.update_traces(patch={\"showlegend\": False}, row=row, col=col - 1)\n    for d in subfig.data:\n        fig.add_trace(d, row=row, col=col, secondary_y=d[\"yaxis\"] == \"y2\")\n\n    fig.update_layout(title=subfig.layout.title.text)\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/#model_diagnostics.calibration.compute_bias","title":"<code>compute_bias(y_obs, y_pred, feature=None, weights=None, *, functional='mean', level=0.5, n_bins=10, bin_method='sturges')</code>","text":"<p>Compute generalised bias conditional on a feature.</p> <p>This function computes and aggregates the generalised bias, i.e. the values of the canonical identification function, versus (grouped by) a feature. This is a good way to assess whether a model is conditionally calibrated or not. Well calibrated models have bias terms around zero. For the mean functional, the generalised bias is the negative residual <code>y_pred - y_obs</code>. See Notes for further details.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>feature</code> <code>array-like of shape (n_obs) or None</code> <p>Some feature column.</p> <code>None</code> <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights. If given, the bias is calculated as weighted average of the identification function with these weights. Note that the standard errors and p-values in the output are based on the assumption that the variance of the bias is inverse proportional to the weights. See the Notes section for details.</p> <code>None</code> <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile of quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> <code>n_bins</code> <code>int</code> <p>The number of bins, at least 2. For numerical features, <code>n_bins</code> only applies when <code>bin_method</code> is set to <code>\"quantile\"</code> or <code>\"uniform\"</code>. For string-like and categorical features, the most frequent values are taken. Ties are dealt with by taking the first value in natural sorting order. The remaining values are merged into <code>\"other n\"</code> with <code>n</code> indicating the unique count.</p> <p>I present, null values are always included in the output, accounting for one bin. NaN values are treated as null values.</p> <code>10</code> <code>bin_method</code> <code>str</code> <p>The method for finding bin edges (boundaries). Options using <code>n_bins</code> are:</p> <ul> <li><code>\"quantile\"</code></li> <li><code>\"uniform\"</code></li> </ul> <p>Options automatically selecting the number of bins for numerical features thereby using uniform bins are same options as numpy.histogram_bin_edges:</p> <ul> <li><code>\"auto\"</code> Minimum bin width between the <code>\"sturges\"</code> and <code>\"fd\"</code> estimators. Provides good all-around performance.</li> <li><code>\"fd\"</code> (Freedman Diaconis Estimator) Robust (resilient to outliers) estimator that takes into account data variability and data size.</li> <li><code>\"doane\"</code> An improved version of Sturges' estimator that works better with non-normal datasets.</li> <li><code>\"scott\"</code> Less robust estimator that takes into account data variability and data size.</li> <li><code>\"stone\"</code> Estimator based on leave-one-out cross-validation estimate of the integrated squared error. Can be regarded as a generalization of Scott's rule.</li> <li><code>\"rice\"</code> Estimator does not take variability into account, only data size. Commonly overestimates number of bins required.</li> <li><code>\"sturges\"</code> R's default method, only accounts for data size. Only optimal for gaussian data and underestimates number of bins for large non-gaussian datasets.</li> <li><code>\"sqrt\"</code> Square root (of data size) estimator, used by Excel and other programs for its speed and simplicity.</li> </ul> <code>'sturges'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The result table contains at least the columns:</p> <ul> <li><code>bias_mean</code>: Mean of the bias</li> <li><code>bias_cout</code>: Number of data rows</li> <li><code>bias_weights</code>: Sum of weights</li> <li><code>bias_stderr</code>: Standard error, i.e. standard deviation of <code>bias_mean</code></li> <li><code>p_value</code>: p-value of the 2-sided t-test with null hypothesis:   <code>bias_mean = 0</code></li> </ul> <p>If <code>feautre</code> is not None, then there is also the column:</p> <ul> <li><code>feature_name</code>: The actual name of the feature with the (binned) feature   values.</li> </ul> Notes <p> A model  A model \\(m(X)\\) is conditionally calibrated iff \\(\\mathbb{E}(V(m(X), Y)|X)=0\\) almost surely with canonical identification function \\(V\\). The empirical version, given some data, reads \\(\\bar{V} = \\frac{1}{n}\\sum_i \\phi(x_i) V(m(x_i), y_i)\\) with a test function \\(\\phi(x_i)\\) that projects on the specified feature. For a feature with only two distinct values <code>\"a\"</code> and <code>\"b\"</code>, this becomes \\(\\bar{V} = \\frac{1}{n_a}\\sum_{i \\text{ with }x_i=a} V(m(a), y_i)\\) with \\(n_a=\\sum_{i \\text{ with }x_i=a}\\) and similar for <code>\"b\"</code>. With case weights, this reads \\(\\bar{V} = \\frac{1}{\\sum_i w_i}\\sum_i w_i \\phi(x_i) V(m(x_i), y_i)\\). This generalises the classical residual (up to a minus sign) for target functionals other than the mean. See <code>[FLM2022]</code>.</p> <p>The standard error for \\(\\bar{V}\\) is calculated in the standard way as \\(\\mathrm{SE} = \\sqrt{\\operatorname{Var}(\\bar{V})} = \\frac{\\sigma}{\\sqrt{n}}\\) and the standard variance estimator for \\(\\sigma^2 = \\operatorname{Var}(\\phi(x_i) V(m(x_i), y_i))\\) with Bessel correction, i.e. division by \\(n-1\\) instead of \\(n\\).</p> <p>With case weights, the variance estimator becomes \\(\\operatorname{Var}(\\bar{V}) = \\frac{1}{n-1} \\frac{1}{\\sum_i w_i} \\sum_i w_i (V(m(x_i), y_i) - \\bar{V})^2\\) with the implied relation \\(\\operatorname{Var}(V(m(x_i), y_i)) \\sim \\frac{1}{w_i} \\). If your weights are for repeated observations, so-called frequency weights, then the above estimate is conservative because it uses \\(n - 1\\) instead of \\((\\sum_i w_i) - 1\\).</p> References <code>[FLM2022]</code> <p>T. Fissler, C. Lorentzen, and M. Mayer. \"Model Comparison and Calibration Assessment\". (2022) arxiv:2202.12780.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; compute_bias(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\nshape: (1, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bias_mean \u2506 bias_count \u2506 bias_weights \u2506 bias_stderr \u2506 p_value  \u2502\n\u2502 ---       \u2506 ---        \u2506 ---          \u2506 ---         \u2506 ---      \u2502\n\u2502 f64       \u2506 u32        \u2506 f64          \u2506 f64         \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.25      \u2506 4          \u2506 4.0          \u2506 0.478714    \u2506 0.637618 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; compute_bias(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2],\n... feature=[\"a\", \"a\", \"b\", \"b\"])\nshape: (2, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 feature \u2506 bias_mean \u2506 bias_count \u2506 bias_weights \u2506 bias_stderr \u2506 p_value \u2502\n\u2502 ---     \u2506 ---       \u2506 ---        \u2506 ---          \u2506 ---         \u2506 ---     \u2502\n\u2502 str     \u2506 f64       \u2506 u32        \u2506 f64          \u2506 f64         \u2506 f64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a       \u2506 0.0       \u2506 2          \u2506 2.0          \u2506 1.0         \u2506 1.0     \u2502\n\u2502 b       \u2506 0.5       \u2506 2          \u2506 2.0          \u2506 0.5         \u2506 0.5     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>src/model_diagnostics/calibration/identification.py</code> <pre><code>def compute_bias(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    feature: Optional[Union[npt.ArrayLike, pl.Series]] = None,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    functional: str = \"mean\",\n    level: float = 0.5,\n    n_bins: int = 10,\n    bin_method: str = \"sturges\",\n):\n    r\"\"\"Compute generalised bias conditional on a feature.\n\n    This function computes and aggregates the generalised bias, i.e. the values of the\n    canonical identification function, versus (grouped by) a feature.\n    This is a good way to assess whether a model is conditionally calibrated or not.\n    Well calibrated models have bias terms around zero.\n    For the mean functional, the generalised bias is the negative residual\n    `y_pred - y_obs`.\n    See [Notes](#notes) for further details.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    feature : array-like of shape (n_obs) or None\n        Some feature column.\n    weights : array-like of shape (n_obs) or None\n        Case weights. If given, the bias is calculated as weighted average of the\n        identification function with these weights.\n        Note that the standard errors and p-values in the output are based on the\n        assumption that the variance of the bias is inverse proportional to the\n        weights. See the Notes section for details.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n\n    level : float\n        The level of the expectile of quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n    n_bins : int\n        The number of bins, at least 2. For numerical features, `n_bins` only applies\n        when `bin_method` is set to `\"quantile\"` or `\"uniform\"`.\n        For string-like and categorical features, the most frequent values are taken.\n        Ties are dealt with by taking the first value in natural sorting order.\n        The remaining values are merged into `\"other n\"` with `n` indicating the unique\n        count.\n\n        I present, null values are always included in the output, accounting for one\n        bin. NaN values are treated as null values.\n    bin_method : str\n        The method for finding bin edges (boundaries). Options using `n_bins` are:\n\n        - `\"quantile\"`\n        - `\"uniform\"`\n\n        Options automatically selecting the number of bins for numerical features\n        thereby using uniform bins are same options as\n        [numpy.histogram_bin_edges](https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html):\n\n        - `\"auto\"`\n        Minimum bin width between the `\"sturges\"` and `\"fd\"` estimators. Provides good\n        all-around performance.\n        - `\"fd\"` (Freedman Diaconis Estimator)\n        Robust (resilient to outliers) estimator that takes into account data\n        variability and data size.\n        - `\"doane\"`\n        An improved version of Sturges' estimator that works better with non-normal\n        datasets.\n        - `\"scott\"`\n        Less robust estimator that takes into account data variability and data size.\n        - `\"stone\"`\n        Estimator based on leave-one-out cross-validation estimate of the integrated\n        squared error. Can be regarded as a generalization of Scott's rule.\n        - `\"rice\"`\n        Estimator does not take variability into account, only data size. Commonly\n        overestimates number of bins required.\n        - `\"sturges\"`\n        R's default method, only accounts for data size. Only optimal for gaussian data\n        and underestimates number of bins for large non-gaussian datasets.\n        - `\"sqrt\"`\n        Square root (of data size) estimator, used by Excel and other programs for its\n        speed and simplicity.\n\n    Returns\n    -------\n    df : polars.DataFrame\n        The result table contains at least the columns:\n\n        - `bias_mean`: Mean of the bias\n        - `bias_cout`: Number of data rows\n        - `bias_weights`: Sum of weights\n        - `bias_stderr`: Standard error, i.e. standard deviation of `bias_mean`\n        - `p_value`: p-value of the 2-sided t-test with null hypothesis:\n          `bias_mean = 0`\n\n        If `feautre ` is not None, then there is also the column:\n\n        - `feature_name`: The actual name of the feature with the (binned) feature\n          values.\n\n    Notes\n    -----\n    [](){#notes}\n    A model \\(m(X)\\) is conditionally calibrated iff\n    \\(\\mathbb{E}(V(m(X), Y)|X)=0\\) almost surely with canonical identification\n    function \\(V\\).\n    The empirical version, given some data, reads\n    \\(\\bar{V} = \\frac{1}{n}\\sum_i \\phi(x_i) V(m(x_i), y_i)\\) with a test function\n    \\(\\phi(x_i)\\) that projects on the specified feature.\n    For a feature with only two distinct values `\"a\"` and `\"b\"`, this becomes\n    \\(\\bar{V} = \\frac{1}{n_a}\\sum_{i \\text{ with }x_i=a} V(m(a), y_i)\\) with\n    \\(n_a=\\sum_{i \\text{ with }x_i=a}\\) and similar for `\"b\"`.\n    With case weights, this reads\n    \\(\\bar{V} = \\frac{1}{\\sum_i w_i}\\sum_i w_i \\phi(x_i) V(m(x_i), y_i)\\).\n    This generalises the classical residual (up to a minus sign) for target functionals\n    other than the mean. See `[FLM2022]`.\n\n    The standard error for \\(\\bar{V}\\) is calculated in the standard way as\n    \\(\\mathrm{SE} = \\sqrt{\\operatorname{Var}(\\bar{V})} = \\frac{\\sigma}{\\sqrt{n}}\\) and\n    the standard variance estimator for \\(\\sigma^2 = \\operatorname{Var}(\\phi(x_i)\n    V(m(x_i), y_i))\\) with Bessel correction, i.e. division by \\(n-1\\) instead of\n    \\(n\\).\n\n    With case weights, the variance estimator becomes \\(\\operatorname{Var}(\\bar{V})\n    = \\frac{1}{n-1} \\frac{1}{\\sum_i w_i} \\sum_i w_i (V(m(x_i), y_i) - \\bar{V})^2\\) with\n    the implied relation \\(\\operatorname{Var}(V(m(x_i), y_i)) \\sim \\frac{1}{w_i} \\).\n    If your weights are for repeated observations, so-called frequency weights, then\n    the above estimate is conservative because it uses \\(n - 1\\) instead\n    of \\((\\sum_i w_i) - 1\\).\n\n    References\n    ----------\n    `[FLM2022]`\n\n    :   T. Fissler, C. Lorentzen, and M. Mayer.\n        \"Model Comparison and Calibration Assessment\". (2022)\n        [arxiv:2202.12780](https://arxiv.org/abs/2202.12780).\n\n    Examples\n    --------\n    &gt;&gt;&gt; compute_bias(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n    shape: (1, 5)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 bias_mean \u2506 bias_count \u2506 bias_weights \u2506 bias_stderr \u2506 p_value  \u2502\n    \u2502 ---       \u2506 ---        \u2506 ---          \u2506 ---         \u2506 ---      \u2502\n    \u2502 f64       \u2506 u32        \u2506 f64          \u2506 f64         \u2506 f64      \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 0.25      \u2506 4          \u2506 4.0          \u2506 0.478714    \u2506 0.637618 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    &gt;&gt;&gt; compute_bias(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2],\n    ... feature=[\"a\", \"a\", \"b\", \"b\"])\n    shape: (2, 6)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 feature \u2506 bias_mean \u2506 bias_count \u2506 bias_weights \u2506 bias_stderr \u2506 p_value \u2502\n    \u2502 ---     \u2506 ---       \u2506 ---        \u2506 ---          \u2506 ---         \u2506 ---     \u2502\n    \u2502 str     \u2506 f64       \u2506 u32        \u2506 f64          \u2506 f64         \u2506 f64     \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 a       \u2506 0.0       \u2506 2          \u2506 2.0          \u2506 1.0         \u2506 1.0     \u2502\n    \u2502 b       \u2506 0.5       \u2506 2          \u2506 2.0          \u2506 0.5         \u2506 0.5     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    validate_same_first_dimension(y_obs, y_pred)\n    n_pred = length_of_second_dimension(y_pred)\n    pred_names, _ = get_sorted_array_names(y_pred)\n\n    if weights is not None:\n        validate_same_first_dimension(weights, y_obs)\n        w = np.asarray(weights)\n        if w.ndim &gt; 1:\n            msg = f\"The array weights must be 1-dimensional, got weights.ndim={w.ndim}.\"\n            raise ValueError(msg)\n    else:\n        w = np.ones_like(y_obs, dtype=float)\n\n    n_obs = length_of_first_dimension(y_pred)\n    df_list = []\n    with pl.StringCache():\n        feature_name = None\n        if feature is not None:\n            feature, n_bins, f_binned = bin_feature(\n                feature=feature,\n                feature_name=None,\n                n_obs=n_obs,\n                n_bins=n_bins,\n                bin_method=bin_method,\n            )\n            feature_name = feature.name\n            is_cat_or_string = feature.dtype in [\n                pl.Categorical,\n                pl.Enum,\n                pl.Utf8,\n                pl.Object,\n            ]\n\n        for i in range(len(pred_names)):\n            # Loop over columns of y_pred.\n            x = y_pred if n_pred == 0 else get_second_dimension(y_pred, i)\n\n            bias = identification_function(\n                y_obs=y_obs,\n                y_pred=x,\n                functional=functional,\n                level=level,\n            )\n\n            if feature is None:\n                bias_mean = np.average(bias, weights=w)\n                bias_weights = np.sum(w)\n                bias_count = bias.shape[0]\n                # Note: with Bessel correction\n                bias_stddev = np.average((bias - bias_mean) ** 2, weights=w) / np.amax(\n                    [1, bias_count - 1]\n                )\n                df = pl.DataFrame(\n                    {\n                        \"bias_mean\": [bias_mean],\n                        \"bias_count\": pl.Series([bias_count], dtype=pl.UInt32),\n                        \"bias_weights\": [bias_weights],\n                        \"bias_stderr\": [np.sqrt(bias_stddev)],\n                    }\n                )\n            else:\n                df = pl.DataFrame(\n                    {\n                        \"y_obs\": y_obs,\n                        \"y_pred\": x,\n                        feature_name: feature,\n                        \"bias\": bias,\n                        \"weights\": w,\n                    }\n                )\n\n                agg_list = [\n                    pl.col(\"bias_mean\").first(),\n                    pl.count(\"bias\").alias(\"bias_count\"),\n                    pl.col(\"weights\").sum().alias(\"bias_weights\"),\n                    (\n                        (\n                            pl.col(\"weights\")\n                            * ((pl.col(\"bias\") - pl.col(\"bias_mean\")) ** 2)\n                        ).sum()\n                        / pl.col(\"weights\").sum()\n                    ).alias(\"variance\"),\n                ]\n\n                groupby_name = \"bin\"\n                df = df.hstack([f_binned.get_column(\"bin\")])\n                if not is_cat_or_string:\n                    agg_list.append(pl.col(feature_name).mean())\n\n                df = (\n                    df.lazy()\n                    .select(\n                        pl.all(),\n                        (\n                            (pl.col(\"weights\") * pl.col(\"bias\"))\n                            .sum()\n                            .over(groupby_name)\n                            / pl.col(\"weights\").sum().over(groupby_name)\n                        ).alias(\"bias_mean\"),\n                    )\n                    .group_by(groupby_name)\n                    .agg(agg_list)\n                    .with_columns(\n                        [\n                            pl.when(pl.col(\"bias_count\") &gt; 1)\n                            .then(pl.col(\"variance\") / (pl.col(\"bias_count\") - 1))\n                            .otherwise(pl.col(\"variance\"))\n                            .sqrt()\n                            .alias(\"bias_stderr\"),\n                        ]\n                    )\n                )\n\n                if is_cat_or_string:\n                    df = df.with_columns(pl.col(groupby_name).alias(feature_name))\n\n                df = (\n                    df\n                    # With sort and head alone, we could lose the null value, but we\n                    # want to keep it.\n                    # .sort(\"bias_count\", descending=True)\n                    # .head(n_bins)\n                    .with_columns(\n                        pl.when(pl.col(feature_name).is_null())\n                        .then(pl.max(\"bias_count\") + 1)\n                        .otherwise(pl.col(\"bias_count\"))\n                        .alias(\"__priority\")\n                    )\n                    .sort(\"__priority\", descending=True)\n                    .head(n_bins)\n                    .sort(feature_name, descending=False)\n                    .select(\n                        pl.col(feature_name),\n                        pl.col(\"bias_mean\"),\n                        pl.col(\"bias_count\"),\n                        pl.col(\"bias_weights\"),\n                        pl.col(\"bias_stderr\"),\n                    )\n                ).collect()\n\n            # Add column with p-value of 2-sided t-test.\n            # We explicitly convert \"to_numpy\", because otherwise we get:\n            #   RuntimeWarning: A builtin ctypes object gave a PEP3118 format string\n            #   that does not match its itemsize, so a best-guess will be made of the\n            #   data type. Newer versions of python may behave correctly.\n            stderr_ = df.get_column(\"bias_stderr\")\n            p_value = np.full_like(stderr_, fill_value=np.nan)\n            n = df.get_column(\"bias_count\")\n            p_value[np.asarray((n &gt; 1) &amp; (stderr_ == 0), dtype=bool)] = 0\n            mask = stderr_ &gt; 0\n            x = df.get_column(\"bias_mean\").filter(mask).to_numpy()\n            n = df.get_column(\"bias_count\").filter(mask).to_numpy()\n            stderr = stderr_.filter(mask).to_numpy()\n            # t-statistic t (-|t| and factor of 2 because of 2-sided test)\n            p_value[np.asarray(mask, dtype=bool)] = 2 * special.stdtr(\n                n - 1,  # degrees of freedom\n                -np.abs(x / stderr),\n            )\n            df = df.with_columns(pl.Series(\"p_value\", p_value))\n\n            # Add column \"model\".\n            if n_pred &gt; 0:\n                model_col_name = \"model_\" if feature_name == \"model\" else \"model\"\n                df = df.with_columns(\n                    pl.Series(model_col_name, [pred_names[i]] * df.shape[0])\n                )\n\n            # Select the columns in the correct order.\n            col_selection = []\n            if n_pred &gt; 0:\n                col_selection.append(model_col_name)\n            if feature_name is not None and feature_name in df.columns:\n                col_selection.append(feature_name)\n            col_selection += [\n                \"bias_mean\",\n                \"bias_count\",\n                \"bias_weights\",\n                \"bias_stderr\",\n                \"p_value\",\n            ]\n            df_list.append(df.select(col_selection))\n\n        df = pl.concat(df_list)\n    return df\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/#model_diagnostics.calibration.compute_marginal","title":"<code>compute_marginal(y_obs, y_pred, X=None, feature_name=None, predict_function=None, weights=None, *, n_bins=10, bin_method='sturges', n_max=1000, rng=None)</code>","text":"<p>Compute the marginal expectation conditional on a single feature.</p> <p>This function computes the (weighted) average of observed response and predictions conditional on a given feature.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>X</code> <code>array-like of shape (n_obs, n_features) or None</code> <p>The dataframe or array of features to be passed to the model predict function.</p> <code>None</code> <code>feature_name</code> <code>(int, str or None)</code> <p>Column name (str) or index (int) of feature in <code>X</code>. If None, the total marginal is computed.</p> <code>None</code> <code>predict_function</code> <code>callable or None</code> <p>A callable to get prediction, i.e. <code>predict_function(X)</code>. Used to compute partial dependence. If <code>None</code>, partial dependence is omitted.</p> <code>None</code> <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights. If given, the bias is calculated as weighted average of the identification function with these weights.</p> <code>None</code> <code>n_bins</code> <code>int</code> <p>The number of bins, at least 2. For numerical features, <code>n_bins</code> only applies when <code>bin_method</code> is set to <code>\"quantile\"</code> or <code>\"uniform\"</code>. For string-like and categorical features, the most frequent values are taken. Ties are dealt with by taking the first value in natural sorting order. The remaining values are merged into <code>\"other n\"</code> with <code>n</code> indicating the unique count.</p> <p>I present, null values are always included in the output, accounting for one bin. NaN values are treated as null values.</p> <code>10</code> <code>bin_method</code> <code>str</code> <p>The method for finding bin edges (boundaries). Options using <code>n_bins</code> are:</p> <ul> <li><code>\"quantile\"</code></li> <li><code>\"uniform\"</code></li> </ul> <p>Options automatically selecting the number of bins for numerical features thereby using uniform bins are same options as numpy.histogram_bin_edges:</p> <ul> <li><code>\"auto\"</code> Minimum bin width between the <code>\"sturges\"</code> and <code>\"fd\"</code> estimators. Provides good all-around performance.</li> <li><code>\"fd\"</code> (Freedman Diaconis Estimator) Robust (resilient to outliers) estimator that takes into account data variability and data size.</li> <li><code>\"doane\"</code> An improved version of Sturges' estimator that works better with non-normal datasets.</li> <li><code>\"scott\"</code> Less robust estimator that takes into account data variability and data size.</li> <li><code>\"stone\"</code> Estimator based on leave-one-out cross-validation estimate of the integrated squared error. Can be regarded as a generalization of Scott's rule.</li> <li><code>\"rice\"</code> Estimator does not take variability into account, only data size. Commonly overestimates number of bins required.</li> <li><code>\"sturges\"</code> R's default method, only accounts for data size. Only optimal for gaussian data and underestimates number of bins for large non-gaussian datasets.</li> <li><code>\"sqrt\"</code> Square root (of data size) estimator, used by Excel and other programs for its speed and simplicity.</li> </ul> <code>'sturges'</code> <code>n_max</code> <code>int or None</code> <p>Used only for partial dependence computation. The number of rows to subsample from X. This speeds up computation, in particular for slow predict functions.</p> <code>1000</code> <code>rng</code> <code>(Generator, int or None)</code> <p>Used only for partial dependence computation. The random number generator used for subsampling of <code>n_max</code> rows. The input is internally wrapped by <code>np.random.default_rng(rng)</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The result table contains at least the columns:</p> <ul> <li><code>y_obs_mean</code>: Mean of <code>y_obs</code></li> <li><code>y_pred_mean</code>: Mean of <code>y_pred</code></li> <li><code>y_obs_stderr</code>: Standard error, i.e. standard deviation of <code>y_obs_mean</code></li> <li><code>y_pred_stderr</code>: Standard error, i.e. standard deviation of <code>y_pred_mean</code></li> <li><code>count</code>: Number of data rows</li> <li><code>weights</code>: Sum of weights</li> </ul> <p>If <code>feature</code> is not None, then there is also the column:</p> <ul> <li><code>feature_name</code>: The actual name of the feature with the (binned) feature   values.</li> </ul> <p>If <code>feature</code> is numerical, one also has:</p> <ul> <li><code>bin_edges</code>: The edges and standard deviation of the bins, i.e.   (min, std, max).</li> </ul> Notes <p>The marginal values are computed as an estimation of:</p> <ul> <li><code>y_obs</code>: \\(\\mathbb{E}(Y|feature)\\)</li> <li><code>y_pred</code>: \\(\\mathbb{E}(m(X)|feature)\\)</li> </ul> <p>with \\(feature\\) the column specified by <code>feature_name</code>. Computationally that is more or less a group-by-aggregate operation on a dataset.</p> <p>The standard error for both are calculated in the standard way as \\(\\mathrm{SE} = \\sqrt{\\operatorname{Var}(\\bar{Y})} = \\frac{\\sigma}{\\sqrt{n}}\\) and the standard variance estimator for \\(\\sigma^2\\) with Bessel correction, i.e. division by \\(n-1\\) instead of \\(n\\).</p> <p>With case weights, the variance estimator becomes \\(\\operatorname{Var}(\\bar{Y}) = \\frac{1}{n-1} \\frac{1}{\\sum_i w_i} \\sum_i w_i (y_i - \\bar{y})^2\\) with the implied relation \\(\\operatorname{Var}(y_i) \\sim \\frac{1}{w_i} \\). If your weights are for repeated observations, so-called frequency weights, then the above estimate is conservative because it uses \\(n - 1\\) instead of \\((\\sum_i w_i) - 1\\).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; compute_marginal(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1, 2])\nshape: (1, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 y_obs_mean \u2506 y_pred_mean \u2506 y_obs_stderr \u2506 y_pred_stderr \u2506 count \u2506 weights \u2502\n\u2502 ---        \u2506 ---         \u2506 ---          \u2506 ---           \u2506 ---   \u2506 ---     \u2502\n\u2502 f64        \u2506 f64         \u2506 f64          \u2506 f64           \u2506 u32   \u2506 f64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.5        \u2506 0.75        \u2506 0.288675     \u2506 0.629153      \u2506 4     \u2506 4.0     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; pl.Config.set_tbl_width_chars(84)\n&lt;class 'polars.config.Config'&gt;\n&gt;&gt;&gt; y_obs, X =[0, 0, 1, 1], [[0, 1], [1, 1], [2, 2], [3, 2]]\n&gt;&gt;&gt; m = Ridge().fit(X, y_obs)\n&gt;&gt;&gt; compute_marginal(y_obs=y_obs, y_pred=m.predict(X), X=X, feature_name=0,\n... predict_function=m.predict)\nshape: (3, 9)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 feature  \u2506 y_obs_m \u2506 y_pred_ \u2506 y_obs_s \u2506 \u2026 \u2506 count \u2506 weights \u2506 bin_edg \u2506 partial \u2502\n\u2502 0        \u2506 ean     \u2506 mean    \u2506 tderr   \u2506   \u2506 ---   \u2506 ---     \u2506 es      \u2506 _depend \u2502\n\u2502 ---      \u2506 ---     \u2506 ---     \u2506 ---     \u2506   \u2506 u32   \u2506 f64     \u2506 ---     \u2506 ence    \u2502\n\u2502 f64      \u2506 f64     \u2506 f64     \u2506 f64     \u2506   \u2506       \u2506         \u2506 array[f \u2506 ---     \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 64, 3]  \u2506 f64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.5      \u2506 0.0     \u2506 0.125   \u2506 0.0     \u2506 \u2026 \u2506 2     \u2506 2.0     \u2506 [0.0,   \u2506 0.25    \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.5,    \u2506         \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 1.0]    \u2506         \u2502\n\u2502 2.0      \u2506 1.0     \u2506 0.75    \u2506 0.0     \u2506 \u2026 \u2506 1     \u2506 1.0     \u2506 [1.0,   \u2506 0.625   \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.0,    \u2506         \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 2.0]    \u2506         \u2502\n\u2502 3.0      \u2506 1.0     \u2506 1.0     \u2506 0.0     \u2506 \u2026 \u2506 1     \u2506 1.0     \u2506 [2.0,   \u2506 0.875   \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.0,    \u2506         \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 3.0]    \u2506         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>src/model_diagnostics/calibration/identification.py</code> <pre><code>def compute_marginal(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    X: Optional[npt.ArrayLike] = None,\n    feature_name: Optional[Union[str, int]] = None,\n    predict_function: Optional[Callable] = None,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    n_bins: int = 10,\n    bin_method: str = \"sturges\",\n    n_max: int = 1000,\n    rng: Optional[Union[np.random.Generator, int]] = None,\n):\n    r\"\"\"Compute the marginal expectation conditional on a single feature.\n\n    This function computes the (weighted) average of observed response and predictions\n    conditional on a given feature.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    X : array-like of shape (n_obs, n_features) or None\n        The dataframe or array of features to be passed to the model predict function.\n    feature_name : int, str or None\n        Column name (str) or index (int) of feature in `X`. If None, the total marginal\n        is computed.\n    predict_function : callable or None\n        A callable to get prediction, i.e. `predict_function(X)`. Used to compute\n        partial dependence. If `None`, partial dependence is omitted.\n    weights : array-like of shape (n_obs) or None\n        Case weights. If given, the bias is calculated as weighted average of the\n        identification function with these weights.\n    n_bins : int\n        The number of bins, at least 2. For numerical features, `n_bins` only applies\n        when `bin_method` is set to `\"quantile\"` or `\"uniform\"`.\n        For string-like and categorical features, the most frequent values are taken.\n        Ties are dealt with by taking the first value in natural sorting order.\n        The remaining values are merged into `\"other n\"` with `n` indicating the unique\n        count.\n\n        I present, null values are always included in the output, accounting for one\n        bin. NaN values are treated as null values.\n    bin_method : str\n        The method for finding bin edges (boundaries). Options using `n_bins` are:\n\n        - `\"quantile\"`\n        - `\"uniform\"`\n\n        Options automatically selecting the number of bins for numerical features\n        thereby using uniform bins are same options as\n        [numpy.histogram_bin_edges](https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html):\n\n        - `\"auto\"`\n        Minimum bin width between the `\"sturges\"` and `\"fd\"` estimators. Provides good\n        all-around performance.\n        - `\"fd\"` (Freedman Diaconis Estimator)\n        Robust (resilient to outliers) estimator that takes into account data\n        variability and data size.\n        - `\"doane\"`\n        An improved version of Sturges' estimator that works better with non-normal\n        datasets.\n        - `\"scott\"`\n        Less robust estimator that takes into account data variability and data size.\n        - `\"stone\"`\n        Estimator based on leave-one-out cross-validation estimate of the integrated\n        squared error. Can be regarded as a generalization of Scott's rule.\n        - `\"rice\"`\n        Estimator does not take variability into account, only data size. Commonly\n        overestimates number of bins required.\n        - `\"sturges\"`\n        R's default method, only accounts for data size. Only optimal for gaussian data\n        and underestimates number of bins for large non-gaussian datasets.\n        - `\"sqrt\"`\n        Square root (of data size) estimator, used by Excel and other programs for its\n        speed and simplicity.\n\n    n_max : int or None\n        Used only for partial dependence computation. The number of rows to subsample\n        from X. This speeds up computation, in particular for slow predict functions.\n    rng : np.random.Generator, int or None\n        Used only for partial dependence computation. The random number generator used\n        for subsampling of `n_max` rows. The input is internally wrapped by\n        `np.random.default_rng(rng)`.\n\n    Returns\n    -------\n    df : polars.DataFrame\n        The result table contains at least the columns:\n\n        - `y_obs_mean`: Mean of `y_obs`\n        - `y_pred_mean`: Mean of `y_pred`\n        - `y_obs_stderr`: Standard error, i.e. standard deviation of `y_obs_mean`\n        - `y_pred_stderr`: Standard error, i.e. standard deviation of `y_pred_mean`\n        - `count`: Number of data rows\n        - `weights`: Sum of weights\n\n        If `feature ` is not None, then there is also the column:\n\n        - `feature_name`: The actual name of the feature with the (binned) feature\n          values.\n\n        If `feature` is numerical, one also has:\n\n        - `bin_edges`: The edges and standard deviation of the bins, i.e.\n          (min, std, max).\n\n    Notes\n    -----\n    The marginal values are computed as an estimation of:\n\n    - `y_obs`: \\(\\mathbb{E}(Y|feature)\\)\n    - `y_pred`: \\(\\mathbb{E}(m(X)|feature)\\)\n\n    with \\(feature\\) the column specified by `feature_name`.\n    Computationally that is more or less a group-by-aggregate operation on a dataset.\n\n    The standard error for both are calculated in the standard way as\n    \\(\\mathrm{SE} = \\sqrt{\\operatorname{Var}(\\bar{Y})} = \\frac{\\sigma}{\\sqrt{n}}\\) and\n    the standard variance estimator for \\(\\sigma^2\\) with Bessel correction, i.e.\n    division by \\(n-1\\) instead of \\(n\\).\n\n    With case weights, the variance estimator becomes \\(\\operatorname{Var}(\\bar{Y})\n    = \\frac{1}{n-1} \\frac{1}{\\sum_i w_i} \\sum_i w_i (y_i - \\bar{y})^2\\) with\n    the implied relation \\(\\operatorname{Var}(y_i) \\sim \\frac{1}{w_i} \\).\n    If your weights are for repeated observations, so-called frequency weights, then\n    the above estimate is conservative because it uses \\(n - 1\\) instead\n    of \\((\\sum_i w_i) - 1\\).\n\n    Examples\n    --------\n    &gt;&gt;&gt; compute_marginal(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1, 2])\n    shape: (1, 6)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 y_obs_mean \u2506 y_pred_mean \u2506 y_obs_stderr \u2506 y_pred_stderr \u2506 count \u2506 weights \u2502\n    \u2502 ---        \u2506 ---         \u2506 ---          \u2506 ---           \u2506 ---   \u2506 ---     \u2502\n    \u2502 f64        \u2506 f64         \u2506 f64          \u2506 f64           \u2506 u32   \u2506 f64     \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 0.5        \u2506 0.75        \u2506 0.288675     \u2506 0.629153      \u2506 4     \u2506 4.0     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    &gt;&gt;&gt; import polars as pl\n    &gt;&gt;&gt; from sklearn.linear_model import Ridge\n    &gt;&gt;&gt; pl.Config.set_tbl_width_chars(84)  # doctest: +ELLIPSIS\n    &lt;class 'polars.config.Config'&gt;\n    &gt;&gt;&gt; y_obs, X =[0, 0, 1, 1], [[0, 1], [1, 1], [2, 2], [3, 2]]\n    &gt;&gt;&gt; m = Ridge().fit(X, y_obs)\n    &gt;&gt;&gt; compute_marginal(y_obs=y_obs, y_pred=m.predict(X), X=X, feature_name=0,\n    ... predict_function=m.predict)\n    shape: (3, 9)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 feature  \u2506 y_obs_m \u2506 y_pred_ \u2506 y_obs_s \u2506 \u2026 \u2506 count \u2506 weights \u2506 bin_edg \u2506 partial \u2502\n    \u2502 0        \u2506 ean     \u2506 mean    \u2506 tderr   \u2506   \u2506 ---   \u2506 ---     \u2506 es      \u2506 _depend \u2502\n    \u2502 ---      \u2506 ---     \u2506 ---     \u2506 ---     \u2506   \u2506 u32   \u2506 f64     \u2506 ---     \u2506 ence    \u2502\n    \u2502 f64      \u2506 f64     \u2506 f64     \u2506 f64     \u2506   \u2506       \u2506         \u2506 array[f \u2506 ---     \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 64, 3]  \u2506 f64     \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 0.5      \u2506 0.0     \u2506 0.125   \u2506 0.0     \u2506 \u2026 \u2506 2     \u2506 2.0     \u2506 [0.0,   \u2506 0.25    \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.5,    \u2506         \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 1.0]    \u2506         \u2502\n    \u2502 2.0      \u2506 1.0     \u2506 0.75    \u2506 0.0     \u2506 \u2026 \u2506 1     \u2506 1.0     \u2506 [1.0,   \u2506 0.625   \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.0,    \u2506         \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 2.0]    \u2506         \u2502\n    \u2502 3.0      \u2506 1.0     \u2506 1.0     \u2506 0.0     \u2506 \u2026 \u2506 1     \u2506 1.0     \u2506 [2.0,   \u2506 0.875   \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.0,    \u2506         \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 3.0]    \u2506         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    validate_same_first_dimension(y_obs, y_pred)\n    n_pred = length_of_second_dimension(y_pred)\n    pred_names, _ = get_sorted_array_names(y_pred)\n    y_obs = np.asarray(y_obs)\n\n    if weights is not None:\n        validate_same_first_dimension(weights, y_obs)\n        w = np.asarray(weights)\n        if w.ndim &gt; 1:\n            msg = f\"The array weights must be 1-dimensional, got weights.ndim={w.ndim}.\"\n            raise ValueError(msg)\n    else:\n        w = np.ones_like(y_obs, dtype=float)\n\n    if feature_name is None:\n        # X is completely ignored.\n        feature_input = feature = None\n    elif X is None:\n        msg = (\n            \"X must be a data container like a (polars) dataframe or an (numpy) array.\"\n        )\n        raise ValueError(msg)\n    elif not isinstance(feature_name, (int, str)):\n        msg = f\"The argument 'feature_name' must be an int or str; got {feature_name}\"\n        raise ValueError(msg)\n    elif isinstance(feature_name, int):\n        feature_index = feature_name\n        feature_input = get_second_dimension(X, feature_name)\n    else:\n        X_names, _ = get_sorted_array_names(X)\n        feature_index = X_names.index(feature_name)\n        feature_input = get_second_dimension(X, feature_index)\n\n    n_obs = length_of_first_dimension(y_pred)\n    df_list = []\n    with pl.StringCache():\n        if feature_input is not None:\n            feature, n_bins, f_binned = bin_feature(\n                feature=feature_input,\n                feature_name=feature_name,\n                n_obs=n_obs,\n                n_bins=n_bins,\n                bin_method=bin_method,\n            )\n            feature_name = feature.name\n            is_cat_or_string = feature.dtype in [\n                pl.Categorical,\n                pl.Enum,\n                pl.Utf8,\n                pl.Object,\n            ]\n\n        for i in range(len(pred_names)):\n            # Loop over columns of y_pred.\n            x = np.asarray(y_pred if n_pred == 0 else get_second_dimension(y_pred, i))\n\n            if feature is None:\n                y_obs_mean = np.average(y_obs, weights=w)\n                y_pred_mean = np.average(x, weights=w)\n                weights_sum = np.sum(w)\n                count = y_obs.shape[0]\n                # Note: with Bessel correction\n                y_obs_stddev = np.average(\n                    (y_obs - y_obs_mean) ** 2, weights=w\n                ) / np.amax([1, count - 1])\n                y_pred_stddev = np.average((x - y_pred_mean) ** 2, weights=w) / np.amax(\n                    [1, count - 1]\n                )\n                df = pl.DataFrame(\n                    {\n                        \"y_obs_mean\": [y_obs_mean],\n                        \"y_pred_mean\": [y_pred_mean],\n                        \"count\": pl.Series([count], dtype=pl.UInt32),\n                        \"weights\": [weights_sum],\n                        \"y_obs_stderr\": [np.sqrt(y_obs_stddev)],\n                        \"y_pred_stderr\": [np.sqrt(y_pred_stddev)],\n                    }\n                )\n            else:\n                df = pl.DataFrame(\n                    {\n                        \"y_obs\": y_obs,\n                        \"y_pred\": x,\n                        feature_name: feature,\n                        \"weights\": w,\n                    }\n                )\n\n                agg_list = [\n                    pl.count(\"y_obs\").alias(\"count\"),\n                    pl.col(\"weights\").sum().alias(\"weights_sum\"),\n                    *chain.from_iterable(\n                        [\n                            pl.col(c + \"_mean\").first(),\n                            (\n                                (\n                                    pl.col(\"weights\")\n                                    * ((pl.col(c) - pl.col(c + \"_mean\")) ** 2)\n                                ).sum()\n                                / pl.col(\"weights\").sum()\n                            ).alias(c + \"_variance\"),\n                        ]\n                        for c in [\"y_obs\", \"y_pred\"]\n                    ),\n                ]\n\n                groupby_name = \"bin\"\n                df = df.hstack([f_binned.get_column(\"bin\")])\n                if not is_cat_or_string:\n                    # We also add the bin edges.\n                    df = df.hstack([f_binned.get_column(\"bin_edges\")])\n                    agg_list += [\n                        pl.col(feature_name).mean(),\n                        pl.col(feature_name).std(ddof=0).alias(\"__feature_std\"),\n                        pl.col(\"bin_edges\").first(),\n                    ]\n\n                df = (\n                    df.lazy()\n                    .select(\n                        pl.all(),\n                        (\n                            (pl.col(\"weights\") * pl.col(\"y_obs\"))\n                            .sum()\n                            .over(groupby_name)\n                            / pl.col(\"weights\").sum().over(groupby_name)\n                        ).alias(\"y_obs_mean\"),\n                        (\n                            (pl.col(\"weights\") * pl.col(\"y_pred\"))\n                            .sum()\n                            .over(groupby_name)\n                            / pl.col(\"weights\").sum().over(groupby_name)\n                        ).alias(\"y_pred_mean\"),\n                    )\n                    .group_by(groupby_name)\n                    .agg(agg_list)\n                    .with_columns(\n                        [\n                            pl.when(pl.col(\"count\") &gt; 1)\n                            .then(pl.col(c + \"_variance\") / (pl.col(\"count\") - 1))\n                            .otherwise(pl.col(c + \"_variance\"))\n                            .sqrt()\n                            .alias(c + \"_stderr\")\n                            for c in (\"y_obs\", \"y_pred\")\n                        ]\n                    )\n                )\n\n                if is_cat_or_string:\n                    df = df.with_columns(pl.col(groupby_name).alias(feature_name))\n\n                # With sort and head alone, we could lose the null value, but we\n                # want to keep it.\n                # .sort(\"bias_count\", descending=True)\n                # .head(n_bins)\n                df = (\n                    df.with_columns(\n                        pl.when(pl.col(feature_name).is_null())\n                        .then(pl.max(\"count\") + 1)\n                        .otherwise(pl.col(\"count\"))\n                        .alias(\"__priority\")\n                    )\n                    .sort(\"__priority\", descending=True)\n                    .head(n_bins)\n                    .sort(feature_name, descending=False)\n                    .select(\n                        pl.col(feature_name),\n                        pl.col(\"y_obs_mean\"),\n                        pl.col(\"y_pred_mean\"),\n                        pl.col(\"y_obs_stderr\"),\n                        pl.col(\"y_pred_stderr\"),\n                        pl.col(\"weights_sum\").alias(\"weights\"),\n                        pl.col(\"count\"),\n                        *(\n                            []\n                            if is_cat_or_string\n                            else [pl.col(\"bin_edges\"), pl.col(\"__feature_std\")]\n                        ),\n                    )\n                )\n\n                if not is_cat_or_string:\n                    df = df.with_columns(\n                        pl.when(pl.col(\"bin_edges\").is_null())\n                        .then(\n                            pl.concat_list(\n                                pl.lit(None),\n                                pl.col(\"__feature_std\"),\n                                pl.lit(None),\n                            )\n                        )\n                        .otherwise(\n                            pl.concat_list(\n                                pl.col(\"bin_edges\").arr.first(),\n                                pl.col(\"__feature_std\"),\n                                pl.col(\"bin_edges\").arr.last(),\n                            )\n                        )\n                        .list.to_array(3)\n                        .alias(\"bin_edges\")\n                    )\n                df = df.collect()\n\n            # Add column \"model\".\n            if n_pred &gt; 0:\n                model_col_name = \"model_\" if feature_name == \"model\" else \"model\"\n                df = df.with_columns(\n                    pl.Series(model_col_name, [pred_names[i]] * df.shape[0])\n                )\n\n            # Add partial dependence.\n            with_pd = predict_function is not None and feature_name is not None\n            if with_pd:\n                # In case we have \"other n\" string/cat/enum, we must exclude it from pd\n                # because it is an artificual value and not part of the real data.\n                has_rest_n = (\n                    is_cat_or_string and \"other \" in df.get_column(feature_name)[-1]\n                )\n                if has_rest_n:\n                    # Note that null, if present, is the first not the last values.\n                    grid = df.get_column(feature_name)[:-1]\n                else:\n                    grid = df.get_column(feature_name)\n                pd_values = compute_partial_dependence(\n                    pred_fun=predict_function,  # type: ignore\n                    X=X,  # type: ignore\n                    feature_index=feature_index,\n                    grid=grid,\n                    weights=weights,\n                    n_max=n_max,\n                    rng=rng,\n                )\n                if has_rest_n:\n                    pd_values = pl.concat([pl.Series(pd_values), pl.Series([None])])\n                df = df.with_columns(\n                    pl.Series(name=\"partial_dependence\", values=pd_values)\n                )\n\n            # Select the columns in the correct order.\n            col_selection = []\n            if n_pred &gt; 0:\n                col_selection.append(model_col_name)\n            if feature_name is not None and feature_name in df.columns:\n                col_selection.append(str(feature_name))\n            col_selection += [\n                \"y_obs_mean\",\n                \"y_pred_mean\",\n                \"y_obs_stderr\",\n                \"y_pred_stderr\",\n                \"count\",\n                \"weights\",\n            ]\n            if feature_name in df.columns and not is_cat_or_string:\n                col_selection += [\"bin_edges\"]\n            if with_pd:\n                col_selection += [\"partial_dependence\"]\n            df_list.append(df.select(col_selection))\n\n        df = pl.concat(df_list)\n    return df\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/#model_diagnostics.calibration.identification_function","title":"<code>identification_function(y_obs, y_pred, *, functional='mean', level=0.5)</code>","text":"<p>Canonical identification function.</p> <p>Identification functions act as generalised residuals. See Notes for further details.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code>, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile of quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>V</code> <code>ndarray of shape (n_obs)</code> <p>Values of the identification function.</p> Notes <p> The function  The function \\(V(y, z)\\) for observation \\(y=y_{pred}\\) and prediction \\(z=y_{pred}\\) is a strict identification function for the functional \\(T\\), or induces the functional \\(T\\) as:</p> \\[ \\mathbb{E}[V(Y, z)] = 0\\quad \\Leftrightarrow\\quad z\\in T(F) \\quad \\forall \\text{ distributions } F \\in \\mathcal{F} \\] <p>for some class of distributions \\(\\mathcal{F}\\). Implemented examples of the functional \\(T\\) are mean, median, expectiles and quantiles.</p> functional strict identification function \\(V(y, z)\\) mean \\(z - y\\) median \\(\\mathbf{1}\\{z \\ge y\\} - \\frac{1}{2}\\) expectile \\(2 \\mid\\mathbf{1}\\{z \\ge y\\} - \\alpha\\mid (z - y)\\) quantile \\(\\mathbf{1}\\{z \\ge y\\} - \\alpha\\) <p>For <code>level</code> \\(\\alpha\\).</p> References <code>[Gneiting2011]</code> <p>T. Gneiting. \"Making and Evaluating Point Forecasts\". (2011) doi:10.1198/jasa.2011.r10138 arxiv:0912.0902</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; identification_function(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\narray([-1,  1,  0,  1])\n</code></pre> Source code in <code>src/model_diagnostics/calibration/identification.py</code> <pre><code>def identification_function(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    *,\n    functional: str = \"mean\",\n    level: float = 0.5,\n) -&gt; np.ndarray:\n    r\"\"\"Canonical identification function.\n\n    Identification functions act as generalised residuals. See [Notes](#notes) for\n    further details.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional`, e.g. the conditional expectation of\n        the response, `E(Y|X)`.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n\n    level : float\n        The level of the expectile of quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n\n    Returns\n    -------\n    V : ndarray of shape (n_obs)\n        Values of the identification function.\n\n    Notes\n    -----\n    [](){#notes}\n    The function \\(V(y, z)\\) for observation \\(y=y_{pred}\\) and prediction\n    \\(z=y_{pred}\\) is a strict identification function for the functional \\(T\\), or\n    induces the functional \\(T\\) as:\n\n    \\[\n    \\mathbb{E}[V(Y, z)] = 0\\quad \\Leftrightarrow\\quad z\\in T(F) \\quad \\forall\n    \\text{ distributions } F\n    \\in \\mathcal{F}\n    \\]\n\n    for some class of distributions \\(\\mathcal{F}\\). Implemented examples of the\n    functional \\(T\\) are mean, median, expectiles and quantiles.\n\n    | functional | strict identification function \\(V(y, z)\\)           |\n    | ---------- | ---------------------------------------------------- |\n    | mean       | \\(z - y\\)                                            |\n    | median     | \\(\\mathbf{1}\\{z \\ge y\\} - \\frac{1}{2}\\)              |\n    | expectile  | \\(2 \\mid\\mathbf{1}\\{z \\ge y\\} - \\alpha\\mid (z - y)\\) |\n    | quantile   | \\(\\mathbf{1}\\{z \\ge y\\} - \\alpha\\)                   |\n\n    For `level` \\(\\alpha\\).\n\n    References\n    ----------\n    `[Gneiting2011]`\n\n    :   T. Gneiting.\n        \"Making and Evaluating Point Forecasts\". (2011)\n        [doi:10.1198/jasa.2011.r10138](https://doi.org/10.1198/jasa.2011.r10138)\n        [arxiv:0912.0902](https://arxiv.org/abs/0912.0902)\n\n    Examples\n    --------\n    &gt;&gt;&gt; identification_function(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n    array([-1,  1,  0,  1])\n    \"\"\"\n    y_o: np.ndarray\n    y_p: np.ndarray\n    y_o, y_p = validate_2_arrays(y_obs, y_pred)\n\n    if functional in (\"expectile\", \"quantile\") and (level &lt;= 0 or level &gt;= 1):\n        msg = f\"Argument level must fulfil 0 &lt; level &lt; 1, got {level}.\"\n        raise ValueError(msg)\n\n    if functional == \"mean\":\n        return y_p - y_o\n    elif functional == \"median\":\n        return np.greater_equal(y_p, y_o) - 0.5\n    elif functional == \"expectile\":\n        return 2 * np.abs(np.greater_equal(y_p, y_o) - level) * (y_p - y_o)\n    elif functional == \"quantile\":\n        return np.greater_equal(y_p, y_o) - level\n    else:\n        allowed_functionals = (\"mean\", \"median\", \"expectile\", \"quantile\")\n        msg = (\n            f\"Argument functional must be one of {allowed_functionals}, got \"\n            f\"{functional}.\"\n        )\n        raise ValueError(msg)\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/#model_diagnostics.calibration.plot_bias","title":"<code>plot_bias(y_obs, y_pred, feature=None, weights=None, *, functional='mean', level=0.5, n_bins=10, bin_method='sturges', confidence_level=0.9, ax=None)</code>","text":"<p>Plot model bias conditional on a feature.</p> <p>This plots the generalised bias (residuals), i.e. the values of the canonical identification function, versus a feature. This is a good way to assess whether a model is conditionally calibrated or not. Well calibrated models have bias terms around zero. See Notes for further details.</p> <p>For numerical features, NaN are treated as Null values. Null values are always plotted as rightmost value on the x-axis and marked with a diamond instead of a dot.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>feature</code> <code>array-like of shape (n_obs) or None</code> <p>Some feature column.</p> <code>None</code> <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights. If given, the bias is calculated as weighted average of the identification function with these weights. Note that the standard errors and p-values in the output are based on the assumption that the variance of the bias is inverse proportional to the weights. See the Notes section for details.</p> <code>None</code> <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile or quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt;= level &lt;= 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> <code>n_bins</code> <code>int</code> <p>The number of bins, at least 2. For numerical features, <code>n_bins</code> only applies when <code>bin_method</code> is set to <code>\"quantile\"</code> or <code>\"uniform\"</code>. For string-like and categorical features, the most frequent values are taken. Ties are dealt with by taking the first value in natural sorting order. The remaining values are merged into <code>\"other n\"</code> with <code>n</code> indicating the unique count.</p> <p>I present, null values are always included in the output, accounting for one bin. NaN values are treated as null values.</p> <code>10</code> <code>bin_method</code> <code>str</code> <p>The method for finding bin edges (boundaries). Options using <code>n_bins</code> are:</p> <ul> <li><code>\"quantile\"</code></li> <li><code>\"uniform\"</code></li> </ul> <p>Options automatically selecting the number of bins for numerical features thereby using uniform bins are same options as numpy.histogram_bin_edges:</p> <ul> <li><code>\"auto\"</code> Minimum bin width between the <code>\"sturges\"</code> and <code>\"fd\"</code> estimators. Provides good all-around performance.</li> <li><code>\"fd\"</code> (Freedman Diaconis Estimator) Robust (resilient to outliers) estimator that takes into account data variability and data size.</li> <li><code>\"doane\"</code> An improved version of Sturges' estimator that works better with non-normal datasets.</li> <li><code>\"scott\"</code> Less robust estimator that takes into account data variability and data size.</li> <li><code>\"stone\"</code> Estimator based on leave-one-out cross-validation estimate of the integrated squared error. Can be regarded as a generalization of Scott's rule.</li> <li><code>\"rice\"</code> Estimator does not take variability into account, only data size. Commonly overestimates number of bins required.</li> <li><code>\"sturges\"</code> R's default method, only accounts for data size. Only optimal for gaussian data and underestimates number of bins for large non-gaussian datasets.</li> <li><code>\"sqrt\"</code> Square root (of data size) estimator, used by Excel and other programs for its speed and simplicity.</li> </ul> <code>'sturges'</code> <code>confidence_level</code> <code>float</code> <p>Confidence level for error bars. If 0, no error bars are plotted. Value must fulfil <code>0 &lt;= confidence_level &lt; 1</code>.</p> <code>0.9</code> <code>ax</code> <code>matplotlib.axes.Axes or plotly Figure</code> <p>Axes object to draw the plot onto, otherwise uses the current Axes.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <p>Either the matplotlib axes or the plotly figure. This is configurable by setting the <code>plot_backend</code> via <code>model_diagnostics.set_config</code> or <code>model_diagnostics.config_context</code>.</p> Notes <p> A model  A model \\(m(X)\\) is conditionally calibrated iff \\(E(V(m(X), Y))=0\\) a.s. The empirical version, given some data, reads \\(\\frac{1}{n}\\sum_i V(m(x_i), y_i)\\). See <code>[FLM2022]</code>.</p> References <code>FLM2022</code> <p>T. Fissler, C. Lorentzen, and M. Mayer. \"Model Comparison and Calibration Assessment\". (2022) arxiv:2202.12780.</p> Source code in <code>src/model_diagnostics/calibration/plots.py</code> <pre><code>def plot_bias(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    feature: Optional[npt.ArrayLike] = None,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    functional: str = \"mean\",\n    level: float = 0.5,\n    n_bins: int = 10,\n    bin_method: str = \"sturges\",\n    confidence_level: float = 0.9,\n    ax: Optional[mpl.axes.Axes] = None,\n):\n    r\"\"\"Plot model bias conditional on a feature.\n\n    This plots the generalised bias (residuals), i.e. the values of the canonical\n    identification function, versus a feature. This is a good way to assess whether\n    a model is conditionally calibrated or not. Well calibrated models have bias terms\n    around zero.\n    See [Notes](#notes) for further details.\n\n    For numerical features, NaN are treated as Null values. Null values are always\n    plotted as rightmost value on the x-axis and marked with a diamond instead of a\n    dot.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    feature : array-like of shape (n_obs) or None\n        Some feature column.\n    weights : array-like of shape (n_obs) or None\n        Case weights. If given, the bias is calculated as weighted average of the\n        identification function with these weights.\n        Note that the standard errors and p-values in the output are based on the\n        assumption that the variance of the bias is inverse proportional to the\n        weights. See the Notes section for details.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n\n    level : float\n        The level of the expectile or quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt;= level &lt;= 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n    n_bins : int\n        The number of bins, at least 2. For numerical features, `n_bins` only applies\n        when `bin_method` is set to `\"quantile\"` or `\"uniform\"`.\n        For string-like and categorical features, the most frequent values are taken.\n        Ties are dealt with by taking the first value in natural sorting order.\n        The remaining values are merged into `\"other n\"` with `n` indicating the unique\n        count.\n\n        I present, null values are always included in the output, accounting for one\n        bin. NaN values are treated as null values.\n    bin_method : str\n        The method for finding bin edges (boundaries). Options using `n_bins` are:\n\n        - `\"quantile\"`\n        - `\"uniform\"`\n\n        Options automatically selecting the number of bins for numerical features\n        thereby using uniform bins are same options as\n        [numpy.histogram_bin_edges](https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html):\n\n        - `\"auto\"`\n        Minimum bin width between the `\"sturges\"` and `\"fd\"` estimators. Provides good\n        all-around performance.\n        - `\"fd\"` (Freedman Diaconis Estimator)\n        Robust (resilient to outliers) estimator that takes into account data\n        variability and data size.\n        - `\"doane\"`\n        An improved version of Sturges' estimator that works better with non-normal\n        datasets.\n        - `\"scott\"`\n        Less robust estimator that takes into account data variability and data size.\n        - `\"stone\"`\n        Estimator based on leave-one-out cross-validation estimate of the integrated\n        squared error. Can be regarded as a generalization of Scott's rule.\n        - `\"rice\"`\n        Estimator does not take variability into account, only data size. Commonly\n        overestimates number of bins required.\n        - `\"sturges\"`\n        R's default method, only accounts for data size. Only optimal for gaussian data\n        and underestimates number of bins for large non-gaussian datasets.\n        - `\"sqrt\"`\n        Square root (of data size) estimator, used by Excel and other programs for its\n        speed and simplicity.\n\n    confidence_level : float\n        Confidence level for error bars. If 0, no error bars are plotted. Value must\n        fulfil `0 &lt;= confidence_level &lt; 1`.\n    ax : matplotlib.axes.Axes or plotly Figure\n        Axes object to draw the plot onto, otherwise uses the current Axes.\n\n    Returns\n    -------\n    ax :\n        Either the matplotlib axes or the plotly figure. This is configurable by\n        setting the `plot_backend` via\n        [`model_diagnostics.set_config`][model_diagnostics.set_config] or\n        [`model_diagnostics.config_context`][model_diagnostics.config_context].\n\n    Notes\n    -----\n    [](){#notes}\n    A model \\(m(X)\\) is conditionally calibrated iff \\(E(V(m(X), Y))=0\\) a.s. The\n    empirical version, given some data, reads \\(\\frac{1}{n}\\sum_i V(m(x_i), y_i)\\).\n    See `[FLM2022]`.\n\n    References\n    ----------\n    `FLM2022`\n\n    :   T. Fissler, C. Lorentzen, and M. Mayer.\n        \"Model Comparison and Calibration Assessment\". (2022)\n        [arxiv:2202.12780](https://arxiv.org/abs/2202.12780).\n    \"\"\"\n    if not (0 &lt;= confidence_level &lt; 1):\n        msg = (\n            f\"Argument confidence_level must fulfil 0 &lt;= level &lt; 1, got \"\n            f\"{confidence_level}.\"\n        )\n        raise ValueError(msg)\n    with_errorbars = confidence_level &gt; 0\n\n    if ax is None:\n        plot_backend = get_config()[\"plot_backend\"]\n        if plot_backend == \"matplotlib\":\n            ax = plt.gca()\n        else:\n            import plotly.graph_objects as go\n\n            fig = ax = go.Figure()\n    elif isinstance(ax, mpl.axes.Axes):\n        plot_backend = \"matplotlib\"\n    elif is_plotly_figure(ax):\n        import plotly.graph_objects as go\n\n        plot_backend = \"plotly\"\n        fig = ax\n    else:\n        msg = (\n            \"The ax argument must be None, a matplotlib Axes or a plotly Figure, \"\n            f\"got {type(ax)}.\"\n        )\n        raise ValueError(msg)\n\n    df = compute_bias(\n        y_obs=y_obs,\n        y_pred=y_pred,\n        feature=feature,\n        weights=weights,\n        functional=functional,\n        level=level,\n        n_bins=n_bins,\n        bin_method=bin_method,\n    )\n\n    if df[\"bias_stderr\"].fill_nan(None).null_count() &gt; 0 and with_errorbars:\n        msg = (\n            \"Some values of 'bias_stderr' are null. Therefore no error bars are \"\n            \"shown for that y_pred/model, despite the fact that confidence_level&gt;0 \"\n            \"was set to True.\"\n        )\n        warnings.warn(msg, UserWarning, stacklevel=2)\n\n    if \"model_\" in df.columns:\n        col_model = \"model_\"\n    elif \"model\" in df.columns:\n        col_model = \"model\"\n    else:\n        col_model = None\n\n    if feature is None:\n        # We treat the predictions from different models as a feature.\n        feature_name = col_model\n        feature_has_nulls = False\n    else:\n        feature_name = array_name(feature, default=\"feature\")\n        feature_has_nulls = df[feature_name].null_count() &gt; 0\n\n    is_categorical = False\n    is_string = False\n    feature_dtype = df.get_column(feature_name).dtype\n    if feature_dtype in [pl.Categorical, pl.Enum]:\n        is_categorical = True\n    elif feature_dtype in [pl.Utf8, pl.Object]:\n        is_string = True\n\n    n_x = df[feature_name].n_unique()\n\n    # horizontal line at y=0\n    if plot_backend == \"matplotlib\":\n        ax.axhline(y=0, xmin=0, xmax=1, color=\"k\", linestyle=\"dotted\")\n    else:\n        fig.add_hline(y=0, line={\"color\": \"black\", \"dash\": \"dot\"}, showlegend=False)\n\n    # bias plot\n    if feature is None or col_model is None:\n        pred_names = [None]\n    else:\n        # pred_names = df[col_model].unique() this automatically sorts\n        pred_names, _ = get_sorted_array_names(y_pred)\n    n_models = len(pred_names)\n    with_label = feature is not None and (n_models &gt;= 2 or feature_has_nulls)\n\n    if (is_string or is_categorical) and feature_has_nulls:\n        # We want the Null values at the end and therefore sort again.\n        df = df.sort(feature_name, descending=False, nulls_last=True)\n\n    for i, m in enumerate(pred_names):\n        filter_condition = True if m is None else pl.col(col_model) == m\n        df_i = df.filter(filter_condition)\n        label = m if with_label else None\n\n        if df_i[\"bias_stderr\"].null_count() &gt; 0:\n            with_errorbars_i = False\n        else:\n            with_errorbars_i = with_errorbars\n\n        if with_errorbars_i:\n            # We scale bias_stderr by the corresponding value of the t-distribution\n            # to get our desired confidence level.\n            n = df_i[\"bias_count\"].to_numpy()\n            conf_level_fct = special.stdtrit(\n                np.maximum(n - 1, 1),  # degrees of freedom, if n=0 =&gt; bias_stderr=0.\n                1 - (1 - confidence_level) / 2,\n            )\n            df_i = df_i.with_columns(\n                [(pl.col(\"bias_stderr\") * conf_level_fct).alias(\"bias_stderr\")]\n            )\n\n        if is_string or is_categorical:\n            df_ii = df_i.filter(pl.col(feature_name).is_not_null())\n            # We x-shift a little for a better visual.\n            span = (n_x - 1) / n_x / n_models  # length for one cat value and one model\n            x = np.arange(n_x - feature_has_nulls)\n            if n_models &gt; 1:\n                x = x + (i - n_models // 2) * span * 0.5\n            if plot_backend == \"matplotlib\":\n                ax.errorbar(\n                    x,\n                    df_ii[\"bias_mean\"],\n                    yerr=df_ii[\"bias_stderr\"] if with_errorbars_i else None,\n                    marker=\"o\",\n                    linestyle=\"None\",\n                    capsize=4,\n                    label=label,\n                )\n            else:\n                fig.add_scatter(\n                    x=x,\n                    y=df_ii[\"bias_mean\"],\n                    error_y={\n                        \"type\": \"data\",  # value of error bar given in data coordinates\n                        \"array\": df_ii[\"bias_stderr\"] if with_errorbars_i else None,\n                        \"width\": 4,\n                        \"visible\": True,\n                    },\n                    marker={\"color\": get_plotly_color(i)},\n                    mode=\"markers\",\n                    name=label,\n                )\n        else:\n            if with_errorbars_i:\n                lower = df_i[\"bias_mean\"] - df_i[\"bias_stderr\"]\n                upper = df_i[\"bias_mean\"] + df_i[\"bias_stderr\"]\n                if plot_backend == \"matplotlib\":\n                    ax.fill_between(\n                        df_i[feature_name],\n                        lower,\n                        upper,\n                        alpha=0.1,\n                    )\n                else:\n                    # plotly has no equivalent of fill_between and needs a bit more\n                    # coding\n                    color = get_plotly_color(i)\n                    fig.add_scatter(\n                        x=pl.concat([df_i[feature_name], df_i[::-1, feature_name]]),\n                        y=pl.concat([lower, upper[::-1]]),\n                        fill=\"toself\",\n                        fillcolor=color,\n                        hoverinfo=\"skip\",\n                        line={\"color\": color},\n                        mode=\"lines\",\n                        opacity=0.1,\n                        showlegend=False,\n                    )\n            if plot_backend == \"matplotlib\":\n                ax.plot(\n                    df_i[feature_name],\n                    df_i[\"bias_mean\"],\n                    linestyle=\"solid\",\n                    marker=\"o\",\n                    label=label,\n                )\n            else:\n                fig.add_scatter(\n                    x=df_i[feature_name],\n                    y=df_i[\"bias_mean\"],\n                    marker_symbol=\"circle\",\n                    mode=\"lines+markers\",\n                    line={\"color\": get_plotly_color(i)},\n                    name=label,\n                )\n\n        if feature_has_nulls:\n            # Null values are plotted as diamonds as rightmost point.\n            df_i_null = df_i.filter(pl.col(feature_name).is_null())\n\n            if is_string or is_categorical:\n                x_null = np.array([n_x - 1])\n            else:\n                x_min = df_i[feature_name].min()\n                x_max = df_i[feature_name].max()\n                if n_x == 1:\n                    # df_i[feature_name] is the null value.\n                    x_null, span = np.array([0]), 1\n                elif n_x == 2:\n                    x_null, span = np.array([2 * x_max]), 0.5 * x_max / n_models\n                else:\n                    x_null = np.array([x_max + (x_max - x_min) / n_x])\n                    span = (x_null - x_max) / n_models\n\n            if n_models &gt; 1:\n                x_null = x_null + (i - n_models // 2) * span * 0.5\n\n            if plot_backend == \"matplotlib\":\n                color = ax.get_lines()[-1].get_color()  # previous line color\n                ax.errorbar(\n                    x_null,\n                    df_i_null[\"bias_mean\"],\n                    yerr=df_i_null[\"bias_stderr\"] if with_errorbars_i else None,\n                    marker=\"D\",\n                    linestyle=\"None\",\n                    capsize=4,\n                    label=None,\n                    color=color,\n                )\n            else:\n                fig.add_scatter(\n                    x=x_null,\n                    y=df_i_null[\"bias_mean\"],\n                    error_y={\n                        \"type\": \"data\",  # value of error bar given in data coordinates\n                        \"array\": df_i_null[\"bias_stderr\"] if with_errorbars_i else None,\n                        \"width\": 4,\n                        \"visible\": True,\n                    },\n                    marker={\"color\": get_plotly_color(i), \"symbol\": \"diamond\"},\n                    mode=\"markers\",\n                    showlegend=False,\n                )\n\n    if is_categorical or is_string:\n        if feature_has_nulls:\n            # Without cast to pl.Uft8, the following error might occur:\n            # exceptions.ComputeError: cannot combine categorical under a global string\n            # cache with a non cached categorical\n            tick_labels = df_i[feature_name].cast(pl.Utf8).fill_null(\"Null\")\n        else:\n            tick_labels = df_i[feature_name]\n        x_label = feature_name\n        if plot_backend == \"matplotlib\":\n            ax.set_xticks(np.arange(n_x), labels=tick_labels)\n        else:\n            fig.update_layout(\n                xaxis={\n                    \"tickmode\": \"array\",\n                    \"tickvals\": np.arange(n_x),\n                    \"ticktext\": tick_labels,\n                }\n            )\n    elif feature_name is not None:\n        x_label = \"binned \" + feature_name\n    else:\n        x_label = \"\"\n\n    if feature is None:\n        title = \"Bias Plot\"\n    else:\n        model_name = array_name(y_pred, default=\"\")\n        # test for empty string \"\"\n        title = \"Bias Plot\" if not model_name else \"Bias Plot \" + model_name\n\n    if plot_backend == \"matplotlib\":\n        ax.set(xlabel=x_label, ylabel=\"bias\", title=title)\n    else:\n        fig.update_layout(xaxis_title=x_label, yaxis_title=\"bias\", title=title)\n\n    if with_label and plot_backend == \"matplotlib\":\n        if feature_has_nulls:\n            # Add legend entry for diamonds as Null values.\n            # Unfortunately, the Null value legend entry often appears first, but we\n            # want it at the end.\n            ax.scatter([], [], marker=\"D\", color=\"grey\", label=\"Null values\")\n            handles, labels = ax.get_legend_handles_labels()\n            if (labels[-1] != \"Null values\") and \"Null values\" in labels:\n                i = labels.index(\"Null values\")\n                # i can't be the last index\n                labels = labels[:i] + labels[i + 1 :] + [labels[i]]\n                handles = handles[:i] + handles[i + 1 :] + [handles[i]]\n            ax.legend(handles=handles, labels=labels)\n        else:\n            ax.legend()\n    elif with_label and feature_has_nulls:\n        fig.add_scatter(\n            x=[None],\n            y=[None],\n            mode=\"markers\",\n            name=\"Null values\",\n            marker={\"size\": 7, \"color\": \"grey\", \"symbol\": \"diamond\"},\n        )\n\n    return ax\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/#model_diagnostics.calibration.plot_marginal","title":"<code>plot_marginal(y_obs, y_pred, X, feature_name, predict_function=None, weights=None, *, n_bins=10, bin_method='sturges', n_max=1000, rng=None, ax=None, show_lines='numerical')</code>","text":"<p>Plot marginal observed and predicted conditional on a feature.</p> <p>This plot provides a means to inspect a model per feature. The average of observed and predicted are plotted as well as a histogram of the feature.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>X</code> <code>array-like of shape (n_obs, n_features)</code> <p>The dataframe or array of features to be passed to the model predict function.</p> required <code>feature_name</code> <code>str or int</code> <p>Column name (str) or index (int) of feature in <code>X</code>.</p> required <code>predict_function</code> <code>callable or None</code> <p>A callable to get prediction, i.e. <code>predict_function(X)</code>. Used to compute partial dependence. If <code>None</code>, partial dependence is omitted.</p> <code>None</code> <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights. If given, the bias is calculated as weighted average of the identification function with these weights.</p> <code>None</code> <code>n_bins</code> <code>int</code> <p>The number of bins, at least 2. For numerical features, <code>n_bins</code> only applies when <code>bin_method</code> is set to <code>\"quantile\"</code> or <code>\"uniform\"</code>. For string-like and categorical features, the most frequent values are taken. Ties are dealt with by taking the first value in natural sorting order. The remaining values are merged into <code>\"other n\"</code> with <code>n</code> indicating the unique count.</p> <p>I present, null values are always included in the output, accounting for one bin. NaN values are treated as null values.</p> <code>10</code> <code>bin_method</code> <code>str</code> <p>The method for finding bin edges (boundaries). Options using <code>n_bins</code> are:</p> <ul> <li><code>\"quantile\"</code></li> <li><code>\"uniform\"</code></li> </ul> <p>Options automatically selecting the number of bins for numerical features thereby using uniform bins are same options as numpy.histogram_bin_edges:</p> <ul> <li><code>\"auto\"</code> Minimum bin width between the <code>\"sturges\"</code> and <code>\"fd\"</code> estimators. Provides good all-around performance.</li> <li><code>\"fd\"</code> (Freedman Diaconis Estimator) Robust (resilient to outliers) estimator that takes into account data variability and data size.</li> <li><code>\"doane\"</code> An improved version of Sturges' estimator that works better with non-normal datasets.</li> <li><code>\"scott\"</code> Less robust estimator that takes into account data variability and data size.</li> <li><code>\"stone\"</code> Estimator based on leave-one-out cross-validation estimate of the integrated squared error. Can be regarded as a generalization of Scott's rule.</li> <li><code>\"rice\"</code> Estimator does not take variability into account, only data size. Commonly overestimates number of bins required.</li> <li><code>\"sturges\"</code> R's default method, only accounts for data size. Only optimal for gaussian data and underestimates number of bins for large non-gaussian datasets.</li> <li><code>\"sqrt\"</code> Square root (of data size) estimator, used by Excel and other programs for its speed and simplicity.</li> </ul> <code>'sturges'</code> <code>n_max</code> <code>int or None</code> <p>Used only for partial dependence computation. The number of rows to subsample from X. This speeds up computation, in particular for slow predict functions.</p> <code>1000</code> <code>rng</code> <code>(Generator, int or None)</code> <p>Used only for partial dependence computation. The random number generator used for subsampling of <code>n_max</code> rows. The input is internally wrapped by <code>np.random.default_rng(rng)</code>.</p> <code>None</code> <code>ax</code> <code>matplotlib.axes.Axes or plotly Figure</code> <p>Axes object to draw the plot onto, otherwise uses the current Axes.</p> <code>None</code> <code>show_lines</code> <code>str</code> <p>Option for how to display mean values and partial dependence:</p> <ul> <li><code>\"always\"</code>: Always draw lines.</li> <li><code>\"numerical\"</code>: String and categorical features are drawn as points, numerical   ones as lines.</li> </ul> <code>'numerical'</code> <p>Returns:</p> Name Type Description <code>ax</code> <p>Either the matplotlib axes or the plotly figure. This is configurable by setting the <code>plot_backend</code> via <code>model_diagnostics.set_config</code> or <code>model_diagnostics.config_context</code>.</p> <p>Examples:</p> <p>If you wish to plot multiple features at once with subfigures, here is how to do it with matplotlib:</p> <pre><code>from math import ceil\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom model_diagnostics.calibration import plot_marginal\n\n# Replace by your own data and model.\nn_obs = 100\ny_obs = np.arange(n_obs)\nX = np.ones((n_obs, 2))\nX[:, 0] = np.sin(np.arange(n_obs))\nX[:, 1] = y_obs ** 2\n\ndef model_predict(X):\n    s = 0.5 * n_obs * np.sin(X)\n    return s.sum(axis=1) + np.sqrt(X[:, 1])\n\n# Now the plotting.\nfeature_list = [0, 1]\nn_rows, n_cols = ceil(len(feature_list) / 2), 2\nfig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, sharey=True)\nfor i, ax in enumerate(axs):\n    plot_marginal(\n        y_obs=y_obs,\n        y_pred=model_predict(X),\n        X=X,\n        feature_name=feature_list[i],\n        predict_function=model_predict,\n        ax=ax,\n    )\nfig.tight_layout()\n</code></pre> <p>For plotly, use the helper function <code>add_marginal_subplot</code>:</p> <pre><code>from math import ceil\nimport numpy as np\nfrom model_diagnostics import config_context\nfrom plotly.subplots import make_subplots\nfrom model_diagnostics.calibration import add_marginal_subplot, plot_marginal\n\n# Replace by your own data and model.\nn_obs = 100\ny_obs = np.arange(n_obs)\nX = np.ones((n_obs, 2))\nX[:, 0] = np.sin(np.arange(n_obs))\nX[:, 1] = y_obs ** 2\n\ndef model_predict(X):\n    s = 0.5 * n_obs * np.sin(X)\n    return s.sum(axis=1) + np.sqrt(X[:, 1])\n\n# Now the plotting.\nfeature_list = [0, 1]\nn_rows, n_cols = ceil(len(feature_list) / 2), 2\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    vertical_spacing=0.3 / n_rows,  # equals default\n    # subplot_titles=feature_list,  # maybe\n    specs=[[{\"secondary_y\": True}] * n_cols] * n_rows,  # This is important!\n)\nfor row in range(n_rows):\n    for col in range(n_cols):\n        i = n_cols * row + col\n        with config_context(plot_backend=\"plotly\"):\n            subfig = plot_marginal(\n                y_obs=y_obs,\n                y_pred=model_predict(X),\n                X=X,\n                feature_name=feature_list[i],\n                predict_function=model_predict,\n            )\n        add_marginal_subplot(subfig, fig, row, col)\nfig.show()\n</code></pre> Source code in <code>src/model_diagnostics/calibration/plots.py</code> <pre><code>def plot_marginal(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    X: npt.ArrayLike,\n    feature_name: Union[str, int],\n    predict_function: Optional[Callable] = None,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    n_bins: int = 10,\n    bin_method: str = \"sturges\",\n    n_max: int = 1000,\n    rng: Optional[Union[np.random.Generator, int]] = None,\n    ax: Optional[mpl.axes.Axes] = None,\n    show_lines: str = \"numerical\",\n):\n    \"\"\"Plot marginal observed and predicted conditional on a feature.\n\n    This plot provides a means to inspect a model per feature.\n    The average of observed and predicted are plotted as well as a histogram of the\n    feature.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    X : array-like of shape (n_obs, n_features)\n        The dataframe or array of features to be passed to the model predict function.\n    feature_name : str or int\n        Column name (str) or index (int) of feature in `X`.\n    predict_function : callable or None\n        A callable to get prediction, i.e. `predict_function(X)`. Used to compute\n        partial dependence. If `None`, partial dependence is omitted.\n    weights : array-like of shape (n_obs) or None\n        Case weights. If given, the bias is calculated as weighted average of the\n        identification function with these weights.\n    n_bins : int\n        The number of bins, at least 2. For numerical features, `n_bins` only applies\n        when `bin_method` is set to `\"quantile\"` or `\"uniform\"`.\n        For string-like and categorical features, the most frequent values are taken.\n        Ties are dealt with by taking the first value in natural sorting order.\n        The remaining values are merged into `\"other n\"` with `n` indicating the unique\n        count.\n\n        I present, null values are always included in the output, accounting for one\n        bin. NaN values are treated as null values.\n    bin_method : str\n        The method for finding bin edges (boundaries). Options using `n_bins` are:\n\n        - `\"quantile\"`\n        - `\"uniform\"`\n\n        Options automatically selecting the number of bins for numerical features\n        thereby using uniform bins are same options as\n        [numpy.histogram_bin_edges](https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html):\n\n        - `\"auto\"`\n        Minimum bin width between the `\"sturges\"` and `\"fd\"` estimators. Provides good\n        all-around performance.\n        - `\"fd\"` (Freedman Diaconis Estimator)\n        Robust (resilient to outliers) estimator that takes into account data\n        variability and data size.\n        - `\"doane\"`\n        An improved version of Sturges' estimator that works better with non-normal\n        datasets.\n        - `\"scott\"`\n        Less robust estimator that takes into account data variability and data size.\n        - `\"stone\"`\n        Estimator based on leave-one-out cross-validation estimate of the integrated\n        squared error. Can be regarded as a generalization of Scott's rule.\n        - `\"rice\"`\n        Estimator does not take variability into account, only data size. Commonly\n        overestimates number of bins required.\n        - `\"sturges\"`\n        R's default method, only accounts for data size. Only optimal for gaussian data\n        and underestimates number of bins for large non-gaussian datasets.\n        - `\"sqrt\"`\n        Square root (of data size) estimator, used by Excel and other programs for its\n        speed and simplicity.\n\n    n_max : int or None\n        Used only for partial dependence computation. The number of rows to subsample\n        from X. This speeds up computation, in particular for slow predict functions.\n    rng : np.random.Generator, int or None\n        Used only for partial dependence computation. The random number generator used\n        for subsampling of `n_max` rows. The input is internally wrapped by\n        `np.random.default_rng(rng)`.\n    ax : matplotlib.axes.Axes or plotly Figure\n        Axes object to draw the plot onto, otherwise uses the current Axes.\n    show_lines : str\n        Option for how to display mean values and partial dependence:\n\n        - `\"always\"`: Always draw lines.\n        - `\"numerical\"`: String and categorical features are drawn as points, numerical\n          ones as lines.\n\n    Returns\n    -------\n    ax :\n        Either the matplotlib axes or the plotly figure. This is configurable by\n        setting the `plot_backend` via\n        [`model_diagnostics.set_config`][model_diagnostics.set_config] or\n        [`model_diagnostics.config_context`][model_diagnostics.config_context].\n\n    Examples\n    -----\n    If you wish to plot multiple features at once with subfigures, here is how to do it\n    with matplotlib:\n\n    ```py\n    from math import ceil\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from model_diagnostics.calibration import plot_marginal\n\n    # Replace by your own data and model.\n    n_obs = 100\n    y_obs = np.arange(n_obs)\n    X = np.ones((n_obs, 2))\n    X[:, 0] = np.sin(np.arange(n_obs))\n    X[:, 1] = y_obs ** 2\n\n    def model_predict(X):\n        s = 0.5 * n_obs * np.sin(X)\n        return s.sum(axis=1) + np.sqrt(X[:, 1])\n\n    # Now the plotting.\n    feature_list = [0, 1]\n    n_rows, n_cols = ceil(len(feature_list) / 2), 2\n    fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, sharey=True)\n    for i, ax in enumerate(axs):\n        plot_marginal(\n            y_obs=y_obs,\n            y_pred=model_predict(X),\n            X=X,\n            feature_name=feature_list[i],\n            predict_function=model_predict,\n            ax=ax,\n        )\n    fig.tight_layout()\n    ```\n\n    For plotly, use the helper function\n    [`add_marginal_subplot`][model_diagnostics.calibration.plots.add_marginal_subplot]:\n\n    ```py\n    from math import ceil\n    import numpy as np\n    from model_diagnostics import config_context\n    from plotly.subplots import make_subplots\n    from model_diagnostics.calibration import add_marginal_subplot, plot_marginal\n\n    # Replace by your own data and model.\n    n_obs = 100\n    y_obs = np.arange(n_obs)\n    X = np.ones((n_obs, 2))\n    X[:, 0] = np.sin(np.arange(n_obs))\n    X[:, 1] = y_obs ** 2\n\n    def model_predict(X):\n        s = 0.5 * n_obs * np.sin(X)\n        return s.sum(axis=1) + np.sqrt(X[:, 1])\n\n    # Now the plotting.\n    feature_list = [0, 1]\n    n_rows, n_cols = ceil(len(feature_list) / 2), 2\n    fig = make_subplots(\n        rows=n_rows,\n        cols=n_cols,\n        vertical_spacing=0.3 / n_rows,  # equals default\n        # subplot_titles=feature_list,  # maybe\n        specs=[[{\"secondary_y\": True}] * n_cols] * n_rows,  # This is important!\n    )\n    for row in range(n_rows):\n        for col in range(n_cols):\n            i = n_cols * row + col\n            with config_context(plot_backend=\"plotly\"):\n                subfig = plot_marginal(\n                    y_obs=y_obs,\n                    y_pred=model_predict(X),\n                    X=X,\n                    feature_name=feature_list[i],\n                    predict_function=model_predict,\n                )\n            add_marginal_subplot(subfig, fig, row, col)\n    fig.show()\n    ```\n\n    \"\"\"\n    if ax is None:\n        plot_backend = get_config()[\"plot_backend\"]\n        if plot_backend == \"matplotlib\":\n            ax = plt.gca()\n        else:\n            from plotly.subplots import make_subplots\n\n            # fig = ax = go.Figure()\n            fig = ax = make_subplots(specs=[[{\"secondary_y\": True}]])\n    elif isinstance(ax, mpl.axes.Axes):\n        plot_backend = \"matplotlib\"\n    elif is_plotly_figure(ax):\n        plot_backend = \"plotly\"\n        fig = ax\n        # Take care to mimick make_subplots for secondary y axis.\n        # The following code is by comparing\n        #   make_subplots(specs=[[{\"secondary_y\": True}]])\n        # vs\n        #   go.Figure()\n        if not hasattr(fig.layout, \"yaxis2\"):\n            fig.update_layout(\n                xaxis={\"anchor\": \"y\", \"domain\": [0.0, 0.94]},\n                yaxis={\"anchor\": \"x\", \"domain\": [0.0, 1.0]},\n                yaxis2={\"anchor\": \"x\", \"overlaying\": \"y\", \"side\": \"right\"},\n            )\n            SubplotRef = collections.namedtuple(  # noqa: PYI024\n                \"SubplotRef\", (\"subplot_type\", \"layout_keys\", \"trace_kwargs\")\n            )\n            fig._grid_ref = [  # noqa: SLF001\n                [\n                    (\n                        SubplotRef(\n                            subplot_type=\"xy\",\n                            layout_keys=(\"xaxis\", \"yaxis\"),\n                            trace_kwargs={\"xaxis\": \"x\", \"yaxis\": \"y\"},\n                        ),\n                        SubplotRef(\n                            subplot_type=\"xy\",\n                            layout_keys=(\"xaxis\", \"yaxis2\"),\n                            trace_kwargs={\"xaxis\": \"x\", \"yaxis\": \"y2\"},\n                        ),\n                    )\n                ]\n            ]\n            fig._grid_str = \"This is the format of your plot grid:\\n[ (1,1) x,y,y2 ]\\n\"  # noqa: SLF001\n    else:\n        msg = (\n            \"The ax argument must be None, a matplotlib Axes or a plotly Figure, \"\n            f\"got {type(ax)}.\"\n        )\n        raise ValueError(msg)\n\n    if show_lines not in (\"always\", \"numerical\"):\n        msg = (\n            f\"The argument show_lines mut be 'always' or 'numerical'; got {show_lines}.\"\n        )\n        raise ValueError(msg)\n\n    # estimator = getattr(predict_callable, \"__self__\", None)\n    n_pred = length_of_second_dimension(y_pred)\n    if n_pred &gt; 1:\n        msg = (\n            f\"Parameter y_pred has shape (n_obs, {n_pred}), but only \"\n            \"(n_obs) and (n_obs, 1) are allowd.\"\n        )\n        raise ValueError(msg)\n\n    df = compute_marginal(\n        y_obs=y_obs,\n        y_pred=y_pred,\n        X=X,\n        feature_name=feature_name,\n        predict_function=predict_function,\n        weights=weights,\n        n_bins=n_bins,\n        bin_method=bin_method,\n        n_max=n_max,\n        rng=rng,\n    )\n    feature_name = df.columns[0]\n\n    feature_has_nulls = df[feature_name].null_count() &gt; 0\n    n_bins_eff = df.shape[0] - feature_has_nulls\n    # If df contains the columns \"bin_edges\", it's a numerical feature.\n    is_categorical = \"bin_edges\" not in df.columns\n\n    n_x = df[feature_name].n_unique()\n\n    # marginal plot\n    if is_categorical and feature_has_nulls:\n        # We want the Null values at the end and therefore sort again.\n        df = df.sort(feature_name, descending=False, nulls_last=True)\n    df_no_nulls = df.filter(pl.col(feature_name).is_not_null())\n\n    # Numerical columns are sometimes better treated as categorical.\n    num_as_cat = False\n    if not is_categorical:\n        bin_edges = df_no_nulls.get_column(\"bin_edges\")\n        num_as_cat = (\n            # left bin edge = right bin edge\n            (bin_edges.arr.first() == bin_edges.arr.last())\n            # feature == left bin edge\n            | (bin_edges.arr.first() == df_no_nulls.get_column(feature_name))\n            # feature == right bin edge\n            | (bin_edges.arr.last() == df_no_nulls.get_column(feature_name))\n            # standard deviation of feature in bin == 0\n            | (bin_edges.arr.get(1) == 0)\n        ).all()\n\n    # First the histogram of weights on secondary y-axis.\n    # Other graph elements should appear on top of it. For plotly, we therefore need to\n    # plot the histogram on the primary y-axis and put primary to the right and\n    # secondary to the left. All other plotly graphs are put on the secondary yaxis.\n    #\n    # We x-shift a little for a better visual.\n    x = (\n        np.arange(n_x - feature_has_nulls)\n        if is_categorical\n        else df_no_nulls[feature_name]\n    )\n    if plot_backend == \"matplotlib\":\n        ax2 = ax.twinx()\n        if is_categorical or num_as_cat:\n            ax2.bar(\n                x=x,\n                height=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n                color=\"lightgrey\",\n            )\n        else:\n            # We can't use\n            #   ax2.hist(\n            #       x=df_no_nulls[feature_name],\n            #       weights=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n            #       bins=np.r_[bin_edges[0][0], bin_edges.arr.last()],  # n_bins_eff,\n            #       color=\"lightgrey\",\n            #       edgecolor=\"grey\",\n            #       rwidth=0.8 if n_bins_eff &lt;= 2 else None,\n            #   )\n            # because we might have empty bins.\n            ax2.bar(\n                x=0.5 * (bin_edges.arr.last() + bin_edges.arr.first()),\n                height=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n                width=(bin_edges.arr.last() - bin_edges.arr.first())\n                * (1 if n_bins_eff &gt; 2 else 0.8),\n                color=\"lightgrey\",\n                edgecolor=\"grey\",\n            )\n        # https://stackoverflow.com/questions/30505616/how-to-arrange-plots-of-secondary-axis-to-be-below-plots-of-primary-axis-in-matp\n        ax.set_zorder(ax2.get_zorder() + 1)\n        ax.set_frame_on(False)\n    else:\n        if is_categorical or num_as_cat:\n            # fig.add_histogram(\n            #     x=x, # df_no_nulls[feature_name],\n            #     y=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n            #     histfunc=\"sum\",\n            #     marker={\"color\": \"lightgrey\"},\n            #     secondary_y=False,\n            #     showlegend=False,\n            # )\n            fig.add_bar(\n                x=x,\n                y=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n                marker={\"color\": \"lightgrey\"},\n                secondary_y=False,\n                showlegend=False,\n            )\n        else:\n            fig.add_bar(\n                x=0.5 * (bin_edges.arr.last() + bin_edges.arr.first()),\n                y=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n                width=bin_edges.arr.last() - bin_edges.arr.first(),\n                marker={\"color\": \"lightgrey\", \"line\": {\"width\": 1.0, \"color\": \"grey\"}},\n                secondary_y=False,\n                showlegend=False,\n            )\n        fig.update_layout(yaxis_side=\"right\", yaxis2_side=\"left\")\n        if n_bins_eff &lt;= 2:\n            fig.update_layout(bargap=0.2)\n\n    if feature_has_nulls:\n        df_null = df.filter(pl.col(feature_name).is_null())\n        # Null values are plotted as rightmost point at x_null.\n        if is_categorical:\n            x_null = np.array([n_x - 1])\n            # matplotlib default width = 0.8\n            width = 0.8 if plot_backend == \"matplotlib\" else None\n        else:\n            x_min = df[feature_name].min()\n            x_max = df[feature_name].max()\n            if n_x == 1:\n                # df[feature_name] is the null value.\n                x_null = np.array([0])\n            elif n_x == 2:\n                x_null = np.array([2 * x_max])\n            else:\n                x_null = np.array([x_max + (x_max - x_min) / n_x])\n            width = x_null - bin_edges.arr.last().max()\n            if width is not None and width &lt;= 0:\n                width = (x_max - x_min) / n_x / 2.0\n\n        # Null value histogram\n        if plot_backend == \"matplotlib\":\n            ax2.bar(\n                x=x_null,\n                height=df_null[\"weights\"] / df[\"weights\"].sum(),\n                width=width,\n                color=\"lightgrey\",\n            )\n        else:\n            fig.add_bar(\n                x=x_null,\n                y=df_null[\"weights\"] / df[\"weights\"].sum(),\n                width=width,\n                marker={\"color\": \"lightgrey\"},\n                secondary_y=False,\n                showlegend=False,\n            )\n\n    plot_items = [\"y_obs_mean\", \"y_pred_mean\"]\n    if predict_function is not None:\n        plot_items.append(\"partial_dependence\")\n    label_dict = {\n        \"y_obs_mean\": \"mean y_obs\",\n        \"y_pred_mean\": \"mean y_pred\",\n        \"partial_dependence\": \"partial dependence\",\n    }\n    for i, m in enumerate(plot_items):\n        label = label_dict[m]\n        if plot_backend == \"matplotlib\":\n            linestyle = \"dashed\" if m == \"partial_dependence\" else \"solid\"\n        else:\n            line = {\n                \"color\": get_plotly_color(i),\n                \"dash\": \"dash\" if m == \"partial_dependence\" else None,\n            }\n        if is_categorical:\n            # We x-shift a little for a better visual.\n            x = np.arange(n_x - feature_has_nulls)\n            if plot_backend == \"matplotlib\":\n                ax.plot(\n                    x,\n                    df_no_nulls[m],\n                    marker=\"o\",\n                    linestyle=\"None\" if show_lines == \"numerical\" else linestyle,\n                    label=label,\n                )\n            else:\n                fig.add_scatter(\n                    x=x,\n                    y=df_no_nulls[m],\n                    marker={\"color\": get_plotly_color(i)},\n                    mode=\"markers\" if show_lines == \"numerical\" else \"lines+markers\",\n                    line=None if show_lines == \"numerical\" else line,\n                    name=label,\n                    secondary_y=True,\n                )\n        elif plot_backend == \"matplotlib\":\n            ax.plot(\n                df[feature_name],\n                df[m],\n                linestyle=linestyle,\n                marker=\"o\",\n                label=label,\n            )\n        else:\n            fig.add_scatter(\n                x=df[feature_name],\n                y=df[m],\n                marker_symbol=\"circle\",\n                mode=\"lines+markers\",\n                line=line,\n                name=label,\n                secondary_y=True,\n            )\n\n        if feature_has_nulls:\n            # Null values are plotted as diamonds as rightmost point.\n            if plot_backend == \"matplotlib\":\n                color = ax.get_lines()[-1].get_color()  # previous line color\n                ax.plot(\n                    x_null,\n                    df_null[m],\n                    marker=\"D\",\n                    linestyle=\"None\",\n                    label=None,\n                    color=color,\n                )\n            else:\n                fig.add_scatter(\n                    x=x_null,\n                    y=df_null[m],\n                    marker={\"color\": get_plotly_color(i), \"symbol\": \"diamond\"},\n                    mode=\"markers\",\n                    secondary_y=True,\n                    showlegend=False,\n                )\n\n    if is_categorical:\n        if df[feature_name].null_count() &gt; 0:\n            # Without cast to pl.Uft8, the following error might occur:\n            # exceptions.ComputeError: cannot combine categorical under a global string\n            # cache with a non cached categorical\n            tick_labels = df[feature_name].cast(pl.Utf8).fill_null(\"Null\")\n        else:\n            tick_labels = df[feature_name]\n        x_label = feature_name\n        if plot_backend == \"matplotlib\":\n            ax.set_xticks(np.arange(n_x), labels=tick_labels)\n        else:\n            fig.update_layout(\n                xaxis={\n                    \"tickmode\": \"array\",\n                    \"tickvals\": np.arange(n_x),\n                    \"ticktext\": tick_labels,\n                }\n            )\n    elif feature_name is not None:\n        x_label = \"binned \" + str(feature_name)\n    else:\n        x_label = \"\"\n\n    model_name = array_name(y_pred, default=\"\")\n    # test for empty string \"\"\n    title = \"Marginal Plot\" if not model_name else \"Marginal Plot \" + model_name\n\n    if plot_backend == \"matplotlib\":\n        ax.set(xlabel=x_label, ylabel=\"y\", title=title)\n    else:\n        fig.update_layout(xaxis_title=x_label, yaxis2_title=\"y\", title=title)\n        fig[\"layout\"][\"yaxis\"][\"showgrid\"] = False\n\n    if plot_backend == \"matplotlib\":\n        if feature_has_nulls:\n            # Add legend entry for diamonds as Null values.\n            # Unfortunately, the Null value legend entry often appears first, but we\n            # want it at the end.\n            ax.scatter([], [], marker=\"D\", color=\"grey\", label=\"Null values\")\n            handles, labels = ax.get_legend_handles_labels()\n            if (labels[-1] != \"Null values\") and \"Null values\" in labels:\n                i = labels.index(\"Null values\")\n                # i can't be the last index\n                labels = labels[:i] + labels[i + 1 :] + [labels[i]]\n                handles = handles[:i] + handles[i + 1 :] + [handles[i]]\n            ax.legend(handles=handles, labels=labels)\n        else:\n            ax.legend()\n    elif feature_has_nulls:\n        fig.add_scatter(\n            x=[None],\n            y=[None],\n            mode=\"markers\",\n            name=\"Null values\",\n            marker={\"size\": 7, \"color\": \"grey\", \"symbol\": \"diamond\"},\n            secondary_y=True,\n        )\n\n    return ax\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/#model_diagnostics.calibration.plot_reliability_diagram","title":"<code>plot_reliability_diagram(y_obs, y_pred, weights=None, *, functional='mean', level=0.5, n_bootstrap=None, confidence_level=0.9, diagram_type='reliability', ax=None)</code>","text":"<p>Plot a reliability diagram.</p> <p>A reliability diagram or calibration curve assesses auto-calibration. It plots the conditional expectation given the predictions <code>E(y_obs|y_pred)</code> (y-axis) vs the predictions <code>y_pred</code> (x-axis). The conditional expectation is estimated via isotonic regression (PAV algorithm) of <code>y_obs</code> on <code>y_pred</code>. See Notes for further details.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile or quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt;= level &lt;= 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> <code>n_bootstrap</code> <code>int or None</code> <p>If not <code>None</code>, then <code>scipy.stats.bootstrap</code> with <code>n_resamples=n_bootstrap</code> is used to calculate confidence intervals at level <code>confidence_level</code>.</p> <code>None</code> <code>confidence_level</code> <code>float</code> <p>Confidence level for bootstrap uncertainty regions.</p> <code>0.9</code> <code>diagram_type</code> <code>str</code> <ul> <li><code>\"reliability\"</code>: Plot a reliability diagram.</li> <li><code>\"bias\"</code>: Plot roughly a 45 degree rotated reliability diagram. The resulting   plot is similar to <code>plot_bias</code>, i.e. <code>y_pred - E(y_obs|y_pred)</code> vs <code>y_pred</code>.</li> </ul> <code>'reliability'</code> <code>ax</code> <code>matplotlib.axes.Axes or plotly Figure</code> <p>Axes object to draw the plot onto, otherwise uses the current Axes.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <p>Either the matplotlib axes or the plotly figure. This is configurable by setting the <code>plot_backend</code> via <code>model_diagnostics.set_config</code> or <code>model_diagnostics.config_context</code>.</p> Notes <p> The expectation conditional on the predictions is  The expectation conditional on the predictions is \\(E(Y|y_{pred})\\). This object is estimated by the pool-adjacent violator (PAV) algorithm, which has very desirable properties:</p> <pre><code>- It is non-parametric without any tuning parameter. Thus, the results are\n  easily reproducible.\n- Optimal selection of bins\n- Statistical consistent estimator\n</code></pre> <p>For details, refer to <code>[Dimitriadis2021]</code>.</p> References <code>[Dimitriadis2021]</code> <p>T. Dimitriadis, T. Gneiting, and A. I. Jordan. \"Stable reliability diagrams for probabilistic classifiers\". In: Proceedings of the National Academy of Sciences 118.8 (2021), e2016191118. doi:10.1073/pnas.2016191118.</p> Source code in <code>src/model_diagnostics/calibration/plots.py</code> <pre><code>def plot_reliability_diagram(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    functional: str = \"mean\",\n    level: float = 0.5,\n    n_bootstrap: Optional[str] = None,\n    confidence_level: float = 0.9,\n    diagram_type: str = \"reliability\",\n    ax: Optional[mpl.axes.Axes] = None,\n):\n    r\"\"\"Plot a reliability diagram.\n\n    A reliability diagram or calibration curve assesses auto-calibration. It plots the\n    conditional expectation given the predictions `E(y_obs|y_pred)` (y-axis) vs the\n    predictions `y_pred` (x-axis).\n    The conditional expectation is estimated via isotonic regression (PAV algorithm)\n    of `y_obs` on `y_pred`.\n    See [Notes](#notes) for further details.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n\n    level : float\n        The level of the expectile or quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt;= level &lt;= 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n    n_bootstrap : int or None\n        If not `None`, then `scipy.stats.bootstrap` with `n_resamples=n_bootstrap`\n        is used to calculate confidence intervals at level `confidence_level`.\n    confidence_level : float\n        Confidence level for bootstrap uncertainty regions.\n    diagram_type: str\n        - `\"reliability\"`: Plot a reliability diagram.\n        - `\"bias\"`: Plot roughly a 45 degree rotated reliability diagram. The resulting\n          plot is similar to `plot_bias`, i.e. `y_pred - E(y_obs|y_pred)` vs `y_pred`.\n    ax : matplotlib.axes.Axes or plotly Figure\n        Axes object to draw the plot onto, otherwise uses the current Axes.\n\n    Returns\n    -------\n    ax :\n        Either the matplotlib axes or the plotly figure. This is configurable by\n        setting the `plot_backend` via\n        [`model_diagnostics.set_config`][model_diagnostics.set_config] or\n        [`model_diagnostics.config_context`][model_diagnostics.config_context].\n\n    Notes\n    -----\n    [](){#notes}\n    The expectation conditional on the predictions is \\(E(Y|y_{pred})\\). This object is\n    estimated by the pool-adjacent violator (PAV) algorithm, which has very desirable\n    properties:\n\n        - It is non-parametric without any tuning parameter. Thus, the results are\n          easily reproducible.\n        - Optimal selection of bins\n        - Statistical consistent estimator\n\n    For details, refer to `[Dimitriadis2021]`.\n\n    References\n    ----------\n    `[Dimitriadis2021]`\n\n    :   T. Dimitriadis, T. Gneiting, and A. I. Jordan.\n        \"Stable reliability diagrams for probabilistic classifiers\".\n        In: Proceedings of the National Academy of Sciences 118.8 (2021), e2016191118.\n        [doi:10.1073/pnas.2016191118](https://doi.org/10.1073/pnas.2016191118).\n    \"\"\"\n    if ax is None:\n        plot_backend = get_config()[\"plot_backend\"]\n        if plot_backend == \"matplotlib\":\n            ax = plt.gca()\n        else:\n            import plotly.graph_objects as go\n\n            fig = ax = go.Figure()\n    elif isinstance(ax, mpl.axes.Axes):\n        plot_backend = \"matplotlib\"\n    elif is_plotly_figure(ax):\n        import plotly.graph_objects as go\n\n        plot_backend = \"plotly\"\n        fig = ax\n    else:\n        msg = (\n            \"The ax argument must be None, a matplotlib Axes or a plotly Figure, \"\n            f\"got {type(ax)}.\"\n        )\n        raise ValueError(msg)\n\n    if diagram_type not in (\"reliability\", \"bias\"):\n        msg = (\n            \"Parameter diagram_type must be either 'reliability', 'bias', \"\n            f\"got {diagram_type}.\"\n        )\n        raise ValueError(msg)\n\n    if (n_cols := length_of_second_dimension(y_obs)) &gt; 0:\n        if n_cols == 1:\n            y_obs = get_second_dimension(y_obs, 0)\n        else:\n            msg = (\n                f\"Array-like y_obs has more than 2 dimensions, y_obs.shape[1]={n_cols}\"\n            )\n            raise ValueError(msg)\n\n    y_min, y_max = get_array_min_max(y_pred)\n    if diagram_type == \"reliability\":\n        if plot_backend == \"matplotlib\":\n            ax.plot([y_min, y_max], [y_min, y_max], color=\"k\", linestyle=\"dotted\")\n        else:\n            fig.add_scatter(\n                x=[y_min, y_max],\n                y=[y_min, y_max],\n                mode=\"lines\",\n                line={\"color\": \"black\", \"dash\": \"dot\"},\n                showlegend=False,\n            )\n    elif plot_backend == \"matplotlib\":\n        # horizontal line at y=0\n\n        # The following plots in axis coordinates\n        # ax.axhline(y=0, xmin=0, xmax=1, color=\"k\", linestyle=\"dotted\")\n        # but we plot in data coordinates instead.\n        ax.hlines(0, xmin=y_min, xmax=y_max, color=\"k\", linestyle=\"dotted\")\n    else:\n        # horizontal line at y=0\n        fig.add_hline(y=0, line={\"color\": \"black\", \"dash\": \"dot\"}, showlegend=False)\n\n    if n_bootstrap is not None:\n        if functional == \"mean\":\n\n            def iso_statistic(y_obs, y_pred, weights=None, x_values=None):\n                iso_b = (\n                    IsotonicRegression_skl(out_of_bounds=\"clip\")\n                    .set_output(transform=\"default\")\n                    .fit(y_pred, y_obs, sample_weight=weights)\n                )\n                return iso_b.predict(x_values)\n\n        else:\n\n            def iso_statistic(y_obs, y_pred, weights=None, x_values=None):\n                iso_b = IsotonicRegression(functional=functional, level=level).fit(\n                    y_pred, y_obs, sample_weight=weights\n                )\n                return iso_b.predict(x_values)\n\n    n_pred = length_of_second_dimension(y_pred)\n    pred_names, _ = get_sorted_array_names(y_pred)\n\n    for i in range(len(pred_names)):\n        y_pred_i = y_pred if n_pred == 0 else get_second_dimension(y_pred, i)\n\n        if functional == \"mean\":\n            iso = (\n                IsotonicRegression_skl()\n                .set_output(transform=\"default\")\n                .fit(y_pred_i, y_obs, sample_weight=weights)\n            )\n        else:\n            iso = IsotonicRegression(functional=functional, level=level).fit(\n                y_pred_i, y_obs, sample_weight=weights\n            )\n\n        # confidence intervals\n        if n_bootstrap is not None:\n            data: tuple[npt.ArrayLike, ...]\n            data = (y_obs, y_pred_i) if weights is None else (y_obs, y_pred_i, weights)\n\n            boot = bootstrap(\n                data=data,\n                statistic=partial(iso_statistic, x_values=iso.X_thresholds_),\n                n_resamples=n_bootstrap,\n                paired=True,\n                confidence_level=confidence_level,\n                # Note: method=\"bca\" might result in\n                # DegenerateDataWarning: The BCa confidence interval cannot be\n                # calculated. This problem is known to occur when the distribution is\n                # degenerate or the statistic is np.min.\n                method=\"basic\",\n            )\n\n            # We make the interval conservatively monotone increasing by applying\n            # np.maximum.accumulate etc.\n            # Conservative here means smaller intervals such that it is more likely\n            # for the prediction to be out of the intervals leading to the conclusion\n            # of \"not auto-calibrated\".\n            lower = np.maximum.accumulate(boot.confidence_interval.low)\n            upper = np.minimum.accumulate(boot.confidence_interval.high[::-1])[::-1]\n            if diagram_type == \"bias\":\n                lower = iso.X_thresholds_ - lower\n                upper = iso.X_thresholds_ - upper\n            if plot_backend == \"matplotlib\":\n                ax.fill_between(iso.X_thresholds_, lower, upper, alpha=0.1)\n            else:\n                # plotly has not equivalent of fill_between and needs a bit more coding\n                color = get_plotly_color(i)\n                fig.add_scatter(\n                    x=np.r_[iso.X_thresholds_, iso.X_thresholds_[::-1]],\n                    y=np.r_[lower, upper[::-1]],\n                    fill=\"toself\",\n                    fillcolor=color,\n                    hoverinfo=\"skip\",\n                    line={\"color\": color},\n                    mode=\"lines\",\n                    opacity=0.1,\n                    showlegend=False,\n                )\n\n        # reliability curve\n        label = pred_names[i] if n_pred &gt;= 2 else None\n\n        y_plot = (\n            iso.y_thresholds_\n            if diagram_type == \"reliability\"\n            else iso.X_thresholds_ - iso.y_thresholds_\n        )\n        if plot_backend == \"matplotlib\":\n            ax.plot(iso.X_thresholds_, y_plot, label=label)\n        else:\n            fig.add_scatter(\n                x=iso.X_thresholds_,\n                y=y_plot,\n                mode=\"lines\",\n                line={\"color\": get_plotly_color(i)},\n                name=label,\n            )\n\n    xlabel_mapping = {\n        \"mean\": \"E(Y|X)\",\n        \"median\": \"median(Y|X)\",\n        \"expectile\": f\"{level}-expectile(Y|X)\",\n        \"quantile\": f\"{level}-quantile(Y|X)\",\n    }\n    ylabel_mapping = {\n        \"mean\": \"E(Y|prediction)\",\n        \"median\": \"median(Y|prediction)\",\n        \"expectile\": f\"{level}-expectile(Y|prediction)\",\n        \"quantile\": f\"{level}-quantile(Y|prediction)\",\n    }\n    xlabel = \"prediction for \" + xlabel_mapping[functional]\n    if diagram_type == \"reliability\":\n        ylabel = \"estimated \" + ylabel_mapping[functional]\n        title = \"Reliability Diagram\"\n    else:\n        ylabel = \"prediction - estimated \" + ylabel_mapping[functional]\n        title = \"Bias Reliability Diagram\"\n\n    if n_pred &lt;= 1 and len(pred_names[0]) &gt; 0:\n        title = title + \" \" + pred_names[0]\n\n    if plot_backend == \"matplotlib\":\n        if n_pred &gt;= 2:\n            ax.legend()\n        ax.set_title(title)\n        ax.set(xlabel=xlabel, ylabel=ylabel)\n    else:\n        if n_pred &lt;= 1:\n            fig.update_layout(showlegend=False)\n        fig.update_layout(xaxis_title=xlabel, yaxis_title=ylabel, title=title)\n\n    return ax\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/identification/","title":"identification","text":""},{"location":"reference/model_diagnostics/calibration/identification/#model_diagnostics.calibration.identification","title":"<code>identification</code>","text":""},{"location":"reference/model_diagnostics/calibration/identification/#model_diagnostics.calibration.identification.compute_bias","title":"<code>compute_bias(y_obs, y_pred, feature=None, weights=None, *, functional='mean', level=0.5, n_bins=10, bin_method='sturges')</code>","text":"<p>Compute generalised bias conditional on a feature.</p> <p>This function computes and aggregates the generalised bias, i.e. the values of the canonical identification function, versus (grouped by) a feature. This is a good way to assess whether a model is conditionally calibrated or not. Well calibrated models have bias terms around zero. For the mean functional, the generalised bias is the negative residual <code>y_pred - y_obs</code>. See Notes for further details.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>feature</code> <code>array-like of shape (n_obs) or None</code> <p>Some feature column.</p> <code>None</code> <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights. If given, the bias is calculated as weighted average of the identification function with these weights. Note that the standard errors and p-values in the output are based on the assumption that the variance of the bias is inverse proportional to the weights. See the Notes section for details.</p> <code>None</code> <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile of quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> <code>n_bins</code> <code>int</code> <p>The number of bins, at least 2. For numerical features, <code>n_bins</code> only applies when <code>bin_method</code> is set to <code>\"quantile\"</code> or <code>\"uniform\"</code>. For string-like and categorical features, the most frequent values are taken. Ties are dealt with by taking the first value in natural sorting order. The remaining values are merged into <code>\"other n\"</code> with <code>n</code> indicating the unique count.</p> <p>I present, null values are always included in the output, accounting for one bin. NaN values are treated as null values.</p> <code>10</code> <code>bin_method</code> <code>str</code> <p>The method for finding bin edges (boundaries). Options using <code>n_bins</code> are:</p> <ul> <li><code>\"quantile\"</code></li> <li><code>\"uniform\"</code></li> </ul> <p>Options automatically selecting the number of bins for numerical features thereby using uniform bins are same options as numpy.histogram_bin_edges:</p> <ul> <li><code>\"auto\"</code> Minimum bin width between the <code>\"sturges\"</code> and <code>\"fd\"</code> estimators. Provides good all-around performance.</li> <li><code>\"fd\"</code> (Freedman Diaconis Estimator) Robust (resilient to outliers) estimator that takes into account data variability and data size.</li> <li><code>\"doane\"</code> An improved version of Sturges' estimator that works better with non-normal datasets.</li> <li><code>\"scott\"</code> Less robust estimator that takes into account data variability and data size.</li> <li><code>\"stone\"</code> Estimator based on leave-one-out cross-validation estimate of the integrated squared error. Can be regarded as a generalization of Scott's rule.</li> <li><code>\"rice\"</code> Estimator does not take variability into account, only data size. Commonly overestimates number of bins required.</li> <li><code>\"sturges\"</code> R's default method, only accounts for data size. Only optimal for gaussian data and underestimates number of bins for large non-gaussian datasets.</li> <li><code>\"sqrt\"</code> Square root (of data size) estimator, used by Excel and other programs for its speed and simplicity.</li> </ul> <code>'sturges'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The result table contains at least the columns:</p> <ul> <li><code>bias_mean</code>: Mean of the bias</li> <li><code>bias_cout</code>: Number of data rows</li> <li><code>bias_weights</code>: Sum of weights</li> <li><code>bias_stderr</code>: Standard error, i.e. standard deviation of <code>bias_mean</code></li> <li><code>p_value</code>: p-value of the 2-sided t-test with null hypothesis:   <code>bias_mean = 0</code></li> </ul> <p>If <code>feautre</code> is not None, then there is also the column:</p> <ul> <li><code>feature_name</code>: The actual name of the feature with the (binned) feature   values.</li> </ul> Notes <p> A model  A model \\(m(X)\\) is conditionally calibrated iff \\(\\mathbb{E}(V(m(X), Y)|X)=0\\) almost surely with canonical identification function \\(V\\). The empirical version, given some data, reads \\(\\bar{V} = \\frac{1}{n}\\sum_i \\phi(x_i) V(m(x_i), y_i)\\) with a test function \\(\\phi(x_i)\\) that projects on the specified feature. For a feature with only two distinct values <code>\"a\"</code> and <code>\"b\"</code>, this becomes \\(\\bar{V} = \\frac{1}{n_a}\\sum_{i \\text{ with }x_i=a} V(m(a), y_i)\\) with \\(n_a=\\sum_{i \\text{ with }x_i=a}\\) and similar for <code>\"b\"</code>. With case weights, this reads \\(\\bar{V} = \\frac{1}{\\sum_i w_i}\\sum_i w_i \\phi(x_i) V(m(x_i), y_i)\\). This generalises the classical residual (up to a minus sign) for target functionals other than the mean. See <code>[FLM2022]</code>.</p> <p>The standard error for \\(\\bar{V}\\) is calculated in the standard way as \\(\\mathrm{SE} = \\sqrt{\\operatorname{Var}(\\bar{V})} = \\frac{\\sigma}{\\sqrt{n}}\\) and the standard variance estimator for \\(\\sigma^2 = \\operatorname{Var}(\\phi(x_i) V(m(x_i), y_i))\\) with Bessel correction, i.e. division by \\(n-1\\) instead of \\(n\\).</p> <p>With case weights, the variance estimator becomes \\(\\operatorname{Var}(\\bar{V}) = \\frac{1}{n-1} \\frac{1}{\\sum_i w_i} \\sum_i w_i (V(m(x_i), y_i) - \\bar{V})^2\\) with the implied relation \\(\\operatorname{Var}(V(m(x_i), y_i)) \\sim \\frac{1}{w_i} \\). If your weights are for repeated observations, so-called frequency weights, then the above estimate is conservative because it uses \\(n - 1\\) instead of \\((\\sum_i w_i) - 1\\).</p> References <code>[FLM2022]</code> <p>T. Fissler, C. Lorentzen, and M. Mayer. \"Model Comparison and Calibration Assessment\". (2022) arxiv:2202.12780.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; compute_bias(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\nshape: (1, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bias_mean \u2506 bias_count \u2506 bias_weights \u2506 bias_stderr \u2506 p_value  \u2502\n\u2502 ---       \u2506 ---        \u2506 ---          \u2506 ---         \u2506 ---      \u2502\n\u2502 f64       \u2506 u32        \u2506 f64          \u2506 f64         \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.25      \u2506 4          \u2506 4.0          \u2506 0.478714    \u2506 0.637618 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; compute_bias(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2],\n... feature=[\"a\", \"a\", \"b\", \"b\"])\nshape: (2, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 feature \u2506 bias_mean \u2506 bias_count \u2506 bias_weights \u2506 bias_stderr \u2506 p_value \u2502\n\u2502 ---     \u2506 ---       \u2506 ---        \u2506 ---          \u2506 ---         \u2506 ---     \u2502\n\u2502 str     \u2506 f64       \u2506 u32        \u2506 f64          \u2506 f64         \u2506 f64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a       \u2506 0.0       \u2506 2          \u2506 2.0          \u2506 1.0         \u2506 1.0     \u2502\n\u2502 b       \u2506 0.5       \u2506 2          \u2506 2.0          \u2506 0.5         \u2506 0.5     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>src/model_diagnostics/calibration/identification.py</code> <pre><code>def compute_bias(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    feature: Optional[Union[npt.ArrayLike, pl.Series]] = None,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    functional: str = \"mean\",\n    level: float = 0.5,\n    n_bins: int = 10,\n    bin_method: str = \"sturges\",\n):\n    r\"\"\"Compute generalised bias conditional on a feature.\n\n    This function computes and aggregates the generalised bias, i.e. the values of the\n    canonical identification function, versus (grouped by) a feature.\n    This is a good way to assess whether a model is conditionally calibrated or not.\n    Well calibrated models have bias terms around zero.\n    For the mean functional, the generalised bias is the negative residual\n    `y_pred - y_obs`.\n    See [Notes](#notes) for further details.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    feature : array-like of shape (n_obs) or None\n        Some feature column.\n    weights : array-like of shape (n_obs) or None\n        Case weights. If given, the bias is calculated as weighted average of the\n        identification function with these weights.\n        Note that the standard errors and p-values in the output are based on the\n        assumption that the variance of the bias is inverse proportional to the\n        weights. See the Notes section for details.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n\n    level : float\n        The level of the expectile of quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n    n_bins : int\n        The number of bins, at least 2. For numerical features, `n_bins` only applies\n        when `bin_method` is set to `\"quantile\"` or `\"uniform\"`.\n        For string-like and categorical features, the most frequent values are taken.\n        Ties are dealt with by taking the first value in natural sorting order.\n        The remaining values are merged into `\"other n\"` with `n` indicating the unique\n        count.\n\n        I present, null values are always included in the output, accounting for one\n        bin. NaN values are treated as null values.\n    bin_method : str\n        The method for finding bin edges (boundaries). Options using `n_bins` are:\n\n        - `\"quantile\"`\n        - `\"uniform\"`\n\n        Options automatically selecting the number of bins for numerical features\n        thereby using uniform bins are same options as\n        [numpy.histogram_bin_edges](https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html):\n\n        - `\"auto\"`\n        Minimum bin width between the `\"sturges\"` and `\"fd\"` estimators. Provides good\n        all-around performance.\n        - `\"fd\"` (Freedman Diaconis Estimator)\n        Robust (resilient to outliers) estimator that takes into account data\n        variability and data size.\n        - `\"doane\"`\n        An improved version of Sturges' estimator that works better with non-normal\n        datasets.\n        - `\"scott\"`\n        Less robust estimator that takes into account data variability and data size.\n        - `\"stone\"`\n        Estimator based on leave-one-out cross-validation estimate of the integrated\n        squared error. Can be regarded as a generalization of Scott's rule.\n        - `\"rice\"`\n        Estimator does not take variability into account, only data size. Commonly\n        overestimates number of bins required.\n        - `\"sturges\"`\n        R's default method, only accounts for data size. Only optimal for gaussian data\n        and underestimates number of bins for large non-gaussian datasets.\n        - `\"sqrt\"`\n        Square root (of data size) estimator, used by Excel and other programs for its\n        speed and simplicity.\n\n    Returns\n    -------\n    df : polars.DataFrame\n        The result table contains at least the columns:\n\n        - `bias_mean`: Mean of the bias\n        - `bias_cout`: Number of data rows\n        - `bias_weights`: Sum of weights\n        - `bias_stderr`: Standard error, i.e. standard deviation of `bias_mean`\n        - `p_value`: p-value of the 2-sided t-test with null hypothesis:\n          `bias_mean = 0`\n\n        If `feautre ` is not None, then there is also the column:\n\n        - `feature_name`: The actual name of the feature with the (binned) feature\n          values.\n\n    Notes\n    -----\n    [](){#notes}\n    A model \\(m(X)\\) is conditionally calibrated iff\n    \\(\\mathbb{E}(V(m(X), Y)|X)=0\\) almost surely with canonical identification\n    function \\(V\\).\n    The empirical version, given some data, reads\n    \\(\\bar{V} = \\frac{1}{n}\\sum_i \\phi(x_i) V(m(x_i), y_i)\\) with a test function\n    \\(\\phi(x_i)\\) that projects on the specified feature.\n    For a feature with only two distinct values `\"a\"` and `\"b\"`, this becomes\n    \\(\\bar{V} = \\frac{1}{n_a}\\sum_{i \\text{ with }x_i=a} V(m(a), y_i)\\) with\n    \\(n_a=\\sum_{i \\text{ with }x_i=a}\\) and similar for `\"b\"`.\n    With case weights, this reads\n    \\(\\bar{V} = \\frac{1}{\\sum_i w_i}\\sum_i w_i \\phi(x_i) V(m(x_i), y_i)\\).\n    This generalises the classical residual (up to a minus sign) for target functionals\n    other than the mean. See `[FLM2022]`.\n\n    The standard error for \\(\\bar{V}\\) is calculated in the standard way as\n    \\(\\mathrm{SE} = \\sqrt{\\operatorname{Var}(\\bar{V})} = \\frac{\\sigma}{\\sqrt{n}}\\) and\n    the standard variance estimator for \\(\\sigma^2 = \\operatorname{Var}(\\phi(x_i)\n    V(m(x_i), y_i))\\) with Bessel correction, i.e. division by \\(n-1\\) instead of\n    \\(n\\).\n\n    With case weights, the variance estimator becomes \\(\\operatorname{Var}(\\bar{V})\n    = \\frac{1}{n-1} \\frac{1}{\\sum_i w_i} \\sum_i w_i (V(m(x_i), y_i) - \\bar{V})^2\\) with\n    the implied relation \\(\\operatorname{Var}(V(m(x_i), y_i)) \\sim \\frac{1}{w_i} \\).\n    If your weights are for repeated observations, so-called frequency weights, then\n    the above estimate is conservative because it uses \\(n - 1\\) instead\n    of \\((\\sum_i w_i) - 1\\).\n\n    References\n    ----------\n    `[FLM2022]`\n\n    :   T. Fissler, C. Lorentzen, and M. Mayer.\n        \"Model Comparison and Calibration Assessment\". (2022)\n        [arxiv:2202.12780](https://arxiv.org/abs/2202.12780).\n\n    Examples\n    --------\n    &gt;&gt;&gt; compute_bias(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n    shape: (1, 5)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 bias_mean \u2506 bias_count \u2506 bias_weights \u2506 bias_stderr \u2506 p_value  \u2502\n    \u2502 ---       \u2506 ---        \u2506 ---          \u2506 ---         \u2506 ---      \u2502\n    \u2502 f64       \u2506 u32        \u2506 f64          \u2506 f64         \u2506 f64      \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 0.25      \u2506 4          \u2506 4.0          \u2506 0.478714    \u2506 0.637618 \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    &gt;&gt;&gt; compute_bias(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2],\n    ... feature=[\"a\", \"a\", \"b\", \"b\"])\n    shape: (2, 6)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 feature \u2506 bias_mean \u2506 bias_count \u2506 bias_weights \u2506 bias_stderr \u2506 p_value \u2502\n    \u2502 ---     \u2506 ---       \u2506 ---        \u2506 ---          \u2506 ---         \u2506 ---     \u2502\n    \u2502 str     \u2506 f64       \u2506 u32        \u2506 f64          \u2506 f64         \u2506 f64     \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 a       \u2506 0.0       \u2506 2          \u2506 2.0          \u2506 1.0         \u2506 1.0     \u2502\n    \u2502 b       \u2506 0.5       \u2506 2          \u2506 2.0          \u2506 0.5         \u2506 0.5     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    validate_same_first_dimension(y_obs, y_pred)\n    n_pred = length_of_second_dimension(y_pred)\n    pred_names, _ = get_sorted_array_names(y_pred)\n\n    if weights is not None:\n        validate_same_first_dimension(weights, y_obs)\n        w = np.asarray(weights)\n        if w.ndim &gt; 1:\n            msg = f\"The array weights must be 1-dimensional, got weights.ndim={w.ndim}.\"\n            raise ValueError(msg)\n    else:\n        w = np.ones_like(y_obs, dtype=float)\n\n    n_obs = length_of_first_dimension(y_pred)\n    df_list = []\n    with pl.StringCache():\n        feature_name = None\n        if feature is not None:\n            feature, n_bins, f_binned = bin_feature(\n                feature=feature,\n                feature_name=None,\n                n_obs=n_obs,\n                n_bins=n_bins,\n                bin_method=bin_method,\n            )\n            feature_name = feature.name\n            is_cat_or_string = feature.dtype in [\n                pl.Categorical,\n                pl.Enum,\n                pl.Utf8,\n                pl.Object,\n            ]\n\n        for i in range(len(pred_names)):\n            # Loop over columns of y_pred.\n            x = y_pred if n_pred == 0 else get_second_dimension(y_pred, i)\n\n            bias = identification_function(\n                y_obs=y_obs,\n                y_pred=x,\n                functional=functional,\n                level=level,\n            )\n\n            if feature is None:\n                bias_mean = np.average(bias, weights=w)\n                bias_weights = np.sum(w)\n                bias_count = bias.shape[0]\n                # Note: with Bessel correction\n                bias_stddev = np.average((bias - bias_mean) ** 2, weights=w) / np.amax(\n                    [1, bias_count - 1]\n                )\n                df = pl.DataFrame(\n                    {\n                        \"bias_mean\": [bias_mean],\n                        \"bias_count\": pl.Series([bias_count], dtype=pl.UInt32),\n                        \"bias_weights\": [bias_weights],\n                        \"bias_stderr\": [np.sqrt(bias_stddev)],\n                    }\n                )\n            else:\n                df = pl.DataFrame(\n                    {\n                        \"y_obs\": y_obs,\n                        \"y_pred\": x,\n                        feature_name: feature,\n                        \"bias\": bias,\n                        \"weights\": w,\n                    }\n                )\n\n                agg_list = [\n                    pl.col(\"bias_mean\").first(),\n                    pl.count(\"bias\").alias(\"bias_count\"),\n                    pl.col(\"weights\").sum().alias(\"bias_weights\"),\n                    (\n                        (\n                            pl.col(\"weights\")\n                            * ((pl.col(\"bias\") - pl.col(\"bias_mean\")) ** 2)\n                        ).sum()\n                        / pl.col(\"weights\").sum()\n                    ).alias(\"variance\"),\n                ]\n\n                groupby_name = \"bin\"\n                df = df.hstack([f_binned.get_column(\"bin\")])\n                if not is_cat_or_string:\n                    agg_list.append(pl.col(feature_name).mean())\n\n                df = (\n                    df.lazy()\n                    .select(\n                        pl.all(),\n                        (\n                            (pl.col(\"weights\") * pl.col(\"bias\"))\n                            .sum()\n                            .over(groupby_name)\n                            / pl.col(\"weights\").sum().over(groupby_name)\n                        ).alias(\"bias_mean\"),\n                    )\n                    .group_by(groupby_name)\n                    .agg(agg_list)\n                    .with_columns(\n                        [\n                            pl.when(pl.col(\"bias_count\") &gt; 1)\n                            .then(pl.col(\"variance\") / (pl.col(\"bias_count\") - 1))\n                            .otherwise(pl.col(\"variance\"))\n                            .sqrt()\n                            .alias(\"bias_stderr\"),\n                        ]\n                    )\n                )\n\n                if is_cat_or_string:\n                    df = df.with_columns(pl.col(groupby_name).alias(feature_name))\n\n                df = (\n                    df\n                    # With sort and head alone, we could lose the null value, but we\n                    # want to keep it.\n                    # .sort(\"bias_count\", descending=True)\n                    # .head(n_bins)\n                    .with_columns(\n                        pl.when(pl.col(feature_name).is_null())\n                        .then(pl.max(\"bias_count\") + 1)\n                        .otherwise(pl.col(\"bias_count\"))\n                        .alias(\"__priority\")\n                    )\n                    .sort(\"__priority\", descending=True)\n                    .head(n_bins)\n                    .sort(feature_name, descending=False)\n                    .select(\n                        pl.col(feature_name),\n                        pl.col(\"bias_mean\"),\n                        pl.col(\"bias_count\"),\n                        pl.col(\"bias_weights\"),\n                        pl.col(\"bias_stderr\"),\n                    )\n                ).collect()\n\n            # Add column with p-value of 2-sided t-test.\n            # We explicitly convert \"to_numpy\", because otherwise we get:\n            #   RuntimeWarning: A builtin ctypes object gave a PEP3118 format string\n            #   that does not match its itemsize, so a best-guess will be made of the\n            #   data type. Newer versions of python may behave correctly.\n            stderr_ = df.get_column(\"bias_stderr\")\n            p_value = np.full_like(stderr_, fill_value=np.nan)\n            n = df.get_column(\"bias_count\")\n            p_value[np.asarray((n &gt; 1) &amp; (stderr_ == 0), dtype=bool)] = 0\n            mask = stderr_ &gt; 0\n            x = df.get_column(\"bias_mean\").filter(mask).to_numpy()\n            n = df.get_column(\"bias_count\").filter(mask).to_numpy()\n            stderr = stderr_.filter(mask).to_numpy()\n            # t-statistic t (-|t| and factor of 2 because of 2-sided test)\n            p_value[np.asarray(mask, dtype=bool)] = 2 * special.stdtr(\n                n - 1,  # degrees of freedom\n                -np.abs(x / stderr),\n            )\n            df = df.with_columns(pl.Series(\"p_value\", p_value))\n\n            # Add column \"model\".\n            if n_pred &gt; 0:\n                model_col_name = \"model_\" if feature_name == \"model\" else \"model\"\n                df = df.with_columns(\n                    pl.Series(model_col_name, [pred_names[i]] * df.shape[0])\n                )\n\n            # Select the columns in the correct order.\n            col_selection = []\n            if n_pred &gt; 0:\n                col_selection.append(model_col_name)\n            if feature_name is not None and feature_name in df.columns:\n                col_selection.append(feature_name)\n            col_selection += [\n                \"bias_mean\",\n                \"bias_count\",\n                \"bias_weights\",\n                \"bias_stderr\",\n                \"p_value\",\n            ]\n            df_list.append(df.select(col_selection))\n\n        df = pl.concat(df_list)\n    return df\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/identification/#model_diagnostics.calibration.identification.compute_marginal","title":"<code>compute_marginal(y_obs, y_pred, X=None, feature_name=None, predict_function=None, weights=None, *, n_bins=10, bin_method='sturges', n_max=1000, rng=None)</code>","text":"<p>Compute the marginal expectation conditional on a single feature.</p> <p>This function computes the (weighted) average of observed response and predictions conditional on a given feature.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>X</code> <code>array-like of shape (n_obs, n_features) or None</code> <p>The dataframe or array of features to be passed to the model predict function.</p> <code>None</code> <code>feature_name</code> <code>(int, str or None)</code> <p>Column name (str) or index (int) of feature in <code>X</code>. If None, the total marginal is computed.</p> <code>None</code> <code>predict_function</code> <code>callable or None</code> <p>A callable to get prediction, i.e. <code>predict_function(X)</code>. Used to compute partial dependence. If <code>None</code>, partial dependence is omitted.</p> <code>None</code> <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights. If given, the bias is calculated as weighted average of the identification function with these weights.</p> <code>None</code> <code>n_bins</code> <code>int</code> <p>The number of bins, at least 2. For numerical features, <code>n_bins</code> only applies when <code>bin_method</code> is set to <code>\"quantile\"</code> or <code>\"uniform\"</code>. For string-like and categorical features, the most frequent values are taken. Ties are dealt with by taking the first value in natural sorting order. The remaining values are merged into <code>\"other n\"</code> with <code>n</code> indicating the unique count.</p> <p>I present, null values are always included in the output, accounting for one bin. NaN values are treated as null values.</p> <code>10</code> <code>bin_method</code> <code>str</code> <p>The method for finding bin edges (boundaries). Options using <code>n_bins</code> are:</p> <ul> <li><code>\"quantile\"</code></li> <li><code>\"uniform\"</code></li> </ul> <p>Options automatically selecting the number of bins for numerical features thereby using uniform bins are same options as numpy.histogram_bin_edges:</p> <ul> <li><code>\"auto\"</code> Minimum bin width between the <code>\"sturges\"</code> and <code>\"fd\"</code> estimators. Provides good all-around performance.</li> <li><code>\"fd\"</code> (Freedman Diaconis Estimator) Robust (resilient to outliers) estimator that takes into account data variability and data size.</li> <li><code>\"doane\"</code> An improved version of Sturges' estimator that works better with non-normal datasets.</li> <li><code>\"scott\"</code> Less robust estimator that takes into account data variability and data size.</li> <li><code>\"stone\"</code> Estimator based on leave-one-out cross-validation estimate of the integrated squared error. Can be regarded as a generalization of Scott's rule.</li> <li><code>\"rice\"</code> Estimator does not take variability into account, only data size. Commonly overestimates number of bins required.</li> <li><code>\"sturges\"</code> R's default method, only accounts for data size. Only optimal for gaussian data and underestimates number of bins for large non-gaussian datasets.</li> <li><code>\"sqrt\"</code> Square root (of data size) estimator, used by Excel and other programs for its speed and simplicity.</li> </ul> <code>'sturges'</code> <code>n_max</code> <code>int or None</code> <p>Used only for partial dependence computation. The number of rows to subsample from X. This speeds up computation, in particular for slow predict functions.</p> <code>1000</code> <code>rng</code> <code>(Generator, int or None)</code> <p>Used only for partial dependence computation. The random number generator used for subsampling of <code>n_max</code> rows. The input is internally wrapped by <code>np.random.default_rng(rng)</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The result table contains at least the columns:</p> <ul> <li><code>y_obs_mean</code>: Mean of <code>y_obs</code></li> <li><code>y_pred_mean</code>: Mean of <code>y_pred</code></li> <li><code>y_obs_stderr</code>: Standard error, i.e. standard deviation of <code>y_obs_mean</code></li> <li><code>y_pred_stderr</code>: Standard error, i.e. standard deviation of <code>y_pred_mean</code></li> <li><code>count</code>: Number of data rows</li> <li><code>weights</code>: Sum of weights</li> </ul> <p>If <code>feature</code> is not None, then there is also the column:</p> <ul> <li><code>feature_name</code>: The actual name of the feature with the (binned) feature   values.</li> </ul> <p>If <code>feature</code> is numerical, one also has:</p> <ul> <li><code>bin_edges</code>: The edges and standard deviation of the bins, i.e.   (min, std, max).</li> </ul> Notes <p>The marginal values are computed as an estimation of:</p> <ul> <li><code>y_obs</code>: \\(\\mathbb{E}(Y|feature)\\)</li> <li><code>y_pred</code>: \\(\\mathbb{E}(m(X)|feature)\\)</li> </ul> <p>with \\(feature\\) the column specified by <code>feature_name</code>. Computationally that is more or less a group-by-aggregate operation on a dataset.</p> <p>The standard error for both are calculated in the standard way as \\(\\mathrm{SE} = \\sqrt{\\operatorname{Var}(\\bar{Y})} = \\frac{\\sigma}{\\sqrt{n}}\\) and the standard variance estimator for \\(\\sigma^2\\) with Bessel correction, i.e. division by \\(n-1\\) instead of \\(n\\).</p> <p>With case weights, the variance estimator becomes \\(\\operatorname{Var}(\\bar{Y}) = \\frac{1}{n-1} \\frac{1}{\\sum_i w_i} \\sum_i w_i (y_i - \\bar{y})^2\\) with the implied relation \\(\\operatorname{Var}(y_i) \\sim \\frac{1}{w_i} \\). If your weights are for repeated observations, so-called frequency weights, then the above estimate is conservative because it uses \\(n - 1\\) instead of \\((\\sum_i w_i) - 1\\).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; compute_marginal(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1, 2])\nshape: (1, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 y_obs_mean \u2506 y_pred_mean \u2506 y_obs_stderr \u2506 y_pred_stderr \u2506 count \u2506 weights \u2502\n\u2502 ---        \u2506 ---         \u2506 ---          \u2506 ---           \u2506 ---   \u2506 ---     \u2502\n\u2502 f64        \u2506 f64         \u2506 f64          \u2506 f64           \u2506 u32   \u2506 f64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.5        \u2506 0.75        \u2506 0.288675     \u2506 0.629153      \u2506 4     \u2506 4.0     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from sklearn.linear_model import Ridge\n&gt;&gt;&gt; pl.Config.set_tbl_width_chars(84)\n&lt;class 'polars.config.Config'&gt;\n&gt;&gt;&gt; y_obs, X =[0, 0, 1, 1], [[0, 1], [1, 1], [2, 2], [3, 2]]\n&gt;&gt;&gt; m = Ridge().fit(X, y_obs)\n&gt;&gt;&gt; compute_marginal(y_obs=y_obs, y_pred=m.predict(X), X=X, feature_name=0,\n... predict_function=m.predict)\nshape: (3, 9)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 feature  \u2506 y_obs_m \u2506 y_pred_ \u2506 y_obs_s \u2506 \u2026 \u2506 count \u2506 weights \u2506 bin_edg \u2506 partial \u2502\n\u2502 0        \u2506 ean     \u2506 mean    \u2506 tderr   \u2506   \u2506 ---   \u2506 ---     \u2506 es      \u2506 _depend \u2502\n\u2502 ---      \u2506 ---     \u2506 ---     \u2506 ---     \u2506   \u2506 u32   \u2506 f64     \u2506 ---     \u2506 ence    \u2502\n\u2502 f64      \u2506 f64     \u2506 f64     \u2506 f64     \u2506   \u2506       \u2506         \u2506 array[f \u2506 ---     \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 64, 3]  \u2506 f64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.5      \u2506 0.0     \u2506 0.125   \u2506 0.0     \u2506 \u2026 \u2506 2     \u2506 2.0     \u2506 [0.0,   \u2506 0.25    \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.5,    \u2506         \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 1.0]    \u2506         \u2502\n\u2502 2.0      \u2506 1.0     \u2506 0.75    \u2506 0.0     \u2506 \u2026 \u2506 1     \u2506 1.0     \u2506 [1.0,   \u2506 0.625   \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.0,    \u2506         \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 2.0]    \u2506         \u2502\n\u2502 3.0      \u2506 1.0     \u2506 1.0     \u2506 0.0     \u2506 \u2026 \u2506 1     \u2506 1.0     \u2506 [2.0,   \u2506 0.875   \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.0,    \u2506         \u2502\n\u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 3.0]    \u2506         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>src/model_diagnostics/calibration/identification.py</code> <pre><code>def compute_marginal(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    X: Optional[npt.ArrayLike] = None,\n    feature_name: Optional[Union[str, int]] = None,\n    predict_function: Optional[Callable] = None,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    n_bins: int = 10,\n    bin_method: str = \"sturges\",\n    n_max: int = 1000,\n    rng: Optional[Union[np.random.Generator, int]] = None,\n):\n    r\"\"\"Compute the marginal expectation conditional on a single feature.\n\n    This function computes the (weighted) average of observed response and predictions\n    conditional on a given feature.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    X : array-like of shape (n_obs, n_features) or None\n        The dataframe or array of features to be passed to the model predict function.\n    feature_name : int, str or None\n        Column name (str) or index (int) of feature in `X`. If None, the total marginal\n        is computed.\n    predict_function : callable or None\n        A callable to get prediction, i.e. `predict_function(X)`. Used to compute\n        partial dependence. If `None`, partial dependence is omitted.\n    weights : array-like of shape (n_obs) or None\n        Case weights. If given, the bias is calculated as weighted average of the\n        identification function with these weights.\n    n_bins : int\n        The number of bins, at least 2. For numerical features, `n_bins` only applies\n        when `bin_method` is set to `\"quantile\"` or `\"uniform\"`.\n        For string-like and categorical features, the most frequent values are taken.\n        Ties are dealt with by taking the first value in natural sorting order.\n        The remaining values are merged into `\"other n\"` with `n` indicating the unique\n        count.\n\n        I present, null values are always included in the output, accounting for one\n        bin. NaN values are treated as null values.\n    bin_method : str\n        The method for finding bin edges (boundaries). Options using `n_bins` are:\n\n        - `\"quantile\"`\n        - `\"uniform\"`\n\n        Options automatically selecting the number of bins for numerical features\n        thereby using uniform bins are same options as\n        [numpy.histogram_bin_edges](https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html):\n\n        - `\"auto\"`\n        Minimum bin width between the `\"sturges\"` and `\"fd\"` estimators. Provides good\n        all-around performance.\n        - `\"fd\"` (Freedman Diaconis Estimator)\n        Robust (resilient to outliers) estimator that takes into account data\n        variability and data size.\n        - `\"doane\"`\n        An improved version of Sturges' estimator that works better with non-normal\n        datasets.\n        - `\"scott\"`\n        Less robust estimator that takes into account data variability and data size.\n        - `\"stone\"`\n        Estimator based on leave-one-out cross-validation estimate of the integrated\n        squared error. Can be regarded as a generalization of Scott's rule.\n        - `\"rice\"`\n        Estimator does not take variability into account, only data size. Commonly\n        overestimates number of bins required.\n        - `\"sturges\"`\n        R's default method, only accounts for data size. Only optimal for gaussian data\n        and underestimates number of bins for large non-gaussian datasets.\n        - `\"sqrt\"`\n        Square root (of data size) estimator, used by Excel and other programs for its\n        speed and simplicity.\n\n    n_max : int or None\n        Used only for partial dependence computation. The number of rows to subsample\n        from X. This speeds up computation, in particular for slow predict functions.\n    rng : np.random.Generator, int or None\n        Used only for partial dependence computation. The random number generator used\n        for subsampling of `n_max` rows. The input is internally wrapped by\n        `np.random.default_rng(rng)`.\n\n    Returns\n    -------\n    df : polars.DataFrame\n        The result table contains at least the columns:\n\n        - `y_obs_mean`: Mean of `y_obs`\n        - `y_pred_mean`: Mean of `y_pred`\n        - `y_obs_stderr`: Standard error, i.e. standard deviation of `y_obs_mean`\n        - `y_pred_stderr`: Standard error, i.e. standard deviation of `y_pred_mean`\n        - `count`: Number of data rows\n        - `weights`: Sum of weights\n\n        If `feature ` is not None, then there is also the column:\n\n        - `feature_name`: The actual name of the feature with the (binned) feature\n          values.\n\n        If `feature` is numerical, one also has:\n\n        - `bin_edges`: The edges and standard deviation of the bins, i.e.\n          (min, std, max).\n\n    Notes\n    -----\n    The marginal values are computed as an estimation of:\n\n    - `y_obs`: \\(\\mathbb{E}(Y|feature)\\)\n    - `y_pred`: \\(\\mathbb{E}(m(X)|feature)\\)\n\n    with \\(feature\\) the column specified by `feature_name`.\n    Computationally that is more or less a group-by-aggregate operation on a dataset.\n\n    The standard error for both are calculated in the standard way as\n    \\(\\mathrm{SE} = \\sqrt{\\operatorname{Var}(\\bar{Y})} = \\frac{\\sigma}{\\sqrt{n}}\\) and\n    the standard variance estimator for \\(\\sigma^2\\) with Bessel correction, i.e.\n    division by \\(n-1\\) instead of \\(n\\).\n\n    With case weights, the variance estimator becomes \\(\\operatorname{Var}(\\bar{Y})\n    = \\frac{1}{n-1} \\frac{1}{\\sum_i w_i} \\sum_i w_i (y_i - \\bar{y})^2\\) with\n    the implied relation \\(\\operatorname{Var}(y_i) \\sim \\frac{1}{w_i} \\).\n    If your weights are for repeated observations, so-called frequency weights, then\n    the above estimate is conservative because it uses \\(n - 1\\) instead\n    of \\((\\sum_i w_i) - 1\\).\n\n    Examples\n    --------\n    &gt;&gt;&gt; compute_marginal(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1, 2])\n    shape: (1, 6)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 y_obs_mean \u2506 y_pred_mean \u2506 y_obs_stderr \u2506 y_pred_stderr \u2506 count \u2506 weights \u2502\n    \u2502 ---        \u2506 ---         \u2506 ---          \u2506 ---           \u2506 ---   \u2506 ---     \u2502\n    \u2502 f64        \u2506 f64         \u2506 f64          \u2506 f64           \u2506 u32   \u2506 f64     \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 0.5        \u2506 0.75        \u2506 0.288675     \u2506 0.629153      \u2506 4     \u2506 4.0     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    &gt;&gt;&gt; import polars as pl\n    &gt;&gt;&gt; from sklearn.linear_model import Ridge\n    &gt;&gt;&gt; pl.Config.set_tbl_width_chars(84)  # doctest: +ELLIPSIS\n    &lt;class 'polars.config.Config'&gt;\n    &gt;&gt;&gt; y_obs, X =[0, 0, 1, 1], [[0, 1], [1, 1], [2, 2], [3, 2]]\n    &gt;&gt;&gt; m = Ridge().fit(X, y_obs)\n    &gt;&gt;&gt; compute_marginal(y_obs=y_obs, y_pred=m.predict(X), X=X, feature_name=0,\n    ... predict_function=m.predict)\n    shape: (3, 9)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 feature  \u2506 y_obs_m \u2506 y_pred_ \u2506 y_obs_s \u2506 \u2026 \u2506 count \u2506 weights \u2506 bin_edg \u2506 partial \u2502\n    \u2502 0        \u2506 ean     \u2506 mean    \u2506 tderr   \u2506   \u2506 ---   \u2506 ---     \u2506 es      \u2506 _depend \u2502\n    \u2502 ---      \u2506 ---     \u2506 ---     \u2506 ---     \u2506   \u2506 u32   \u2506 f64     \u2506 ---     \u2506 ence    \u2502\n    \u2502 f64      \u2506 f64     \u2506 f64     \u2506 f64     \u2506   \u2506       \u2506         \u2506 array[f \u2506 ---     \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 64, 3]  \u2506 f64     \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 0.5      \u2506 0.0     \u2506 0.125   \u2506 0.0     \u2506 \u2026 \u2506 2     \u2506 2.0     \u2506 [0.0,   \u2506 0.25    \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.5,    \u2506         \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 1.0]    \u2506         \u2502\n    \u2502 2.0      \u2506 1.0     \u2506 0.75    \u2506 0.0     \u2506 \u2026 \u2506 1     \u2506 1.0     \u2506 [1.0,   \u2506 0.625   \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.0,    \u2506         \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 2.0]    \u2506         \u2502\n    \u2502 3.0      \u2506 1.0     \u2506 1.0     \u2506 0.0     \u2506 \u2026 \u2506 1     \u2506 1.0     \u2506 [2.0,   \u2506 0.875   \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 0.0,    \u2506         \u2502\n    \u2502          \u2506         \u2506         \u2506         \u2506   \u2506       \u2506         \u2506 3.0]    \u2506         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    validate_same_first_dimension(y_obs, y_pred)\n    n_pred = length_of_second_dimension(y_pred)\n    pred_names, _ = get_sorted_array_names(y_pred)\n    y_obs = np.asarray(y_obs)\n\n    if weights is not None:\n        validate_same_first_dimension(weights, y_obs)\n        w = np.asarray(weights)\n        if w.ndim &gt; 1:\n            msg = f\"The array weights must be 1-dimensional, got weights.ndim={w.ndim}.\"\n            raise ValueError(msg)\n    else:\n        w = np.ones_like(y_obs, dtype=float)\n\n    if feature_name is None:\n        # X is completely ignored.\n        feature_input = feature = None\n    elif X is None:\n        msg = (\n            \"X must be a data container like a (polars) dataframe or an (numpy) array.\"\n        )\n        raise ValueError(msg)\n    elif not isinstance(feature_name, (int, str)):\n        msg = f\"The argument 'feature_name' must be an int or str; got {feature_name}\"\n        raise ValueError(msg)\n    elif isinstance(feature_name, int):\n        feature_index = feature_name\n        feature_input = get_second_dimension(X, feature_name)\n    else:\n        X_names, _ = get_sorted_array_names(X)\n        feature_index = X_names.index(feature_name)\n        feature_input = get_second_dimension(X, feature_index)\n\n    n_obs = length_of_first_dimension(y_pred)\n    df_list = []\n    with pl.StringCache():\n        if feature_input is not None:\n            feature, n_bins, f_binned = bin_feature(\n                feature=feature_input,\n                feature_name=feature_name,\n                n_obs=n_obs,\n                n_bins=n_bins,\n                bin_method=bin_method,\n            )\n            feature_name = feature.name\n            is_cat_or_string = feature.dtype in [\n                pl.Categorical,\n                pl.Enum,\n                pl.Utf8,\n                pl.Object,\n            ]\n\n        for i in range(len(pred_names)):\n            # Loop over columns of y_pred.\n            x = np.asarray(y_pred if n_pred == 0 else get_second_dimension(y_pred, i))\n\n            if feature is None:\n                y_obs_mean = np.average(y_obs, weights=w)\n                y_pred_mean = np.average(x, weights=w)\n                weights_sum = np.sum(w)\n                count = y_obs.shape[0]\n                # Note: with Bessel correction\n                y_obs_stddev = np.average(\n                    (y_obs - y_obs_mean) ** 2, weights=w\n                ) / np.amax([1, count - 1])\n                y_pred_stddev = np.average((x - y_pred_mean) ** 2, weights=w) / np.amax(\n                    [1, count - 1]\n                )\n                df = pl.DataFrame(\n                    {\n                        \"y_obs_mean\": [y_obs_mean],\n                        \"y_pred_mean\": [y_pred_mean],\n                        \"count\": pl.Series([count], dtype=pl.UInt32),\n                        \"weights\": [weights_sum],\n                        \"y_obs_stderr\": [np.sqrt(y_obs_stddev)],\n                        \"y_pred_stderr\": [np.sqrt(y_pred_stddev)],\n                    }\n                )\n            else:\n                df = pl.DataFrame(\n                    {\n                        \"y_obs\": y_obs,\n                        \"y_pred\": x,\n                        feature_name: feature,\n                        \"weights\": w,\n                    }\n                )\n\n                agg_list = [\n                    pl.count(\"y_obs\").alias(\"count\"),\n                    pl.col(\"weights\").sum().alias(\"weights_sum\"),\n                    *chain.from_iterable(\n                        [\n                            pl.col(c + \"_mean\").first(),\n                            (\n                                (\n                                    pl.col(\"weights\")\n                                    * ((pl.col(c) - pl.col(c + \"_mean\")) ** 2)\n                                ).sum()\n                                / pl.col(\"weights\").sum()\n                            ).alias(c + \"_variance\"),\n                        ]\n                        for c in [\"y_obs\", \"y_pred\"]\n                    ),\n                ]\n\n                groupby_name = \"bin\"\n                df = df.hstack([f_binned.get_column(\"bin\")])\n                if not is_cat_or_string:\n                    # We also add the bin edges.\n                    df = df.hstack([f_binned.get_column(\"bin_edges\")])\n                    agg_list += [\n                        pl.col(feature_name).mean(),\n                        pl.col(feature_name).std(ddof=0).alias(\"__feature_std\"),\n                        pl.col(\"bin_edges\").first(),\n                    ]\n\n                df = (\n                    df.lazy()\n                    .select(\n                        pl.all(),\n                        (\n                            (pl.col(\"weights\") * pl.col(\"y_obs\"))\n                            .sum()\n                            .over(groupby_name)\n                            / pl.col(\"weights\").sum().over(groupby_name)\n                        ).alias(\"y_obs_mean\"),\n                        (\n                            (pl.col(\"weights\") * pl.col(\"y_pred\"))\n                            .sum()\n                            .over(groupby_name)\n                            / pl.col(\"weights\").sum().over(groupby_name)\n                        ).alias(\"y_pred_mean\"),\n                    )\n                    .group_by(groupby_name)\n                    .agg(agg_list)\n                    .with_columns(\n                        [\n                            pl.when(pl.col(\"count\") &gt; 1)\n                            .then(pl.col(c + \"_variance\") / (pl.col(\"count\") - 1))\n                            .otherwise(pl.col(c + \"_variance\"))\n                            .sqrt()\n                            .alias(c + \"_stderr\")\n                            for c in (\"y_obs\", \"y_pred\")\n                        ]\n                    )\n                )\n\n                if is_cat_or_string:\n                    df = df.with_columns(pl.col(groupby_name).alias(feature_name))\n\n                # With sort and head alone, we could lose the null value, but we\n                # want to keep it.\n                # .sort(\"bias_count\", descending=True)\n                # .head(n_bins)\n                df = (\n                    df.with_columns(\n                        pl.when(pl.col(feature_name).is_null())\n                        .then(pl.max(\"count\") + 1)\n                        .otherwise(pl.col(\"count\"))\n                        .alias(\"__priority\")\n                    )\n                    .sort(\"__priority\", descending=True)\n                    .head(n_bins)\n                    .sort(feature_name, descending=False)\n                    .select(\n                        pl.col(feature_name),\n                        pl.col(\"y_obs_mean\"),\n                        pl.col(\"y_pred_mean\"),\n                        pl.col(\"y_obs_stderr\"),\n                        pl.col(\"y_pred_stderr\"),\n                        pl.col(\"weights_sum\").alias(\"weights\"),\n                        pl.col(\"count\"),\n                        *(\n                            []\n                            if is_cat_or_string\n                            else [pl.col(\"bin_edges\"), pl.col(\"__feature_std\")]\n                        ),\n                    )\n                )\n\n                if not is_cat_or_string:\n                    df = df.with_columns(\n                        pl.when(pl.col(\"bin_edges\").is_null())\n                        .then(\n                            pl.concat_list(\n                                pl.lit(None),\n                                pl.col(\"__feature_std\"),\n                                pl.lit(None),\n                            )\n                        )\n                        .otherwise(\n                            pl.concat_list(\n                                pl.col(\"bin_edges\").arr.first(),\n                                pl.col(\"__feature_std\"),\n                                pl.col(\"bin_edges\").arr.last(),\n                            )\n                        )\n                        .list.to_array(3)\n                        .alias(\"bin_edges\")\n                    )\n                df = df.collect()\n\n            # Add column \"model\".\n            if n_pred &gt; 0:\n                model_col_name = \"model_\" if feature_name == \"model\" else \"model\"\n                df = df.with_columns(\n                    pl.Series(model_col_name, [pred_names[i]] * df.shape[0])\n                )\n\n            # Add partial dependence.\n            with_pd = predict_function is not None and feature_name is not None\n            if with_pd:\n                # In case we have \"other n\" string/cat/enum, we must exclude it from pd\n                # because it is an artificual value and not part of the real data.\n                has_rest_n = (\n                    is_cat_or_string and \"other \" in df.get_column(feature_name)[-1]\n                )\n                if has_rest_n:\n                    # Note that null, if present, is the first not the last values.\n                    grid = df.get_column(feature_name)[:-1]\n                else:\n                    grid = df.get_column(feature_name)\n                pd_values = compute_partial_dependence(\n                    pred_fun=predict_function,  # type: ignore\n                    X=X,  # type: ignore\n                    feature_index=feature_index,\n                    grid=grid,\n                    weights=weights,\n                    n_max=n_max,\n                    rng=rng,\n                )\n                if has_rest_n:\n                    pd_values = pl.concat([pl.Series(pd_values), pl.Series([None])])\n                df = df.with_columns(\n                    pl.Series(name=\"partial_dependence\", values=pd_values)\n                )\n\n            # Select the columns in the correct order.\n            col_selection = []\n            if n_pred &gt; 0:\n                col_selection.append(model_col_name)\n            if feature_name is not None and feature_name in df.columns:\n                col_selection.append(str(feature_name))\n            col_selection += [\n                \"y_obs_mean\",\n                \"y_pred_mean\",\n                \"y_obs_stderr\",\n                \"y_pred_stderr\",\n                \"count\",\n                \"weights\",\n            ]\n            if feature_name in df.columns and not is_cat_or_string:\n                col_selection += [\"bin_edges\"]\n            if with_pd:\n                col_selection += [\"partial_dependence\"]\n            df_list.append(df.select(col_selection))\n\n        df = pl.concat(df_list)\n    return df\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/identification/#model_diagnostics.calibration.identification.identification_function","title":"<code>identification_function(y_obs, y_pred, *, functional='mean', level=0.5)</code>","text":"<p>Canonical identification function.</p> <p>Identification functions act as generalised residuals. See Notes for further details.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code>, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile of quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>V</code> <code>ndarray of shape (n_obs)</code> <p>Values of the identification function.</p> Notes <p> The function  The function \\(V(y, z)\\) for observation \\(y=y_{pred}\\) and prediction \\(z=y_{pred}\\) is a strict identification function for the functional \\(T\\), or induces the functional \\(T\\) as:</p> \\[ \\mathbb{E}[V(Y, z)] = 0\\quad \\Leftrightarrow\\quad z\\in T(F) \\quad \\forall \\text{ distributions } F \\in \\mathcal{F} \\] <p>for some class of distributions \\(\\mathcal{F}\\). Implemented examples of the functional \\(T\\) are mean, median, expectiles and quantiles.</p> functional strict identification function \\(V(y, z)\\) mean \\(z - y\\) median \\(\\mathbf{1}\\{z \\ge y\\} - \\frac{1}{2}\\) expectile \\(2 \\mid\\mathbf{1}\\{z \\ge y\\} - \\alpha\\mid (z - y)\\) quantile \\(\\mathbf{1}\\{z \\ge y\\} - \\alpha\\) <p>For <code>level</code> \\(\\alpha\\).</p> References <code>[Gneiting2011]</code> <p>T. Gneiting. \"Making and Evaluating Point Forecasts\". (2011) doi:10.1198/jasa.2011.r10138 arxiv:0912.0902</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; identification_function(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\narray([-1,  1,  0,  1])\n</code></pre> Source code in <code>src/model_diagnostics/calibration/identification.py</code> <pre><code>def identification_function(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    *,\n    functional: str = \"mean\",\n    level: float = 0.5,\n) -&gt; np.ndarray:\n    r\"\"\"Canonical identification function.\n\n    Identification functions act as generalised residuals. See [Notes](#notes) for\n    further details.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional`, e.g. the conditional expectation of\n        the response, `E(Y|X)`.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n\n    level : float\n        The level of the expectile of quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n\n    Returns\n    -------\n    V : ndarray of shape (n_obs)\n        Values of the identification function.\n\n    Notes\n    -----\n    [](){#notes}\n    The function \\(V(y, z)\\) for observation \\(y=y_{pred}\\) and prediction\n    \\(z=y_{pred}\\) is a strict identification function for the functional \\(T\\), or\n    induces the functional \\(T\\) as:\n\n    \\[\n    \\mathbb{E}[V(Y, z)] = 0\\quad \\Leftrightarrow\\quad z\\in T(F) \\quad \\forall\n    \\text{ distributions } F\n    \\in \\mathcal{F}\n    \\]\n\n    for some class of distributions \\(\\mathcal{F}\\). Implemented examples of the\n    functional \\(T\\) are mean, median, expectiles and quantiles.\n\n    | functional | strict identification function \\(V(y, z)\\)           |\n    | ---------- | ---------------------------------------------------- |\n    | mean       | \\(z - y\\)                                            |\n    | median     | \\(\\mathbf{1}\\{z \\ge y\\} - \\frac{1}{2}\\)              |\n    | expectile  | \\(2 \\mid\\mathbf{1}\\{z \\ge y\\} - \\alpha\\mid (z - y)\\) |\n    | quantile   | \\(\\mathbf{1}\\{z \\ge y\\} - \\alpha\\)                   |\n\n    For `level` \\(\\alpha\\).\n\n    References\n    ----------\n    `[Gneiting2011]`\n\n    :   T. Gneiting.\n        \"Making and Evaluating Point Forecasts\". (2011)\n        [doi:10.1198/jasa.2011.r10138](https://doi.org/10.1198/jasa.2011.r10138)\n        [arxiv:0912.0902](https://arxiv.org/abs/0912.0902)\n\n    Examples\n    --------\n    &gt;&gt;&gt; identification_function(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n    array([-1,  1,  0,  1])\n    \"\"\"\n    y_o: np.ndarray\n    y_p: np.ndarray\n    y_o, y_p = validate_2_arrays(y_obs, y_pred)\n\n    if functional in (\"expectile\", \"quantile\") and (level &lt;= 0 or level &gt;= 1):\n        msg = f\"Argument level must fulfil 0 &lt; level &lt; 1, got {level}.\"\n        raise ValueError(msg)\n\n    if functional == \"mean\":\n        return y_p - y_o\n    elif functional == \"median\":\n        return np.greater_equal(y_p, y_o) - 0.5\n    elif functional == \"expectile\":\n        return 2 * np.abs(np.greater_equal(y_p, y_o) - level) * (y_p - y_o)\n    elif functional == \"quantile\":\n        return np.greater_equal(y_p, y_o) - level\n    else:\n        allowed_functionals = (\"mean\", \"median\", \"expectile\", \"quantile\")\n        msg = (\n            f\"Argument functional must be one of {allowed_functionals}, got \"\n            f\"{functional}.\"\n        )\n        raise ValueError(msg)\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/plots/","title":"plots","text":""},{"location":"reference/model_diagnostics/calibration/plots/#model_diagnostics.calibration.plots","title":"<code>plots</code>","text":""},{"location":"reference/model_diagnostics/calibration/plots/#model_diagnostics.calibration.plots.add_marginal_subplot","title":"<code>add_marginal_subplot(subfig, fig, row, col)</code>","text":"<p>Add a plotly subplot from plot_marginal to a multi-plot figure.</p> <p>This auxiliary function is accompanies <code>plot_marginal</code> in order to ease plotting with subfigures with the plotly backend.</p> <p>For it to work, you must call plotly's <code>make_subplots</code> with the <code>specs</code> argument and set the appropriate number of <code>{\"secondary_y\": True}</code> in a list of lists. <pre><code>from plotly.subplots import make_subplots\n\nn_rows, n_cols = ...\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    specs=[[{\"secondary_y\": True}] * n_cols] * n_rows,  # This is important!\n)\n</code></pre> The reason is that <code>plot_marginal</code> uses a secondary yaxis (and swapped sides with the primary yaxis).</p> <p>Parameters:</p> Name Type Description Default <code>subfig</code> <code>plotly Figure</code> <p>The subfigure which is added to <code>fig</code>.</p> required <code>fig</code> <code>plotly Figure</code> <p>The multi-plot figure to which <code>subfig</code> is added at positions <code>row</code> and <code>col</code>.</p> required <code>row</code> <code>int</code> <p>The (0-based) row index of <code>fig</code> at which <code>subfig</code> is added.</p> required <code>col</code> <code>int</code> <p>The (0-based) column index of <code>fig</code> at which <code>subfig</code> is added.</p> required <p>Returns:</p> Type Description <code>fig</code> <p>The plotly figure <code>fig</code>.</p> Source code in <code>src/model_diagnostics/calibration/plots.py</code> <pre><code>def add_marginal_subplot(subfig, fig, row: int, col: int):\n    \"\"\"Add a plotly subplot from plot_marginal to a multi-plot figure.\n\n    This auxiliary function is accompanies\n    [`plot_marginal`][model_diagnostics.calibration.plot_marginal] in order to ease\n    plotting with subfigures with the plotly backend.\n\n    For it to work, you must call plotly's `make_subplots` with the `specs` argument\n    and set the appropriate number of `{\"secondary_y\": True}` in a list of lists.\n    ```py hl_lines=\"7\"\n    from plotly.subplots import make_subplots\n\n    n_rows, n_cols = ...\n    fig = make_subplots(\n        rows=n_rows,\n        cols=n_cols,\n        specs=[[{\"secondary_y\": True}] * n_cols] * n_rows,  # This is important!\n    )\n    ```\n    The reason is that `plot_marginal` uses a secondary yaxis (and swapped sides with\n    the primary yaxis).\n\n    Parameters\n    ----------\n    subfig : plotly Figure\n        The subfigure which is added to `fig`.\n    fig : plotly Figure\n        The multi-plot figure to which `subfig` is added at positions `row` and `col`.\n    row : int\n        The (0-based) row index of `fig` at which `subfig` is added.\n    col : int\n        The (0-based) column index of `fig` at which `subfig` is added.\n\n    Returns\n    -------\n    fig\n        The plotly figure `fig`.\n    \"\"\"\n    # It returns a tuple of `range`s starting at 1.\n    plotly_rows, plotly_cols = fig._get_subplot_rows_columns()  # noqa: SLF001\n    n_rows = len(plotly_rows)\n    n_cols = len(plotly_cols)\n    if row &gt;= n_rows or col &gt;= n_cols:\n        msg = (\n            f\"The `fig` only has {n_rows} rows and {n_cols} columns. You specified \"\n            f\"(0-based) {row=} and {col=}.\"\n        )\n        raise ValueError(msg)\n    i = n_cols * row + col\n    # Plotly uses 1-based indices:\n    row += 1\n    col += 1\n    # Transfer the x-axis titles of the subfig to fig.\n    xaxis = \"xaxis\" if i == 0 else f\"xaxis{i + 1}\"\n    fig[\"layout\"][xaxis][\"title\"] = subfig[\"layout\"][\"xaxis\"][\"title\"]\n    # Change sides of y-axis.\n    yaxis = \"yaxis\" if i == 0 else f\"yaxis{2 * i + 1}\"\n    yaxis2 = f\"yaxis{2 * (i + 1)}\"\n    fig.update_layout(\n        **{\n            yaxis: {\"side\": \"right\", \"showgrid\": False},\n            yaxis2: {\"side\": \"left\", \"title\": \"y\"},\n        }\n    )\n    # Only the last added subfig should show the legends, but all the ones before\n    # should not.\n    # So don't show legends for row-1 and col-1.\n    if row &gt; 1:\n        fig.update_traces(patch={\"showlegend\": False}, row=row - 1, col=col)\n    if col &gt; 1:\n        fig.update_traces(patch={\"showlegend\": False}, row=row, col=col - 1)\n    for d in subfig.data:\n        fig.add_trace(d, row=row, col=col, secondary_y=d[\"yaxis\"] == \"y2\")\n\n    fig.update_layout(title=subfig.layout.title.text)\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/plots/#model_diagnostics.calibration.plots.plot_bias","title":"<code>plot_bias(y_obs, y_pred, feature=None, weights=None, *, functional='mean', level=0.5, n_bins=10, bin_method='sturges', confidence_level=0.9, ax=None)</code>","text":"<p>Plot model bias conditional on a feature.</p> <p>This plots the generalised bias (residuals), i.e. the values of the canonical identification function, versus a feature. This is a good way to assess whether a model is conditionally calibrated or not. Well calibrated models have bias terms around zero. See Notes for further details.</p> <p>For numerical features, NaN are treated as Null values. Null values are always plotted as rightmost value on the x-axis and marked with a diamond instead of a dot.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>feature</code> <code>array-like of shape (n_obs) or None</code> <p>Some feature column.</p> <code>None</code> <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights. If given, the bias is calculated as weighted average of the identification function with these weights. Note that the standard errors and p-values in the output are based on the assumption that the variance of the bias is inverse proportional to the weights. See the Notes section for details.</p> <code>None</code> <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile or quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt;= level &lt;= 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> <code>n_bins</code> <code>int</code> <p>The number of bins, at least 2. For numerical features, <code>n_bins</code> only applies when <code>bin_method</code> is set to <code>\"quantile\"</code> or <code>\"uniform\"</code>. For string-like and categorical features, the most frequent values are taken. Ties are dealt with by taking the first value in natural sorting order. The remaining values are merged into <code>\"other n\"</code> with <code>n</code> indicating the unique count.</p> <p>I present, null values are always included in the output, accounting for one bin. NaN values are treated as null values.</p> <code>10</code> <code>bin_method</code> <code>str</code> <p>The method for finding bin edges (boundaries). Options using <code>n_bins</code> are:</p> <ul> <li><code>\"quantile\"</code></li> <li><code>\"uniform\"</code></li> </ul> <p>Options automatically selecting the number of bins for numerical features thereby using uniform bins are same options as numpy.histogram_bin_edges:</p> <ul> <li><code>\"auto\"</code> Minimum bin width between the <code>\"sturges\"</code> and <code>\"fd\"</code> estimators. Provides good all-around performance.</li> <li><code>\"fd\"</code> (Freedman Diaconis Estimator) Robust (resilient to outliers) estimator that takes into account data variability and data size.</li> <li><code>\"doane\"</code> An improved version of Sturges' estimator that works better with non-normal datasets.</li> <li><code>\"scott\"</code> Less robust estimator that takes into account data variability and data size.</li> <li><code>\"stone\"</code> Estimator based on leave-one-out cross-validation estimate of the integrated squared error. Can be regarded as a generalization of Scott's rule.</li> <li><code>\"rice\"</code> Estimator does not take variability into account, only data size. Commonly overestimates number of bins required.</li> <li><code>\"sturges\"</code> R's default method, only accounts for data size. Only optimal for gaussian data and underestimates number of bins for large non-gaussian datasets.</li> <li><code>\"sqrt\"</code> Square root (of data size) estimator, used by Excel and other programs for its speed and simplicity.</li> </ul> <code>'sturges'</code> <code>confidence_level</code> <code>float</code> <p>Confidence level for error bars. If 0, no error bars are plotted. Value must fulfil <code>0 &lt;= confidence_level &lt; 1</code>.</p> <code>0.9</code> <code>ax</code> <code>matplotlib.axes.Axes or plotly Figure</code> <p>Axes object to draw the plot onto, otherwise uses the current Axes.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <p>Either the matplotlib axes or the plotly figure. This is configurable by setting the <code>plot_backend</code> via <code>model_diagnostics.set_config</code> or <code>model_diagnostics.config_context</code>.</p> Notes <p> A model  A model \\(m(X)\\) is conditionally calibrated iff \\(E(V(m(X), Y))=0\\) a.s. The empirical version, given some data, reads \\(\\frac{1}{n}\\sum_i V(m(x_i), y_i)\\). See <code>[FLM2022]</code>.</p> References <code>FLM2022</code> <p>T. Fissler, C. Lorentzen, and M. Mayer. \"Model Comparison and Calibration Assessment\". (2022) arxiv:2202.12780.</p> Source code in <code>src/model_diagnostics/calibration/plots.py</code> <pre><code>def plot_bias(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    feature: Optional[npt.ArrayLike] = None,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    functional: str = \"mean\",\n    level: float = 0.5,\n    n_bins: int = 10,\n    bin_method: str = \"sturges\",\n    confidence_level: float = 0.9,\n    ax: Optional[mpl.axes.Axes] = None,\n):\n    r\"\"\"Plot model bias conditional on a feature.\n\n    This plots the generalised bias (residuals), i.e. the values of the canonical\n    identification function, versus a feature. This is a good way to assess whether\n    a model is conditionally calibrated or not. Well calibrated models have bias terms\n    around zero.\n    See [Notes](#notes) for further details.\n\n    For numerical features, NaN are treated as Null values. Null values are always\n    plotted as rightmost value on the x-axis and marked with a diamond instead of a\n    dot.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    feature : array-like of shape (n_obs) or None\n        Some feature column.\n    weights : array-like of shape (n_obs) or None\n        Case weights. If given, the bias is calculated as weighted average of the\n        identification function with these weights.\n        Note that the standard errors and p-values in the output are based on the\n        assumption that the variance of the bias is inverse proportional to the\n        weights. See the Notes section for details.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n\n    level : float\n        The level of the expectile or quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt;= level &lt;= 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n    n_bins : int\n        The number of bins, at least 2. For numerical features, `n_bins` only applies\n        when `bin_method` is set to `\"quantile\"` or `\"uniform\"`.\n        For string-like and categorical features, the most frequent values are taken.\n        Ties are dealt with by taking the first value in natural sorting order.\n        The remaining values are merged into `\"other n\"` with `n` indicating the unique\n        count.\n\n        I present, null values are always included in the output, accounting for one\n        bin. NaN values are treated as null values.\n    bin_method : str\n        The method for finding bin edges (boundaries). Options using `n_bins` are:\n\n        - `\"quantile\"`\n        - `\"uniform\"`\n\n        Options automatically selecting the number of bins for numerical features\n        thereby using uniform bins are same options as\n        [numpy.histogram_bin_edges](https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html):\n\n        - `\"auto\"`\n        Minimum bin width between the `\"sturges\"` and `\"fd\"` estimators. Provides good\n        all-around performance.\n        - `\"fd\"` (Freedman Diaconis Estimator)\n        Robust (resilient to outliers) estimator that takes into account data\n        variability and data size.\n        - `\"doane\"`\n        An improved version of Sturges' estimator that works better with non-normal\n        datasets.\n        - `\"scott\"`\n        Less robust estimator that takes into account data variability and data size.\n        - `\"stone\"`\n        Estimator based on leave-one-out cross-validation estimate of the integrated\n        squared error. Can be regarded as a generalization of Scott's rule.\n        - `\"rice\"`\n        Estimator does not take variability into account, only data size. Commonly\n        overestimates number of bins required.\n        - `\"sturges\"`\n        R's default method, only accounts for data size. Only optimal for gaussian data\n        and underestimates number of bins for large non-gaussian datasets.\n        - `\"sqrt\"`\n        Square root (of data size) estimator, used by Excel and other programs for its\n        speed and simplicity.\n\n    confidence_level : float\n        Confidence level for error bars. If 0, no error bars are plotted. Value must\n        fulfil `0 &lt;= confidence_level &lt; 1`.\n    ax : matplotlib.axes.Axes or plotly Figure\n        Axes object to draw the plot onto, otherwise uses the current Axes.\n\n    Returns\n    -------\n    ax :\n        Either the matplotlib axes or the plotly figure. This is configurable by\n        setting the `plot_backend` via\n        [`model_diagnostics.set_config`][model_diagnostics.set_config] or\n        [`model_diagnostics.config_context`][model_diagnostics.config_context].\n\n    Notes\n    -----\n    [](){#notes}\n    A model \\(m(X)\\) is conditionally calibrated iff \\(E(V(m(X), Y))=0\\) a.s. The\n    empirical version, given some data, reads \\(\\frac{1}{n}\\sum_i V(m(x_i), y_i)\\).\n    See `[FLM2022]`.\n\n    References\n    ----------\n    `FLM2022`\n\n    :   T. Fissler, C. Lorentzen, and M. Mayer.\n        \"Model Comparison and Calibration Assessment\". (2022)\n        [arxiv:2202.12780](https://arxiv.org/abs/2202.12780).\n    \"\"\"\n    if not (0 &lt;= confidence_level &lt; 1):\n        msg = (\n            f\"Argument confidence_level must fulfil 0 &lt;= level &lt; 1, got \"\n            f\"{confidence_level}.\"\n        )\n        raise ValueError(msg)\n    with_errorbars = confidence_level &gt; 0\n\n    if ax is None:\n        plot_backend = get_config()[\"plot_backend\"]\n        if plot_backend == \"matplotlib\":\n            ax = plt.gca()\n        else:\n            import plotly.graph_objects as go\n\n            fig = ax = go.Figure()\n    elif isinstance(ax, mpl.axes.Axes):\n        plot_backend = \"matplotlib\"\n    elif is_plotly_figure(ax):\n        import plotly.graph_objects as go\n\n        plot_backend = \"plotly\"\n        fig = ax\n    else:\n        msg = (\n            \"The ax argument must be None, a matplotlib Axes or a plotly Figure, \"\n            f\"got {type(ax)}.\"\n        )\n        raise ValueError(msg)\n\n    df = compute_bias(\n        y_obs=y_obs,\n        y_pred=y_pred,\n        feature=feature,\n        weights=weights,\n        functional=functional,\n        level=level,\n        n_bins=n_bins,\n        bin_method=bin_method,\n    )\n\n    if df[\"bias_stderr\"].fill_nan(None).null_count() &gt; 0 and with_errorbars:\n        msg = (\n            \"Some values of 'bias_stderr' are null. Therefore no error bars are \"\n            \"shown for that y_pred/model, despite the fact that confidence_level&gt;0 \"\n            \"was set to True.\"\n        )\n        warnings.warn(msg, UserWarning, stacklevel=2)\n\n    if \"model_\" in df.columns:\n        col_model = \"model_\"\n    elif \"model\" in df.columns:\n        col_model = \"model\"\n    else:\n        col_model = None\n\n    if feature is None:\n        # We treat the predictions from different models as a feature.\n        feature_name = col_model\n        feature_has_nulls = False\n    else:\n        feature_name = array_name(feature, default=\"feature\")\n        feature_has_nulls = df[feature_name].null_count() &gt; 0\n\n    is_categorical = False\n    is_string = False\n    feature_dtype = df.get_column(feature_name).dtype\n    if feature_dtype in [pl.Categorical, pl.Enum]:\n        is_categorical = True\n    elif feature_dtype in [pl.Utf8, pl.Object]:\n        is_string = True\n\n    n_x = df[feature_name].n_unique()\n\n    # horizontal line at y=0\n    if plot_backend == \"matplotlib\":\n        ax.axhline(y=0, xmin=0, xmax=1, color=\"k\", linestyle=\"dotted\")\n    else:\n        fig.add_hline(y=0, line={\"color\": \"black\", \"dash\": \"dot\"}, showlegend=False)\n\n    # bias plot\n    if feature is None or col_model is None:\n        pred_names = [None]\n    else:\n        # pred_names = df[col_model].unique() this automatically sorts\n        pred_names, _ = get_sorted_array_names(y_pred)\n    n_models = len(pred_names)\n    with_label = feature is not None and (n_models &gt;= 2 or feature_has_nulls)\n\n    if (is_string or is_categorical) and feature_has_nulls:\n        # We want the Null values at the end and therefore sort again.\n        df = df.sort(feature_name, descending=False, nulls_last=True)\n\n    for i, m in enumerate(pred_names):\n        filter_condition = True if m is None else pl.col(col_model) == m\n        df_i = df.filter(filter_condition)\n        label = m if with_label else None\n\n        if df_i[\"bias_stderr\"].null_count() &gt; 0:\n            with_errorbars_i = False\n        else:\n            with_errorbars_i = with_errorbars\n\n        if with_errorbars_i:\n            # We scale bias_stderr by the corresponding value of the t-distribution\n            # to get our desired confidence level.\n            n = df_i[\"bias_count\"].to_numpy()\n            conf_level_fct = special.stdtrit(\n                np.maximum(n - 1, 1),  # degrees of freedom, if n=0 =&gt; bias_stderr=0.\n                1 - (1 - confidence_level) / 2,\n            )\n            df_i = df_i.with_columns(\n                [(pl.col(\"bias_stderr\") * conf_level_fct).alias(\"bias_stderr\")]\n            )\n\n        if is_string or is_categorical:\n            df_ii = df_i.filter(pl.col(feature_name).is_not_null())\n            # We x-shift a little for a better visual.\n            span = (n_x - 1) / n_x / n_models  # length for one cat value and one model\n            x = np.arange(n_x - feature_has_nulls)\n            if n_models &gt; 1:\n                x = x + (i - n_models // 2) * span * 0.5\n            if plot_backend == \"matplotlib\":\n                ax.errorbar(\n                    x,\n                    df_ii[\"bias_mean\"],\n                    yerr=df_ii[\"bias_stderr\"] if with_errorbars_i else None,\n                    marker=\"o\",\n                    linestyle=\"None\",\n                    capsize=4,\n                    label=label,\n                )\n            else:\n                fig.add_scatter(\n                    x=x,\n                    y=df_ii[\"bias_mean\"],\n                    error_y={\n                        \"type\": \"data\",  # value of error bar given in data coordinates\n                        \"array\": df_ii[\"bias_stderr\"] if with_errorbars_i else None,\n                        \"width\": 4,\n                        \"visible\": True,\n                    },\n                    marker={\"color\": get_plotly_color(i)},\n                    mode=\"markers\",\n                    name=label,\n                )\n        else:\n            if with_errorbars_i:\n                lower = df_i[\"bias_mean\"] - df_i[\"bias_stderr\"]\n                upper = df_i[\"bias_mean\"] + df_i[\"bias_stderr\"]\n                if plot_backend == \"matplotlib\":\n                    ax.fill_between(\n                        df_i[feature_name],\n                        lower,\n                        upper,\n                        alpha=0.1,\n                    )\n                else:\n                    # plotly has no equivalent of fill_between and needs a bit more\n                    # coding\n                    color = get_plotly_color(i)\n                    fig.add_scatter(\n                        x=pl.concat([df_i[feature_name], df_i[::-1, feature_name]]),\n                        y=pl.concat([lower, upper[::-1]]),\n                        fill=\"toself\",\n                        fillcolor=color,\n                        hoverinfo=\"skip\",\n                        line={\"color\": color},\n                        mode=\"lines\",\n                        opacity=0.1,\n                        showlegend=False,\n                    )\n            if plot_backend == \"matplotlib\":\n                ax.plot(\n                    df_i[feature_name],\n                    df_i[\"bias_mean\"],\n                    linestyle=\"solid\",\n                    marker=\"o\",\n                    label=label,\n                )\n            else:\n                fig.add_scatter(\n                    x=df_i[feature_name],\n                    y=df_i[\"bias_mean\"],\n                    marker_symbol=\"circle\",\n                    mode=\"lines+markers\",\n                    line={\"color\": get_plotly_color(i)},\n                    name=label,\n                )\n\n        if feature_has_nulls:\n            # Null values are plotted as diamonds as rightmost point.\n            df_i_null = df_i.filter(pl.col(feature_name).is_null())\n\n            if is_string or is_categorical:\n                x_null = np.array([n_x - 1])\n            else:\n                x_min = df_i[feature_name].min()\n                x_max = df_i[feature_name].max()\n                if n_x == 1:\n                    # df_i[feature_name] is the null value.\n                    x_null, span = np.array([0]), 1\n                elif n_x == 2:\n                    x_null, span = np.array([2 * x_max]), 0.5 * x_max / n_models\n                else:\n                    x_null = np.array([x_max + (x_max - x_min) / n_x])\n                    span = (x_null - x_max) / n_models\n\n            if n_models &gt; 1:\n                x_null = x_null + (i - n_models // 2) * span * 0.5\n\n            if plot_backend == \"matplotlib\":\n                color = ax.get_lines()[-1].get_color()  # previous line color\n                ax.errorbar(\n                    x_null,\n                    df_i_null[\"bias_mean\"],\n                    yerr=df_i_null[\"bias_stderr\"] if with_errorbars_i else None,\n                    marker=\"D\",\n                    linestyle=\"None\",\n                    capsize=4,\n                    label=None,\n                    color=color,\n                )\n            else:\n                fig.add_scatter(\n                    x=x_null,\n                    y=df_i_null[\"bias_mean\"],\n                    error_y={\n                        \"type\": \"data\",  # value of error bar given in data coordinates\n                        \"array\": df_i_null[\"bias_stderr\"] if with_errorbars_i else None,\n                        \"width\": 4,\n                        \"visible\": True,\n                    },\n                    marker={\"color\": get_plotly_color(i), \"symbol\": \"diamond\"},\n                    mode=\"markers\",\n                    showlegend=False,\n                )\n\n    if is_categorical or is_string:\n        if feature_has_nulls:\n            # Without cast to pl.Uft8, the following error might occur:\n            # exceptions.ComputeError: cannot combine categorical under a global string\n            # cache with a non cached categorical\n            tick_labels = df_i[feature_name].cast(pl.Utf8).fill_null(\"Null\")\n        else:\n            tick_labels = df_i[feature_name]\n        x_label = feature_name\n        if plot_backend == \"matplotlib\":\n            ax.set_xticks(np.arange(n_x), labels=tick_labels)\n        else:\n            fig.update_layout(\n                xaxis={\n                    \"tickmode\": \"array\",\n                    \"tickvals\": np.arange(n_x),\n                    \"ticktext\": tick_labels,\n                }\n            )\n    elif feature_name is not None:\n        x_label = \"binned \" + feature_name\n    else:\n        x_label = \"\"\n\n    if feature is None:\n        title = \"Bias Plot\"\n    else:\n        model_name = array_name(y_pred, default=\"\")\n        # test for empty string \"\"\n        title = \"Bias Plot\" if not model_name else \"Bias Plot \" + model_name\n\n    if plot_backend == \"matplotlib\":\n        ax.set(xlabel=x_label, ylabel=\"bias\", title=title)\n    else:\n        fig.update_layout(xaxis_title=x_label, yaxis_title=\"bias\", title=title)\n\n    if with_label and plot_backend == \"matplotlib\":\n        if feature_has_nulls:\n            # Add legend entry for diamonds as Null values.\n            # Unfortunately, the Null value legend entry often appears first, but we\n            # want it at the end.\n            ax.scatter([], [], marker=\"D\", color=\"grey\", label=\"Null values\")\n            handles, labels = ax.get_legend_handles_labels()\n            if (labels[-1] != \"Null values\") and \"Null values\" in labels:\n                i = labels.index(\"Null values\")\n                # i can't be the last index\n                labels = labels[:i] + labels[i + 1 :] + [labels[i]]\n                handles = handles[:i] + handles[i + 1 :] + [handles[i]]\n            ax.legend(handles=handles, labels=labels)\n        else:\n            ax.legend()\n    elif with_label and feature_has_nulls:\n        fig.add_scatter(\n            x=[None],\n            y=[None],\n            mode=\"markers\",\n            name=\"Null values\",\n            marker={\"size\": 7, \"color\": \"grey\", \"symbol\": \"diamond\"},\n        )\n\n    return ax\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/plots/#model_diagnostics.calibration.plots.plot_marginal","title":"<code>plot_marginal(y_obs, y_pred, X, feature_name, predict_function=None, weights=None, *, n_bins=10, bin_method='sturges', n_max=1000, rng=None, ax=None, show_lines='numerical')</code>","text":"<p>Plot marginal observed and predicted conditional on a feature.</p> <p>This plot provides a means to inspect a model per feature. The average of observed and predicted are plotted as well as a histogram of the feature.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>X</code> <code>array-like of shape (n_obs, n_features)</code> <p>The dataframe or array of features to be passed to the model predict function.</p> required <code>feature_name</code> <code>str or int</code> <p>Column name (str) or index (int) of feature in <code>X</code>.</p> required <code>predict_function</code> <code>callable or None</code> <p>A callable to get prediction, i.e. <code>predict_function(X)</code>. Used to compute partial dependence. If <code>None</code>, partial dependence is omitted.</p> <code>None</code> <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights. If given, the bias is calculated as weighted average of the identification function with these weights.</p> <code>None</code> <code>n_bins</code> <code>int</code> <p>The number of bins, at least 2. For numerical features, <code>n_bins</code> only applies when <code>bin_method</code> is set to <code>\"quantile\"</code> or <code>\"uniform\"</code>. For string-like and categorical features, the most frequent values are taken. Ties are dealt with by taking the first value in natural sorting order. The remaining values are merged into <code>\"other n\"</code> with <code>n</code> indicating the unique count.</p> <p>I present, null values are always included in the output, accounting for one bin. NaN values are treated as null values.</p> <code>10</code> <code>bin_method</code> <code>str</code> <p>The method for finding bin edges (boundaries). Options using <code>n_bins</code> are:</p> <ul> <li><code>\"quantile\"</code></li> <li><code>\"uniform\"</code></li> </ul> <p>Options automatically selecting the number of bins for numerical features thereby using uniform bins are same options as numpy.histogram_bin_edges:</p> <ul> <li><code>\"auto\"</code> Minimum bin width between the <code>\"sturges\"</code> and <code>\"fd\"</code> estimators. Provides good all-around performance.</li> <li><code>\"fd\"</code> (Freedman Diaconis Estimator) Robust (resilient to outliers) estimator that takes into account data variability and data size.</li> <li><code>\"doane\"</code> An improved version of Sturges' estimator that works better with non-normal datasets.</li> <li><code>\"scott\"</code> Less robust estimator that takes into account data variability and data size.</li> <li><code>\"stone\"</code> Estimator based on leave-one-out cross-validation estimate of the integrated squared error. Can be regarded as a generalization of Scott's rule.</li> <li><code>\"rice\"</code> Estimator does not take variability into account, only data size. Commonly overestimates number of bins required.</li> <li><code>\"sturges\"</code> R's default method, only accounts for data size. Only optimal for gaussian data and underestimates number of bins for large non-gaussian datasets.</li> <li><code>\"sqrt\"</code> Square root (of data size) estimator, used by Excel and other programs for its speed and simplicity.</li> </ul> <code>'sturges'</code> <code>n_max</code> <code>int or None</code> <p>Used only for partial dependence computation. The number of rows to subsample from X. This speeds up computation, in particular for slow predict functions.</p> <code>1000</code> <code>rng</code> <code>(Generator, int or None)</code> <p>Used only for partial dependence computation. The random number generator used for subsampling of <code>n_max</code> rows. The input is internally wrapped by <code>np.random.default_rng(rng)</code>.</p> <code>None</code> <code>ax</code> <code>matplotlib.axes.Axes or plotly Figure</code> <p>Axes object to draw the plot onto, otherwise uses the current Axes.</p> <code>None</code> <code>show_lines</code> <code>str</code> <p>Option for how to display mean values and partial dependence:</p> <ul> <li><code>\"always\"</code>: Always draw lines.</li> <li><code>\"numerical\"</code>: String and categorical features are drawn as points, numerical   ones as lines.</li> </ul> <code>'numerical'</code> <p>Returns:</p> Name Type Description <code>ax</code> <p>Either the matplotlib axes or the plotly figure. This is configurable by setting the <code>plot_backend</code> via <code>model_diagnostics.set_config</code> or <code>model_diagnostics.config_context</code>.</p> <p>Examples:</p> <p>If you wish to plot multiple features at once with subfigures, here is how to do it with matplotlib:</p> <pre><code>from math import ceil\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom model_diagnostics.calibration import plot_marginal\n\n# Replace by your own data and model.\nn_obs = 100\ny_obs = np.arange(n_obs)\nX = np.ones((n_obs, 2))\nX[:, 0] = np.sin(np.arange(n_obs))\nX[:, 1] = y_obs ** 2\n\ndef model_predict(X):\n    s = 0.5 * n_obs * np.sin(X)\n    return s.sum(axis=1) + np.sqrt(X[:, 1])\n\n# Now the plotting.\nfeature_list = [0, 1]\nn_rows, n_cols = ceil(len(feature_list) / 2), 2\nfig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, sharey=True)\nfor i, ax in enumerate(axs):\n    plot_marginal(\n        y_obs=y_obs,\n        y_pred=model_predict(X),\n        X=X,\n        feature_name=feature_list[i],\n        predict_function=model_predict,\n        ax=ax,\n    )\nfig.tight_layout()\n</code></pre> <p>For plotly, use the helper function <code>add_marginal_subplot</code>:</p> <pre><code>from math import ceil\nimport numpy as np\nfrom model_diagnostics import config_context\nfrom plotly.subplots import make_subplots\nfrom model_diagnostics.calibration import add_marginal_subplot, plot_marginal\n\n# Replace by your own data and model.\nn_obs = 100\ny_obs = np.arange(n_obs)\nX = np.ones((n_obs, 2))\nX[:, 0] = np.sin(np.arange(n_obs))\nX[:, 1] = y_obs ** 2\n\ndef model_predict(X):\n    s = 0.5 * n_obs * np.sin(X)\n    return s.sum(axis=1) + np.sqrt(X[:, 1])\n\n# Now the plotting.\nfeature_list = [0, 1]\nn_rows, n_cols = ceil(len(feature_list) / 2), 2\nfig = make_subplots(\n    rows=n_rows,\n    cols=n_cols,\n    vertical_spacing=0.3 / n_rows,  # equals default\n    # subplot_titles=feature_list,  # maybe\n    specs=[[{\"secondary_y\": True}] * n_cols] * n_rows,  # This is important!\n)\nfor row in range(n_rows):\n    for col in range(n_cols):\n        i = n_cols * row + col\n        with config_context(plot_backend=\"plotly\"):\n            subfig = plot_marginal(\n                y_obs=y_obs,\n                y_pred=model_predict(X),\n                X=X,\n                feature_name=feature_list[i],\n                predict_function=model_predict,\n            )\n        add_marginal_subplot(subfig, fig, row, col)\nfig.show()\n</code></pre> Source code in <code>src/model_diagnostics/calibration/plots.py</code> <pre><code>def plot_marginal(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    X: npt.ArrayLike,\n    feature_name: Union[str, int],\n    predict_function: Optional[Callable] = None,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    n_bins: int = 10,\n    bin_method: str = \"sturges\",\n    n_max: int = 1000,\n    rng: Optional[Union[np.random.Generator, int]] = None,\n    ax: Optional[mpl.axes.Axes] = None,\n    show_lines: str = \"numerical\",\n):\n    \"\"\"Plot marginal observed and predicted conditional on a feature.\n\n    This plot provides a means to inspect a model per feature.\n    The average of observed and predicted are plotted as well as a histogram of the\n    feature.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    X : array-like of shape (n_obs, n_features)\n        The dataframe or array of features to be passed to the model predict function.\n    feature_name : str or int\n        Column name (str) or index (int) of feature in `X`.\n    predict_function : callable or None\n        A callable to get prediction, i.e. `predict_function(X)`. Used to compute\n        partial dependence. If `None`, partial dependence is omitted.\n    weights : array-like of shape (n_obs) or None\n        Case weights. If given, the bias is calculated as weighted average of the\n        identification function with these weights.\n    n_bins : int\n        The number of bins, at least 2. For numerical features, `n_bins` only applies\n        when `bin_method` is set to `\"quantile\"` or `\"uniform\"`.\n        For string-like and categorical features, the most frequent values are taken.\n        Ties are dealt with by taking the first value in natural sorting order.\n        The remaining values are merged into `\"other n\"` with `n` indicating the unique\n        count.\n\n        I present, null values are always included in the output, accounting for one\n        bin. NaN values are treated as null values.\n    bin_method : str\n        The method for finding bin edges (boundaries). Options using `n_bins` are:\n\n        - `\"quantile\"`\n        - `\"uniform\"`\n\n        Options automatically selecting the number of bins for numerical features\n        thereby using uniform bins are same options as\n        [numpy.histogram_bin_edges](https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html):\n\n        - `\"auto\"`\n        Minimum bin width between the `\"sturges\"` and `\"fd\"` estimators. Provides good\n        all-around performance.\n        - `\"fd\"` (Freedman Diaconis Estimator)\n        Robust (resilient to outliers) estimator that takes into account data\n        variability and data size.\n        - `\"doane\"`\n        An improved version of Sturges' estimator that works better with non-normal\n        datasets.\n        - `\"scott\"`\n        Less robust estimator that takes into account data variability and data size.\n        - `\"stone\"`\n        Estimator based on leave-one-out cross-validation estimate of the integrated\n        squared error. Can be regarded as a generalization of Scott's rule.\n        - `\"rice\"`\n        Estimator does not take variability into account, only data size. Commonly\n        overestimates number of bins required.\n        - `\"sturges\"`\n        R's default method, only accounts for data size. Only optimal for gaussian data\n        and underestimates number of bins for large non-gaussian datasets.\n        - `\"sqrt\"`\n        Square root (of data size) estimator, used by Excel and other programs for its\n        speed and simplicity.\n\n    n_max : int or None\n        Used only for partial dependence computation. The number of rows to subsample\n        from X. This speeds up computation, in particular for slow predict functions.\n    rng : np.random.Generator, int or None\n        Used only for partial dependence computation. The random number generator used\n        for subsampling of `n_max` rows. The input is internally wrapped by\n        `np.random.default_rng(rng)`.\n    ax : matplotlib.axes.Axes or plotly Figure\n        Axes object to draw the plot onto, otherwise uses the current Axes.\n    show_lines : str\n        Option for how to display mean values and partial dependence:\n\n        - `\"always\"`: Always draw lines.\n        - `\"numerical\"`: String and categorical features are drawn as points, numerical\n          ones as lines.\n\n    Returns\n    -------\n    ax :\n        Either the matplotlib axes or the plotly figure. This is configurable by\n        setting the `plot_backend` via\n        [`model_diagnostics.set_config`][model_diagnostics.set_config] or\n        [`model_diagnostics.config_context`][model_diagnostics.config_context].\n\n    Examples\n    -----\n    If you wish to plot multiple features at once with subfigures, here is how to do it\n    with matplotlib:\n\n    ```py\n    from math import ceil\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from model_diagnostics.calibration import plot_marginal\n\n    # Replace by your own data and model.\n    n_obs = 100\n    y_obs = np.arange(n_obs)\n    X = np.ones((n_obs, 2))\n    X[:, 0] = np.sin(np.arange(n_obs))\n    X[:, 1] = y_obs ** 2\n\n    def model_predict(X):\n        s = 0.5 * n_obs * np.sin(X)\n        return s.sum(axis=1) + np.sqrt(X[:, 1])\n\n    # Now the plotting.\n    feature_list = [0, 1]\n    n_rows, n_cols = ceil(len(feature_list) / 2), 2\n    fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, sharey=True)\n    for i, ax in enumerate(axs):\n        plot_marginal(\n            y_obs=y_obs,\n            y_pred=model_predict(X),\n            X=X,\n            feature_name=feature_list[i],\n            predict_function=model_predict,\n            ax=ax,\n        )\n    fig.tight_layout()\n    ```\n\n    For plotly, use the helper function\n    [`add_marginal_subplot`][model_diagnostics.calibration.plots.add_marginal_subplot]:\n\n    ```py\n    from math import ceil\n    import numpy as np\n    from model_diagnostics import config_context\n    from plotly.subplots import make_subplots\n    from model_diagnostics.calibration import add_marginal_subplot, plot_marginal\n\n    # Replace by your own data and model.\n    n_obs = 100\n    y_obs = np.arange(n_obs)\n    X = np.ones((n_obs, 2))\n    X[:, 0] = np.sin(np.arange(n_obs))\n    X[:, 1] = y_obs ** 2\n\n    def model_predict(X):\n        s = 0.5 * n_obs * np.sin(X)\n        return s.sum(axis=1) + np.sqrt(X[:, 1])\n\n    # Now the plotting.\n    feature_list = [0, 1]\n    n_rows, n_cols = ceil(len(feature_list) / 2), 2\n    fig = make_subplots(\n        rows=n_rows,\n        cols=n_cols,\n        vertical_spacing=0.3 / n_rows,  # equals default\n        # subplot_titles=feature_list,  # maybe\n        specs=[[{\"secondary_y\": True}] * n_cols] * n_rows,  # This is important!\n    )\n    for row in range(n_rows):\n        for col in range(n_cols):\n            i = n_cols * row + col\n            with config_context(plot_backend=\"plotly\"):\n                subfig = plot_marginal(\n                    y_obs=y_obs,\n                    y_pred=model_predict(X),\n                    X=X,\n                    feature_name=feature_list[i],\n                    predict_function=model_predict,\n                )\n            add_marginal_subplot(subfig, fig, row, col)\n    fig.show()\n    ```\n\n    \"\"\"\n    if ax is None:\n        plot_backend = get_config()[\"plot_backend\"]\n        if plot_backend == \"matplotlib\":\n            ax = plt.gca()\n        else:\n            from plotly.subplots import make_subplots\n\n            # fig = ax = go.Figure()\n            fig = ax = make_subplots(specs=[[{\"secondary_y\": True}]])\n    elif isinstance(ax, mpl.axes.Axes):\n        plot_backend = \"matplotlib\"\n    elif is_plotly_figure(ax):\n        plot_backend = \"plotly\"\n        fig = ax\n        # Take care to mimick make_subplots for secondary y axis.\n        # The following code is by comparing\n        #   make_subplots(specs=[[{\"secondary_y\": True}]])\n        # vs\n        #   go.Figure()\n        if not hasattr(fig.layout, \"yaxis2\"):\n            fig.update_layout(\n                xaxis={\"anchor\": \"y\", \"domain\": [0.0, 0.94]},\n                yaxis={\"anchor\": \"x\", \"domain\": [0.0, 1.0]},\n                yaxis2={\"anchor\": \"x\", \"overlaying\": \"y\", \"side\": \"right\"},\n            )\n            SubplotRef = collections.namedtuple(  # noqa: PYI024\n                \"SubplotRef\", (\"subplot_type\", \"layout_keys\", \"trace_kwargs\")\n            )\n            fig._grid_ref = [  # noqa: SLF001\n                [\n                    (\n                        SubplotRef(\n                            subplot_type=\"xy\",\n                            layout_keys=(\"xaxis\", \"yaxis\"),\n                            trace_kwargs={\"xaxis\": \"x\", \"yaxis\": \"y\"},\n                        ),\n                        SubplotRef(\n                            subplot_type=\"xy\",\n                            layout_keys=(\"xaxis\", \"yaxis2\"),\n                            trace_kwargs={\"xaxis\": \"x\", \"yaxis\": \"y2\"},\n                        ),\n                    )\n                ]\n            ]\n            fig._grid_str = \"This is the format of your plot grid:\\n[ (1,1) x,y,y2 ]\\n\"  # noqa: SLF001\n    else:\n        msg = (\n            \"The ax argument must be None, a matplotlib Axes or a plotly Figure, \"\n            f\"got {type(ax)}.\"\n        )\n        raise ValueError(msg)\n\n    if show_lines not in (\"always\", \"numerical\"):\n        msg = (\n            f\"The argument show_lines mut be 'always' or 'numerical'; got {show_lines}.\"\n        )\n        raise ValueError(msg)\n\n    # estimator = getattr(predict_callable, \"__self__\", None)\n    n_pred = length_of_second_dimension(y_pred)\n    if n_pred &gt; 1:\n        msg = (\n            f\"Parameter y_pred has shape (n_obs, {n_pred}), but only \"\n            \"(n_obs) and (n_obs, 1) are allowd.\"\n        )\n        raise ValueError(msg)\n\n    df = compute_marginal(\n        y_obs=y_obs,\n        y_pred=y_pred,\n        X=X,\n        feature_name=feature_name,\n        predict_function=predict_function,\n        weights=weights,\n        n_bins=n_bins,\n        bin_method=bin_method,\n        n_max=n_max,\n        rng=rng,\n    )\n    feature_name = df.columns[0]\n\n    feature_has_nulls = df[feature_name].null_count() &gt; 0\n    n_bins_eff = df.shape[0] - feature_has_nulls\n    # If df contains the columns \"bin_edges\", it's a numerical feature.\n    is_categorical = \"bin_edges\" not in df.columns\n\n    n_x = df[feature_name].n_unique()\n\n    # marginal plot\n    if is_categorical and feature_has_nulls:\n        # We want the Null values at the end and therefore sort again.\n        df = df.sort(feature_name, descending=False, nulls_last=True)\n    df_no_nulls = df.filter(pl.col(feature_name).is_not_null())\n\n    # Numerical columns are sometimes better treated as categorical.\n    num_as_cat = False\n    if not is_categorical:\n        bin_edges = df_no_nulls.get_column(\"bin_edges\")\n        num_as_cat = (\n            # left bin edge = right bin edge\n            (bin_edges.arr.first() == bin_edges.arr.last())\n            # feature == left bin edge\n            | (bin_edges.arr.first() == df_no_nulls.get_column(feature_name))\n            # feature == right bin edge\n            | (bin_edges.arr.last() == df_no_nulls.get_column(feature_name))\n            # standard deviation of feature in bin == 0\n            | (bin_edges.arr.get(1) == 0)\n        ).all()\n\n    # First the histogram of weights on secondary y-axis.\n    # Other graph elements should appear on top of it. For plotly, we therefore need to\n    # plot the histogram on the primary y-axis and put primary to the right and\n    # secondary to the left. All other plotly graphs are put on the secondary yaxis.\n    #\n    # We x-shift a little for a better visual.\n    x = (\n        np.arange(n_x - feature_has_nulls)\n        if is_categorical\n        else df_no_nulls[feature_name]\n    )\n    if plot_backend == \"matplotlib\":\n        ax2 = ax.twinx()\n        if is_categorical or num_as_cat:\n            ax2.bar(\n                x=x,\n                height=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n                color=\"lightgrey\",\n            )\n        else:\n            # We can't use\n            #   ax2.hist(\n            #       x=df_no_nulls[feature_name],\n            #       weights=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n            #       bins=np.r_[bin_edges[0][0], bin_edges.arr.last()],  # n_bins_eff,\n            #       color=\"lightgrey\",\n            #       edgecolor=\"grey\",\n            #       rwidth=0.8 if n_bins_eff &lt;= 2 else None,\n            #   )\n            # because we might have empty bins.\n            ax2.bar(\n                x=0.5 * (bin_edges.arr.last() + bin_edges.arr.first()),\n                height=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n                width=(bin_edges.arr.last() - bin_edges.arr.first())\n                * (1 if n_bins_eff &gt; 2 else 0.8),\n                color=\"lightgrey\",\n                edgecolor=\"grey\",\n            )\n        # https://stackoverflow.com/questions/30505616/how-to-arrange-plots-of-secondary-axis-to-be-below-plots-of-primary-axis-in-matp\n        ax.set_zorder(ax2.get_zorder() + 1)\n        ax.set_frame_on(False)\n    else:\n        if is_categorical or num_as_cat:\n            # fig.add_histogram(\n            #     x=x, # df_no_nulls[feature_name],\n            #     y=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n            #     histfunc=\"sum\",\n            #     marker={\"color\": \"lightgrey\"},\n            #     secondary_y=False,\n            #     showlegend=False,\n            # )\n            fig.add_bar(\n                x=x,\n                y=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n                marker={\"color\": \"lightgrey\"},\n                secondary_y=False,\n                showlegend=False,\n            )\n        else:\n            fig.add_bar(\n                x=0.5 * (bin_edges.arr.last() + bin_edges.arr.first()),\n                y=df_no_nulls[\"weights\"] / df[\"weights\"].sum(),\n                width=bin_edges.arr.last() - bin_edges.arr.first(),\n                marker={\"color\": \"lightgrey\", \"line\": {\"width\": 1.0, \"color\": \"grey\"}},\n                secondary_y=False,\n                showlegend=False,\n            )\n        fig.update_layout(yaxis_side=\"right\", yaxis2_side=\"left\")\n        if n_bins_eff &lt;= 2:\n            fig.update_layout(bargap=0.2)\n\n    if feature_has_nulls:\n        df_null = df.filter(pl.col(feature_name).is_null())\n        # Null values are plotted as rightmost point at x_null.\n        if is_categorical:\n            x_null = np.array([n_x - 1])\n            # matplotlib default width = 0.8\n            width = 0.8 if plot_backend == \"matplotlib\" else None\n        else:\n            x_min = df[feature_name].min()\n            x_max = df[feature_name].max()\n            if n_x == 1:\n                # df[feature_name] is the null value.\n                x_null = np.array([0])\n            elif n_x == 2:\n                x_null = np.array([2 * x_max])\n            else:\n                x_null = np.array([x_max + (x_max - x_min) / n_x])\n            width = x_null - bin_edges.arr.last().max()\n            if width is not None and width &lt;= 0:\n                width = (x_max - x_min) / n_x / 2.0\n\n        # Null value histogram\n        if plot_backend == \"matplotlib\":\n            ax2.bar(\n                x=x_null,\n                height=df_null[\"weights\"] / df[\"weights\"].sum(),\n                width=width,\n                color=\"lightgrey\",\n            )\n        else:\n            fig.add_bar(\n                x=x_null,\n                y=df_null[\"weights\"] / df[\"weights\"].sum(),\n                width=width,\n                marker={\"color\": \"lightgrey\"},\n                secondary_y=False,\n                showlegend=False,\n            )\n\n    plot_items = [\"y_obs_mean\", \"y_pred_mean\"]\n    if predict_function is not None:\n        plot_items.append(\"partial_dependence\")\n    label_dict = {\n        \"y_obs_mean\": \"mean y_obs\",\n        \"y_pred_mean\": \"mean y_pred\",\n        \"partial_dependence\": \"partial dependence\",\n    }\n    for i, m in enumerate(plot_items):\n        label = label_dict[m]\n        if plot_backend == \"matplotlib\":\n            linestyle = \"dashed\" if m == \"partial_dependence\" else \"solid\"\n        else:\n            line = {\n                \"color\": get_plotly_color(i),\n                \"dash\": \"dash\" if m == \"partial_dependence\" else None,\n            }\n        if is_categorical:\n            # We x-shift a little for a better visual.\n            x = np.arange(n_x - feature_has_nulls)\n            if plot_backend == \"matplotlib\":\n                ax.plot(\n                    x,\n                    df_no_nulls[m],\n                    marker=\"o\",\n                    linestyle=\"None\" if show_lines == \"numerical\" else linestyle,\n                    label=label,\n                )\n            else:\n                fig.add_scatter(\n                    x=x,\n                    y=df_no_nulls[m],\n                    marker={\"color\": get_plotly_color(i)},\n                    mode=\"markers\" if show_lines == \"numerical\" else \"lines+markers\",\n                    line=None if show_lines == \"numerical\" else line,\n                    name=label,\n                    secondary_y=True,\n                )\n        elif plot_backend == \"matplotlib\":\n            ax.plot(\n                df[feature_name],\n                df[m],\n                linestyle=linestyle,\n                marker=\"o\",\n                label=label,\n            )\n        else:\n            fig.add_scatter(\n                x=df[feature_name],\n                y=df[m],\n                marker_symbol=\"circle\",\n                mode=\"lines+markers\",\n                line=line,\n                name=label,\n                secondary_y=True,\n            )\n\n        if feature_has_nulls:\n            # Null values are plotted as diamonds as rightmost point.\n            if plot_backend == \"matplotlib\":\n                color = ax.get_lines()[-1].get_color()  # previous line color\n                ax.plot(\n                    x_null,\n                    df_null[m],\n                    marker=\"D\",\n                    linestyle=\"None\",\n                    label=None,\n                    color=color,\n                )\n            else:\n                fig.add_scatter(\n                    x=x_null,\n                    y=df_null[m],\n                    marker={\"color\": get_plotly_color(i), \"symbol\": \"diamond\"},\n                    mode=\"markers\",\n                    secondary_y=True,\n                    showlegend=False,\n                )\n\n    if is_categorical:\n        if df[feature_name].null_count() &gt; 0:\n            # Without cast to pl.Uft8, the following error might occur:\n            # exceptions.ComputeError: cannot combine categorical under a global string\n            # cache with a non cached categorical\n            tick_labels = df[feature_name].cast(pl.Utf8).fill_null(\"Null\")\n        else:\n            tick_labels = df[feature_name]\n        x_label = feature_name\n        if plot_backend == \"matplotlib\":\n            ax.set_xticks(np.arange(n_x), labels=tick_labels)\n        else:\n            fig.update_layout(\n                xaxis={\n                    \"tickmode\": \"array\",\n                    \"tickvals\": np.arange(n_x),\n                    \"ticktext\": tick_labels,\n                }\n            )\n    elif feature_name is not None:\n        x_label = \"binned \" + str(feature_name)\n    else:\n        x_label = \"\"\n\n    model_name = array_name(y_pred, default=\"\")\n    # test for empty string \"\"\n    title = \"Marginal Plot\" if not model_name else \"Marginal Plot \" + model_name\n\n    if plot_backend == \"matplotlib\":\n        ax.set(xlabel=x_label, ylabel=\"y\", title=title)\n    else:\n        fig.update_layout(xaxis_title=x_label, yaxis2_title=\"y\", title=title)\n        fig[\"layout\"][\"yaxis\"][\"showgrid\"] = False\n\n    if plot_backend == \"matplotlib\":\n        if feature_has_nulls:\n            # Add legend entry for diamonds as Null values.\n            # Unfortunately, the Null value legend entry often appears first, but we\n            # want it at the end.\n            ax.scatter([], [], marker=\"D\", color=\"grey\", label=\"Null values\")\n            handles, labels = ax.get_legend_handles_labels()\n            if (labels[-1] != \"Null values\") and \"Null values\" in labels:\n                i = labels.index(\"Null values\")\n                # i can't be the last index\n                labels = labels[:i] + labels[i + 1 :] + [labels[i]]\n                handles = handles[:i] + handles[i + 1 :] + [handles[i]]\n            ax.legend(handles=handles, labels=labels)\n        else:\n            ax.legend()\n    elif feature_has_nulls:\n        fig.add_scatter(\n            x=[None],\n            y=[None],\n            mode=\"markers\",\n            name=\"Null values\",\n            marker={\"size\": 7, \"color\": \"grey\", \"symbol\": \"diamond\"},\n            secondary_y=True,\n        )\n\n    return ax\n</code></pre>"},{"location":"reference/model_diagnostics/calibration/plots/#model_diagnostics.calibration.plots.plot_reliability_diagram","title":"<code>plot_reliability_diagram(y_obs, y_pred, weights=None, *, functional='mean', level=0.5, n_bootstrap=None, confidence_level=0.9, diagram_type='reliability', ax=None)</code>","text":"<p>Plot a reliability diagram.</p> <p>A reliability diagram or calibration curve assesses auto-calibration. It plots the conditional expectation given the predictions <code>E(y_obs|y_pred)</code> (y-axis) vs the predictions <code>y_pred</code> (x-axis). The conditional expectation is estimated via isotonic regression (PAV algorithm) of <code>y_obs</code> on <code>y_pred</code>. See Notes for further details.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile or quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt;= level &lt;= 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> <code>n_bootstrap</code> <code>int or None</code> <p>If not <code>None</code>, then <code>scipy.stats.bootstrap</code> with <code>n_resamples=n_bootstrap</code> is used to calculate confidence intervals at level <code>confidence_level</code>.</p> <code>None</code> <code>confidence_level</code> <code>float</code> <p>Confidence level for bootstrap uncertainty regions.</p> <code>0.9</code> <code>diagram_type</code> <code>str</code> <ul> <li><code>\"reliability\"</code>: Plot a reliability diagram.</li> <li><code>\"bias\"</code>: Plot roughly a 45 degree rotated reliability diagram. The resulting   plot is similar to <code>plot_bias</code>, i.e. <code>y_pred - E(y_obs|y_pred)</code> vs <code>y_pred</code>.</li> </ul> <code>'reliability'</code> <code>ax</code> <code>matplotlib.axes.Axes or plotly Figure</code> <p>Axes object to draw the plot onto, otherwise uses the current Axes.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <p>Either the matplotlib axes or the plotly figure. This is configurable by setting the <code>plot_backend</code> via <code>model_diagnostics.set_config</code> or <code>model_diagnostics.config_context</code>.</p> Notes <p> The expectation conditional on the predictions is  The expectation conditional on the predictions is \\(E(Y|y_{pred})\\). This object is estimated by the pool-adjacent violator (PAV) algorithm, which has very desirable properties:</p> <pre><code>- It is non-parametric without any tuning parameter. Thus, the results are\n  easily reproducible.\n- Optimal selection of bins\n- Statistical consistent estimator\n</code></pre> <p>For details, refer to <code>[Dimitriadis2021]</code>.</p> References <code>[Dimitriadis2021]</code> <p>T. Dimitriadis, T. Gneiting, and A. I. Jordan. \"Stable reliability diagrams for probabilistic classifiers\". In: Proceedings of the National Academy of Sciences 118.8 (2021), e2016191118. doi:10.1073/pnas.2016191118.</p> Source code in <code>src/model_diagnostics/calibration/plots.py</code> <pre><code>def plot_reliability_diagram(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    functional: str = \"mean\",\n    level: float = 0.5,\n    n_bootstrap: Optional[str] = None,\n    confidence_level: float = 0.9,\n    diagram_type: str = \"reliability\",\n    ax: Optional[mpl.axes.Axes] = None,\n):\n    r\"\"\"Plot a reliability diagram.\n\n    A reliability diagram or calibration curve assesses auto-calibration. It plots the\n    conditional expectation given the predictions `E(y_obs|y_pred)` (y-axis) vs the\n    predictions `y_pred` (x-axis).\n    The conditional expectation is estimated via isotonic regression (PAV algorithm)\n    of `y_obs` on `y_pred`.\n    See [Notes](#notes) for further details.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n\n    level : float\n        The level of the expectile or quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt;= level &lt;= 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n    n_bootstrap : int or None\n        If not `None`, then `scipy.stats.bootstrap` with `n_resamples=n_bootstrap`\n        is used to calculate confidence intervals at level `confidence_level`.\n    confidence_level : float\n        Confidence level for bootstrap uncertainty regions.\n    diagram_type: str\n        - `\"reliability\"`: Plot a reliability diagram.\n        - `\"bias\"`: Plot roughly a 45 degree rotated reliability diagram. The resulting\n          plot is similar to `plot_bias`, i.e. `y_pred - E(y_obs|y_pred)` vs `y_pred`.\n    ax : matplotlib.axes.Axes or plotly Figure\n        Axes object to draw the plot onto, otherwise uses the current Axes.\n\n    Returns\n    -------\n    ax :\n        Either the matplotlib axes or the plotly figure. This is configurable by\n        setting the `plot_backend` via\n        [`model_diagnostics.set_config`][model_diagnostics.set_config] or\n        [`model_diagnostics.config_context`][model_diagnostics.config_context].\n\n    Notes\n    -----\n    [](){#notes}\n    The expectation conditional on the predictions is \\(E(Y|y_{pred})\\). This object is\n    estimated by the pool-adjacent violator (PAV) algorithm, which has very desirable\n    properties:\n\n        - It is non-parametric without any tuning parameter. Thus, the results are\n          easily reproducible.\n        - Optimal selection of bins\n        - Statistical consistent estimator\n\n    For details, refer to `[Dimitriadis2021]`.\n\n    References\n    ----------\n    `[Dimitriadis2021]`\n\n    :   T. Dimitriadis, T. Gneiting, and A. I. Jordan.\n        \"Stable reliability diagrams for probabilistic classifiers\".\n        In: Proceedings of the National Academy of Sciences 118.8 (2021), e2016191118.\n        [doi:10.1073/pnas.2016191118](https://doi.org/10.1073/pnas.2016191118).\n    \"\"\"\n    if ax is None:\n        plot_backend = get_config()[\"plot_backend\"]\n        if plot_backend == \"matplotlib\":\n            ax = plt.gca()\n        else:\n            import plotly.graph_objects as go\n\n            fig = ax = go.Figure()\n    elif isinstance(ax, mpl.axes.Axes):\n        plot_backend = \"matplotlib\"\n    elif is_plotly_figure(ax):\n        import plotly.graph_objects as go\n\n        plot_backend = \"plotly\"\n        fig = ax\n    else:\n        msg = (\n            \"The ax argument must be None, a matplotlib Axes or a plotly Figure, \"\n            f\"got {type(ax)}.\"\n        )\n        raise ValueError(msg)\n\n    if diagram_type not in (\"reliability\", \"bias\"):\n        msg = (\n            \"Parameter diagram_type must be either 'reliability', 'bias', \"\n            f\"got {diagram_type}.\"\n        )\n        raise ValueError(msg)\n\n    if (n_cols := length_of_second_dimension(y_obs)) &gt; 0:\n        if n_cols == 1:\n            y_obs = get_second_dimension(y_obs, 0)\n        else:\n            msg = (\n                f\"Array-like y_obs has more than 2 dimensions, y_obs.shape[1]={n_cols}\"\n            )\n            raise ValueError(msg)\n\n    y_min, y_max = get_array_min_max(y_pred)\n    if diagram_type == \"reliability\":\n        if plot_backend == \"matplotlib\":\n            ax.plot([y_min, y_max], [y_min, y_max], color=\"k\", linestyle=\"dotted\")\n        else:\n            fig.add_scatter(\n                x=[y_min, y_max],\n                y=[y_min, y_max],\n                mode=\"lines\",\n                line={\"color\": \"black\", \"dash\": \"dot\"},\n                showlegend=False,\n            )\n    elif plot_backend == \"matplotlib\":\n        # horizontal line at y=0\n\n        # The following plots in axis coordinates\n        # ax.axhline(y=0, xmin=0, xmax=1, color=\"k\", linestyle=\"dotted\")\n        # but we plot in data coordinates instead.\n        ax.hlines(0, xmin=y_min, xmax=y_max, color=\"k\", linestyle=\"dotted\")\n    else:\n        # horizontal line at y=0\n        fig.add_hline(y=0, line={\"color\": \"black\", \"dash\": \"dot\"}, showlegend=False)\n\n    if n_bootstrap is not None:\n        if functional == \"mean\":\n\n            def iso_statistic(y_obs, y_pred, weights=None, x_values=None):\n                iso_b = (\n                    IsotonicRegression_skl(out_of_bounds=\"clip\")\n                    .set_output(transform=\"default\")\n                    .fit(y_pred, y_obs, sample_weight=weights)\n                )\n                return iso_b.predict(x_values)\n\n        else:\n\n            def iso_statistic(y_obs, y_pred, weights=None, x_values=None):\n                iso_b = IsotonicRegression(functional=functional, level=level).fit(\n                    y_pred, y_obs, sample_weight=weights\n                )\n                return iso_b.predict(x_values)\n\n    n_pred = length_of_second_dimension(y_pred)\n    pred_names, _ = get_sorted_array_names(y_pred)\n\n    for i in range(len(pred_names)):\n        y_pred_i = y_pred if n_pred == 0 else get_second_dimension(y_pred, i)\n\n        if functional == \"mean\":\n            iso = (\n                IsotonicRegression_skl()\n                .set_output(transform=\"default\")\n                .fit(y_pred_i, y_obs, sample_weight=weights)\n            )\n        else:\n            iso = IsotonicRegression(functional=functional, level=level).fit(\n                y_pred_i, y_obs, sample_weight=weights\n            )\n\n        # confidence intervals\n        if n_bootstrap is not None:\n            data: tuple[npt.ArrayLike, ...]\n            data = (y_obs, y_pred_i) if weights is None else (y_obs, y_pred_i, weights)\n\n            boot = bootstrap(\n                data=data,\n                statistic=partial(iso_statistic, x_values=iso.X_thresholds_),\n                n_resamples=n_bootstrap,\n                paired=True,\n                confidence_level=confidence_level,\n                # Note: method=\"bca\" might result in\n                # DegenerateDataWarning: The BCa confidence interval cannot be\n                # calculated. This problem is known to occur when the distribution is\n                # degenerate or the statistic is np.min.\n                method=\"basic\",\n            )\n\n            # We make the interval conservatively monotone increasing by applying\n            # np.maximum.accumulate etc.\n            # Conservative here means smaller intervals such that it is more likely\n            # for the prediction to be out of the intervals leading to the conclusion\n            # of \"not auto-calibrated\".\n            lower = np.maximum.accumulate(boot.confidence_interval.low)\n            upper = np.minimum.accumulate(boot.confidence_interval.high[::-1])[::-1]\n            if diagram_type == \"bias\":\n                lower = iso.X_thresholds_ - lower\n                upper = iso.X_thresholds_ - upper\n            if plot_backend == \"matplotlib\":\n                ax.fill_between(iso.X_thresholds_, lower, upper, alpha=0.1)\n            else:\n                # plotly has not equivalent of fill_between and needs a bit more coding\n                color = get_plotly_color(i)\n                fig.add_scatter(\n                    x=np.r_[iso.X_thresholds_, iso.X_thresholds_[::-1]],\n                    y=np.r_[lower, upper[::-1]],\n                    fill=\"toself\",\n                    fillcolor=color,\n                    hoverinfo=\"skip\",\n                    line={\"color\": color},\n                    mode=\"lines\",\n                    opacity=0.1,\n                    showlegend=False,\n                )\n\n        # reliability curve\n        label = pred_names[i] if n_pred &gt;= 2 else None\n\n        y_plot = (\n            iso.y_thresholds_\n            if diagram_type == \"reliability\"\n            else iso.X_thresholds_ - iso.y_thresholds_\n        )\n        if plot_backend == \"matplotlib\":\n            ax.plot(iso.X_thresholds_, y_plot, label=label)\n        else:\n            fig.add_scatter(\n                x=iso.X_thresholds_,\n                y=y_plot,\n                mode=\"lines\",\n                line={\"color\": get_plotly_color(i)},\n                name=label,\n            )\n\n    xlabel_mapping = {\n        \"mean\": \"E(Y|X)\",\n        \"median\": \"median(Y|X)\",\n        \"expectile\": f\"{level}-expectile(Y|X)\",\n        \"quantile\": f\"{level}-quantile(Y|X)\",\n    }\n    ylabel_mapping = {\n        \"mean\": \"E(Y|prediction)\",\n        \"median\": \"median(Y|prediction)\",\n        \"expectile\": f\"{level}-expectile(Y|prediction)\",\n        \"quantile\": f\"{level}-quantile(Y|prediction)\",\n    }\n    xlabel = \"prediction for \" + xlabel_mapping[functional]\n    if diagram_type == \"reliability\":\n        ylabel = \"estimated \" + ylabel_mapping[functional]\n        title = \"Reliability Diagram\"\n    else:\n        ylabel = \"prediction - estimated \" + ylabel_mapping[functional]\n        title = \"Bias Reliability Diagram\"\n\n    if n_pred &lt;= 1 and len(pred_names[0]) &gt; 0:\n        title = title + \" \" + pred_names[0]\n\n    if plot_backend == \"matplotlib\":\n        if n_pred &gt;= 2:\n            ax.legend()\n        ax.set_title(title)\n        ax.set(xlabel=xlabel, ylabel=ylabel)\n    else:\n        if n_pred &lt;= 1:\n            fig.update_layout(showlegend=False)\n        fig.update_layout(xaxis_title=xlabel, yaxis_title=ylabel, title=title)\n\n    return ax\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/","title":"scoring","text":""},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring","title":"<code>scoring</code>","text":""},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.ElementaryScore","title":"<code>ElementaryScore</code>","text":"<p>Elementary scoring function.</p> <p>The smaller the better.</p> <p>The elementary scoring function is consistent for the specified <code>functional</code> for all values of <code>eta</code> and is the main ingredient for Murphy diagrams. See Notes for further details.</p> <p>Parameters:</p> Name Type Description Default <code>eta</code> <code>float</code> <p>Free parameter.</p> required <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile of quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> Notes <p> The elementary scoring or loss function is given by The elementary scoring or loss function is given by</p> \\[ S_\\eta(y, z) = (\\mathbf{1}\\{\\eta \\le z\\} - \\mathbf{1}\\{\\eta \\le y\\}) V(y, \\eta) \\] <p>with identification functions \\(V\\) for the given <code>functional</code> \\(T\\) . If allows for the mixture or Choquet representation</p> \\[ S(y, z) = \\int S_\\eta(y, z) \\,dH(\\eta) \\] <p>for some locally finite measure \\(H\\). It follows that the scoring function \\(S\\) is consistent for \\(T\\).</p> References <code>[Jordan2022]</code> <p>A.I. Jordan, A. M\u00fchlemann, J.F. Ziegel. \"Characterizing the optimal solutions to the isotonic regression problem for identifiable functionals\". (2022) doi:10.1007/s10463-021-00808-0</p> <code>[GneitingResin2022]</code> <p>T. Gneiting, J. Resin. \"Regression Diagnostics meets Forecast Evaluation: Conditional Calibration, Reliability Diagrams, and Coefficient of Determination\". arxiv:2108.03210</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; el_score = ElementaryScore(eta=2, functional=\"mean\")\n&gt;&gt;&gt; el_score(y_obs=[1, 2, 2, 1], y_pred=[4, 1, 2, 3])\n0.5\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class ElementaryScore(_BaseScoringFunction):\n    r\"\"\"Elementary scoring function.\n\n    The smaller the better.\n\n    The elementary scoring function is consistent for the specified `functional` for\n    all values of `eta` and is the main ingredient for Murphy diagrams.\n    See [Notes](#notes) for further details.\n\n    Parameters\n    ----------\n    eta : float\n        Free parameter.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n    level : float\n        The level of the expectile of quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n\n    Notes\n    -----\n    [](){#notes}\n    The elementary scoring or loss function is given by\n\n    \\[\n    S_\\eta(y, z) = (\\mathbf{1}\\{\\eta \\le z\\} - \\mathbf{1}\\{\\eta \\le y\\})\n    V(y, \\eta)\n    \\]\n\n    with [identification functions]\n    [model_diagnostics.calibration.identification_function]\n    \\(V\\) for the given `functional` \\(T\\) . If allows for the mixture or Choquet\n    representation\n\n    \\[\n    S(y, z) = \\int S_\\eta(y, z) \\,dH(\\eta)\n    \\]\n\n    for some locally finite measure \\(H\\). It follows that the scoring function \\(S\\)\n    is consistent for \\(T\\).\n\n    References\n    ----------\n    `[Jordan2022]`\n\n    :   A.I. Jordan, A. M\u00fchlemann, J.F. Ziegel.\n        \"Characterizing the optimal solutions to the isotonic regression problem for\n        identifiable functionals\". (2022)\n        [doi:10.1007/s10463-021-00808-0](https://doi.org/10.1007/s10463-021-00808-0)\n\n    `[GneitingResin2022]`\n\n    :   T. Gneiting, J. Resin.\n        \"Regression Diagnostics meets Forecast Evaluation: Conditional Calibration,\n        Reliability Diagrams, and Coefficient of Determination\".\n        [arxiv:2108.03210](https://arxiv.org/abs/2108.03210)\n\n    Examples\n    --------\n    &gt;&gt;&gt; el_score = ElementaryScore(eta=2, functional=\"mean\")\n    &gt;&gt;&gt; el_score(y_obs=[1, 2, 2, 1], y_pred=[4, 1, 2, 3])  # doctest: +SKIP\n    0.5\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(\n        self, eta: float, functional: str = \"mean\", level: float = 0.5\n    ) -&gt; None:\n        self.eta = eta\n        self._functional = functional\n        if level &lt;= 0 or level &gt;= 1:\n            msg = f\"Argument level must fulfil 0 &lt; level &lt; 1, got {level}.\"\n            raise ValueError(msg)\n        self.level = level\n\n    @property\n    def functional(self):\n        return self._functional\n\n    def score_per_obs(\n        self,\n        y_obs: npt.ArrayLike,\n        y_pred: npt.ArrayLike,\n    ) -&gt; np.ndarray:\n        \"\"\"Score per observation.\n\n        Parameters\n        ----------\n        y_obs : array-like of shape (n_obs)\n            Observed values of the response variable.\n        y_pred : array-like of shape (n_obs)\n            Predicted values of the `functional` of interest, e.g. the conditional\n            expectation of the response, `E(Y|X)`.\n\n        Returns\n        -------\n        score_per_obs : ndarray\n            Values of the scoring function for each observation.\n        \"\"\"\n        y: np.ndarray\n        z: np.ndarray\n        y, z = validate_2_arrays(y_obs, y_pred)\n\n        eta = self.eta\n        eta_term = np.less_equal(eta, z).astype(float) - np.less_equal(eta, y)\n        return eta_term * identification_function(\n            y_obs=y_obs,\n            y_pred=np.full(y.shape, fill_value=float(eta)),\n            functional=self.functional,\n            level=self.level,\n        )\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.ElementaryScore.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.ElementaryScore.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    eta = self.eta\n    eta_term = np.less_equal(eta, z).astype(float) - np.less_equal(eta, y)\n    return eta_term * identification_function(\n        y_obs=y_obs,\n        y_pred=np.full(y.shape, fill_value=float(eta)),\n        functional=self.functional,\n        level=self.level,\n    )\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.GammaDeviance","title":"<code>GammaDeviance</code>","text":"<p>Gamma deviance.</p> <p>The smaller the better, minimum is zero.</p> <p>The Gamma deviance is strictly consistent for the mean. It has a degree of homogeneity of 0 and is therefore insensitive to a change of units or multiplication of <code>y_obs</code> and <code>y_pred</code> by the same positive constant.</p> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"mean\"</p> Notes <p>\\(S(y, z) = 2(\\frac{y}{z} -\\log\\frac{y}{z} - 1)\\)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gd = GammaDeviance()\n&gt;&gt;&gt; gd(y_obs=[3, 2, 1, 1], y_pred=[2, 1, 1 , 2])\n0.2972674459459178\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class GammaDeviance(HomogeneousExpectileScore):\n    r\"\"\"Gamma deviance.\n\n    The smaller the better, minimum is zero.\n\n    The Gamma deviance is strictly consistent for the mean.\n    It has a degree of homogeneity of 0 and is therefore insensitive to a change of\n    units or multiplication of `y_obs` and `y_pred` by the same positive constant.\n\n    Attributes\n    ----------\n    functional: str\n        \"mean\"\n\n    Notes\n    -----\n    \\(S(y, z) = 2(\\frac{y}{z} -\\log\\frac{y}{z} - 1)\\)\n\n    Examples\n    --------\n    &gt;&gt;&gt; gd = GammaDeviance()\n    &gt;&gt;&gt; gd(y_obs=[3, 2, 1, 1], y_pred=[2, 1, 1 , 2])  # doctest: +SKIP\n    0.2972674459459178\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self) -&gt; None:\n        super().__init__(degree=0, level=0.5)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.GammaDeviance.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.GammaDeviance.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 2:\n        # Fast path\n        score = np.square(z - y)\n    elif self.degree &gt; 1:\n        z_abs = np.abs(z)\n        score = 2 * (\n            (np.power(np.abs(y), self.degree) - np.power(z_abs, self.degree))\n            / (self.degree * (self.degree - 1))\n            - np.sign(z)\n            / (self.degree - 1)\n            * np.power(z_abs, self.degree - 1)\n            * (y - z)\n        )\n    elif self.degree == 1:\n        # Domain: y &gt;= 0 and z &gt; 0\n        if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is y_obs &gt;= 0 and \"\n                \"y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = 2 * (special.xlogy(y, y / z) - y + z)\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        y_z = y / z\n        score = 2 * (y_z - np.log(y_z) - 1)\n    else:  # self.degree &lt; 1\n        # Domain: y &gt;= 0 and z &gt; 0 for 0 &lt; self.degree &lt; 1\n        # Domain: y &gt; 0  and z &gt; 0 else\n        if self.degree &gt; 0:\n            if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt;= 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n        elif not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        # Note: We add 0.0 to be sure we have floating points. Integers are not\n        # allowerd to be raised to a negative power.\n        score = 2 * (\n            (np.power(y + 0.0, self.degree) - np.power(z + 0.0, self.degree))\n            / (self.degree * (self.degree - 1))\n            - 1 / (self.degree - 1) * np.power(z + 0.0, self.degree - 1) * (y - z)\n        )\n\n    if self.level == 0.5:\n        return score\n    else:\n        return 2 * np.abs(np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.HomogeneousExpectileScore","title":"<code>HomogeneousExpectileScore</code>","text":"<p>Homogeneous scoring function of degree h for expectiles.</p> <p>The smaller the better, minimum is zero.</p> <p>Up to a multiplicative constant, these are the only scoring functions that are strictly consistent for expectiles at level alpha and homogeneous functions. The possible additive constant is chosen such that the minimal function value equals zero.</p> <p>Note that the \u00bd-expectile (level alpha=0.5) equals the mean.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>float</code> <p>Degree of homogeneity.</p> <code>2</code> <code>level</code> <code>float</code> <p>The level of the expectile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> gives the mean.</p> <code>0.5</code> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"mean\" if <code>level=0.5</code>, else \"expectile\"</p> Notes <p>The homogeneous score of degree \\(h\\) is given by</p> \\[ S_\\alpha^h(y, z) = 2 |\\mathbf{1}\\{z \\ge y\\} - \\alpha| \\frac{2}{h(h-1)} \\left(|y|^h - |z|^h - h \\operatorname{sign}(z) |z|^{h-1} (y-z)\\right) \\] <p>Note that the first term, \\(2 |\\mathbf{1}\\{z \\ge y\\} - \\alpha|\\) equals 1 for \\(\\alpha=0.5\\). There are important domain restrictions and limits:</p> <ul> <li> <p>\\(h&gt;1\\): All real numbers \\(y\\) and \\(z\\) are allowed.</p> <p>Special case \\(h=2, \\alpha=\\frac{1}{2}\\) equals the squared error, aka Normal deviance \\(S(y, z) = (y - z)^2\\).</p> </li> <li> <p>\\(0 &lt; h \\leq 1\\): Only \\(y \\geq 0\\), \\(z&gt;0\\) are allowed.</p> <p>Special case \\(h=1, \\alpha=\\frac{1}{2}\\) (by taking the limit) equals the Poisson deviance \\(S(y, z) = 2(y\\log\\frac{y}{z} - y + z)\\).</p> </li> <li> <p>\\(h \\leq 0\\): Only \\(y&gt;0\\), \\(z&gt;0\\) are allowed.</p> <p>Special case \\(h=0, \\alpha=\\frac{1}{2}\\) (by taking the limit) equals the Gamma deviance \\(S(y, z) = 2(\\frac{y}{z} -\\log\\frac{y}{z} - 1)\\).</p> </li> </ul> <p>For the common domains, \\(S_{\\frac{1}{2}}^h\\) equals the Tweedie deviance with the following relation between the degree of homogeneity \\(h\\) and the Tweedie power \\(p\\): \\(h = 2-p\\).</p> References <code>[Gneiting2011]</code> <p>T. Gneiting. \"Making and Evaluating Point Forecasts\". (2011) doi:10.1198/jasa.2011.r10138 arxiv:0912.0902</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hes = HomogeneousExpectileScore(degree=2, level=0.1)\n&gt;&gt;&gt; hes(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n0.95\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class HomogeneousExpectileScore(_BaseScoringFunction):\n    r\"\"\"Homogeneous scoring function of degree h for expectiles.\n\n    The smaller the better, minimum is zero.\n\n    Up to a multiplicative constant, these are the only scoring functions that are\n    strictly consistent for expectiles at level alpha and homogeneous functions.\n    The possible additive constant is chosen such that the minimal function value\n    equals zero.\n\n    Note that the 1/2-expectile (level alpha=0.5) equals the mean.\n\n    Parameters\n    ----------\n    degree : float\n        Degree of homogeneity.\n    level : float\n        The level of the expectile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` gives the mean.\n\n    Attributes\n    ----------\n    functional: str\n        \"mean\" if `level=0.5`, else \"expectile\"\n\n    Notes\n    -----\n    The homogeneous score of degree \\(h\\) is given by\n\n    \\[\n    S_\\alpha^h(y, z) = 2 |\\mathbf{1}\\{z \\ge y\\} - \\alpha| \\frac{2}{h(h-1)}\n    \\left(|y|^h - |z|^h - h \\operatorname{sign}(z) |z|^{h-1} (y-z)\\right)\n    \\]\n\n    Note that the first term, \\(2 |\\mathbf{1}\\{z \\ge y\\} - \\alpha|\\) equals 1 for\n    \\(\\alpha=0.5\\).\n    There are important domain restrictions and limits:\n\n    - \\(h&gt;1\\): All real numbers \\(y\\) and \\(z\\) are allowed.\n\n        Special case \\(h=2, \\alpha=\\frac{1}{2}\\) equals the squared error, aka Normal\n        deviance \\(S(y, z) = (y - z)^2\\).\n\n    - \\(0 &lt; h \\leq 1\\): Only \\(y \\geq 0\\), \\(z&gt;0\\) are allowed.\n\n        Special case \\(h=1, \\alpha=\\frac{1}{2}\\) (by taking the limit) equals the\n        Poisson deviance \\(S(y, z) = 2(y\\log\\frac{y}{z} - y + z)\\).\n\n    - \\(h \\leq 0\\): Only \\(y&gt;0\\), \\(z&gt;0\\) are allowed.\n\n        Special case \\(h=0, \\alpha=\\frac{1}{2}\\) (by taking the limit) equals the Gamma\n        deviance \\(S(y, z) = 2(\\frac{y}{z} -\\log\\frac{y}{z} - 1)\\).\n\n    For the common domains, \\(S_{\\frac{1}{2}}^h\\) equals the\n    [Tweedie deviance](https://en.wikipedia.org/wiki/Tweedie_distribution) with the\n    following relation between the degree of homogeneity \\(h\\) and the Tweedie\n    power \\(p\\): \\(h = 2-p\\).\n\n    References\n    ----------\n    `[Gneiting2011]`\n\n    :   T. Gneiting.\n        \"Making and Evaluating Point Forecasts\". (2011)\n        [doi:10.1198/jasa.2011.r10138](https://doi.org/10.1198/jasa.2011.r10138)\n        [arxiv:0912.0902](https://arxiv.org/abs/0912.0902)\n\n    Examples\n    --------\n    &gt;&gt;&gt; hes = HomogeneousExpectileScore(degree=2, level=0.1)\n    &gt;&gt;&gt; hes(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])  # doctest: +SKIP\n    0.95\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self, degree: float = 2, level: float = 0.5) -&gt; None:\n        self.degree = degree\n        if level &lt;= 0 or level &gt;= 1:\n            msg = f\"Argument level must fulfil 0 &lt; level &lt; 1, got {level}.\"\n            raise ValueError(msg)\n        self.level = level\n\n    @property\n    def functional(self):\n        if self.level == 0.5:\n            return \"mean\"\n        else:\n            return \"expectile\"\n\n    def score_per_obs(\n        self,\n        y_obs: npt.ArrayLike,\n        y_pred: npt.ArrayLike,\n    ) -&gt; np.ndarray:\n        \"\"\"Score per observation.\n\n        Parameters\n        ----------\n        y_obs : array-like of shape (n_obs)\n            Observed values of the response variable.\n        y_pred : array-like of shape (n_obs)\n            Predicted values of the `functional` of interest, e.g. the conditional\n            expectation of the response, `E(Y|X)`.\n\n        Returns\n        -------\n        score_per_obs : ndarray\n            Values of the scoring function for each observation.\n        \"\"\"\n        y: np.ndarray\n        z: np.ndarray\n        y, z = validate_2_arrays(y_obs, y_pred)\n\n        if self.degree == 2:\n            # Fast path\n            score = np.square(z - y)\n        elif self.degree &gt; 1:\n            z_abs = np.abs(z)\n            score = 2 * (\n                (np.power(np.abs(y), self.degree) - np.power(z_abs, self.degree))\n                / (self.degree * (self.degree - 1))\n                - np.sign(z)\n                / (self.degree - 1)\n                * np.power(z_abs, self.degree - 1)\n                * (y - z)\n            )\n        elif self.degree == 1:\n            # Domain: y &gt;= 0 and z &gt; 0\n            if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is y_obs &gt;= 0 and \"\n                    \"y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n            score = 2 * (special.xlogy(y, y / z) - y + z)\n        elif self.degree == 0:\n            # Domain: y &gt; 0 and z &gt; 0.\n            if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt; 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n            y_z = y / z\n            score = 2 * (y_z - np.log(y_z) - 1)\n        else:  # self.degree &lt; 1\n            # Domain: y &gt;= 0 and z &gt; 0 for 0 &lt; self.degree &lt; 1\n            # Domain: y &gt; 0  and z &gt; 0 else\n            if self.degree &gt; 0:\n                if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                    msg = (\n                        f\"Valid domain for degree={self.degree} is \"\n                        \"y_obs &gt;= 0 and y_pred &gt; 0.\"\n                    )\n                    raise ValueError(msg)\n            elif not np.all((y &gt; 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt; 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n            # Note: We add 0.0 to be sure we have floating points. Integers are not\n            # allowerd to be raised to a negative power.\n            score = 2 * (\n                (np.power(y + 0.0, self.degree) - np.power(z + 0.0, self.degree))\n                / (self.degree * (self.degree - 1))\n                - 1 / (self.degree - 1) * np.power(z + 0.0, self.degree - 1) * (y - z)\n            )\n\n        if self.level == 0.5:\n            return score\n        else:\n            return 2 * np.abs(np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.HomogeneousExpectileScore.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.HomogeneousExpectileScore.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 2:\n        # Fast path\n        score = np.square(z - y)\n    elif self.degree &gt; 1:\n        z_abs = np.abs(z)\n        score = 2 * (\n            (np.power(np.abs(y), self.degree) - np.power(z_abs, self.degree))\n            / (self.degree * (self.degree - 1))\n            - np.sign(z)\n            / (self.degree - 1)\n            * np.power(z_abs, self.degree - 1)\n            * (y - z)\n        )\n    elif self.degree == 1:\n        # Domain: y &gt;= 0 and z &gt; 0\n        if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is y_obs &gt;= 0 and \"\n                \"y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = 2 * (special.xlogy(y, y / z) - y + z)\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        y_z = y / z\n        score = 2 * (y_z - np.log(y_z) - 1)\n    else:  # self.degree &lt; 1\n        # Domain: y &gt;= 0 and z &gt; 0 for 0 &lt; self.degree &lt; 1\n        # Domain: y &gt; 0  and z &gt; 0 else\n        if self.degree &gt; 0:\n            if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt;= 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n        elif not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        # Note: We add 0.0 to be sure we have floating points. Integers are not\n        # allowerd to be raised to a negative power.\n        score = 2 * (\n            (np.power(y + 0.0, self.degree) - np.power(z + 0.0, self.degree))\n            / (self.degree * (self.degree - 1))\n            - 1 / (self.degree - 1) * np.power(z + 0.0, self.degree - 1) * (y - z)\n        )\n\n    if self.level == 0.5:\n        return score\n    else:\n        return 2 * np.abs(np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.HomogeneousQuantileScore","title":"<code>HomogeneousQuantileScore</code>","text":"<p>Homogeneous scoring function of degree h for quantiles.</p> <p>The smaller the better, minimum is zero.</p> <p>Up to a multiplicative constant, these are the only scoring funtions that are strictly consistent for quantiles at level alpha and homogeneous functions. The possible additive constant is chosen such that the minimal function value equals zero.</p> <p>Note that the \u00bd-quantile (level alpha=0.5) equals the median.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>float</code> <p>Degree of homogeneity.</p> <code>2</code> <code>level</code> <code>float</code> <p>The level of the quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> gives the median.</p> <code>0.5</code> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"quantile\"</p> Notes <p>The homogeneous score of degree \\(h\\) is given by</p> \\[ S_\\alpha^h(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) \\frac{z^h - y^h}{h} \\] <p>There are important domain restrictions and limits:</p> <ul> <li> <p>\\(h\\) positive odd integer: All real numbers \\(y\\) and \\(z\\) are allowed.</p> <ul> <li>Special case \\(h=1\\) equals the pinball loss,   \\(S(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) (z - y)\\).</li> <li>Special case \\(h=1, \\alpha=\\frac{1}{2}\\) equals half the absolute error   \\(S(y, z) = \\frac{1}{2}|z - y|\\).</li> </ul> </li> <li> <p>\\(h\\) real valued: Only \\(y&gt;0\\), \\(z&gt;0\\) are allowed.</p> <p>Special case \\(h=0\\) (by taking the limit) equals \\(S(y, z) = |\\mathbf{1}\\{z \\ge y\\} - \\alpha| \\log\\frac{z}{y}\\).</p> </li> </ul> References <code>[Gneiting2011]</code> <p>T. Gneiting. \"Making and Evaluating Point Forecasts\". (2011) doi:10.1198/jasa.2011.r10138 arxiv:0912.0902</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hqs = HomogeneousQuantileScore(degree=3, level=0.1)\n&gt;&gt;&gt; hqs(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n0.6083333333333334\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class HomogeneousQuantileScore(_BaseScoringFunction):\n    r\"\"\"Homogeneous scoring function of degree h for quantiles.\n\n    The smaller the better, minimum is zero.\n\n    Up to a multiplicative constant, these are the only scoring funtions that are\n    strictly consistent for quantiles at level alpha and homogeneous functions.\n    The possible additive constant is chosen such that the minimal function value\n    equals zero.\n\n    Note that the 1/2-quantile (level alpha=0.5) equals the median.\n\n    Parameters\n    ----------\n    degree : float\n        Degree of homogeneity.\n    level : float\n        The level of the quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` gives the median.\n\n    Attributes\n    ----------\n    functional: str\n        \"quantile\"\n\n    Notes\n    -----\n    The homogeneous score of degree \\(h\\) is given by\n\n    \\[\n    S_\\alpha^h(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) \\frac{z^h - y^h}{h}\n    \\]\n\n    There are important domain restrictions and limits:\n\n    - \\(h\\) positive odd integer: All real numbers \\(y\\) and \\(z\\) are allowed.\n\n        - Special case \\(h=1\\) equals the pinball loss,\n          \\(S(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) (z - y)\\).\n        - Special case \\(h=1, \\alpha=\\frac{1}{2}\\) equals half the absolute error\n          \\(S(y, z) = \\frac{1}{2}|z - y|\\).\n\n    - \\(h\\) real valued: Only \\(y&gt;0\\), \\(z&gt;0\\) are allowed.\n\n        Special case \\(h=0\\) (by taking the limit) equals\n        \\(S(y, z) = |\\mathbf{1}\\{z \\ge y\\} - \\alpha| \\log\\frac{z}{y}\\).\n\n    References\n    ----------\n    `[Gneiting2011]`\n\n    :   T. Gneiting.\n        \"Making and Evaluating Point Forecasts\". (2011)\n        [doi:10.1198/jasa.2011.r10138](https://doi.org/10.1198/jasa.2011.r10138)\n        [arxiv:0912.0902](https://arxiv.org/abs/0912.0902)\n\n    Examples\n    --------\n    &gt;&gt;&gt; hqs = HomogeneousQuantileScore(degree=3, level=0.1)\n    &gt;&gt;&gt; hqs(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])  # doctest: +SKIP\n    0.6083333333333334\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self, degree: float = 2, level: float = 0.5) -&gt; None:\n        self.degree = degree\n        if level &lt;= 0 or level &gt;= 1:\n            msg = f\"Argument level must fulfil 0 &lt; level &lt; 1, got {level}.\"\n            raise ValueError(msg)\n        self.level = level\n\n    @property\n    def functional(self):\n        return \"quantile\"\n\n    def score_per_obs(\n        self,\n        y_obs: npt.ArrayLike,\n        y_pred: npt.ArrayLike,\n    ) -&gt; np.ndarray:\n        \"\"\"Score per observation.\n\n        Parameters\n        ----------\n        y_obs : array-like of shape (n_obs)\n            Observed values of the response variable.\n        y_pred : array-like of shape (n_obs)\n            Predicted values of the `functional` of interest, e.g. the conditional\n            expectation of the response, `E(Y|X)`.\n\n        Returns\n        -------\n        score_per_obs : ndarray\n            Values of the scoring function for each observation.\n        \"\"\"\n        y: np.ndarray\n        z: np.ndarray\n        y, z = validate_2_arrays(y_obs, y_pred)\n\n        if self.degree == 1:\n            # Fast path\n            score = z - y\n        elif self.degree &gt; 1 and self.degree % 2 == 1:\n            # Odd positive degree\n            score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n        elif self.degree == 0:\n            # Domain: y &gt; 0 and z &gt; 0.\n            if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt; 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n            score = np.log(z / y)\n        else:\n            # Domain: y &gt; 0 and z &gt; 0.\n            if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt; 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n            score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n\n        if self.level == 0.5:\n            return 0.5 * np.abs(score)\n        else:\n            return (np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.HomogeneousQuantileScore.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.HomogeneousQuantileScore.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 1:\n        # Fast path\n        score = z - y\n    elif self.degree &gt; 1 and self.degree % 2 == 1:\n        # Odd positive degree\n        score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = np.log(z / y)\n    else:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n\n    if self.level == 0.5:\n        return 0.5 * np.abs(score)\n    else:\n        return (np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.LogLoss","title":"<code>LogLoss</code>","text":"<p>Log loss.</p> <p>The smaller the better, minimum is zero.</p> <p>The log loss is a strictly consistent scoring function for the mean for observations and predictions in the range 0 to 1. It is also referred to as (half the) Bernoulli deviance, (half the) Binomial log-likelihood, logistic loss and binary cross-entropy. Its minimal function value is zero.</p> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"mean\"</p> Notes <p>The log loss for \\(y,z \\in [0,1]\\) is given by</p> \\[ S(y, z) = - y \\log\\frac{z}{y} - (1 - y) \\log\\frac{1-z}{1-y} \\] <p>If one restricts to \\(y\\in \\{0, 1\\}\\), this simplifies to</p> \\[ S(y, z) = - y \\log(z) - (1 - y) \\log(1-z) \\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; ll = LogLoss()\n&gt;&gt;&gt; ll(y_obs=[0, 0.5, 1, 1], y_pred=[0.1, 0.2, 0.8 , 0.9], weights=[1, 2, 1, 1])\n0.17603033705165635\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class LogLoss(_BaseScoringFunction):\n    r\"\"\"Log loss.\n\n    The smaller the better, minimum is zero.\n\n    The log loss is a strictly consistent scoring function for the mean for\n    observations and predictions in the range 0 to 1.\n    It is also referred to as (half the) Bernoulli deviance,\n    (half the) Binomial log-likelihood, logistic loss and binary cross-entropy.\n    Its minimal function value is zero.\n\n    Attributes\n    ----------\n    functional: str\n        \"mean\"\n\n    Notes\n    -----\n    The log loss for \\(y,z \\in [0,1]\\) is given by\n\n    \\[\n    S(y, z) = - y \\log\\frac{z}{y} - (1 - y) \\log\\frac{1-z}{1-y}\n    \\]\n\n    If one restricts to \\(y\\in \\{0, 1\\}\\), this simplifies to\n\n    \\[\n    S(y, z) = - y \\log(z) - (1 - y) \\log(1-z)\n    \\]\n\n    Examples\n    --------\n    &gt;&gt;&gt; ll = LogLoss()\n    &gt;&gt;&gt; ll(y_obs=[0, 0.5, 1, 1], y_pred=[0.1, 0.2, 0.8 , 0.9], weights=[1, 2, 1, 1])  # doctest: +SKIP\n    0.17603033705165635\n    \"\"\"  # noqa: E501\n\n    # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    @property\n    def functional(self):\n        return \"mean\"\n\n    def score_per_obs(\n        self,\n        y_obs: npt.ArrayLike,\n        y_pred: npt.ArrayLike,\n    ) -&gt; np.ndarray:\n        \"\"\"Score per observation.\n\n        Parameters\n        ----------\n        y_obs : array-like of shape (n_obs)\n            Observed values of the response variable.\n        y_pred : array-like of shape (n_obs)\n            Predicted values of the `functional` of interest, e.g. the conditional\n            expectation of the response, `E(Y|X)`.\n\n        Returns\n        -------\n        score_per_obs : ndarray\n            Values of the scoring function for each observation.\n        \"\"\"\n        y: np.ndarray\n        z: np.ndarray\n        y, z = validate_2_arrays(y_obs, y_pred)\n\n        score = -special.xlogy(y, z) - special.xlogy(1 - y, 1 - z)\n        if np.any((y &gt; 0) &amp; (y &lt; 1)):\n            score += special.xlogy(y, y) + special.xlogy(1 - y, 1 - y)\n        return score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.LogLoss.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.LogLoss.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    score = -special.xlogy(y, z) - special.xlogy(1 - y, 1 - z)\n    if np.any((y &gt; 0) &amp; (y &lt; 1)):\n        score += special.xlogy(y, y) + special.xlogy(1 - y, 1 - y)\n    return score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.PinballLoss","title":"<code>PinballLoss</code>","text":"<p>Pinball loss.</p> <p>The smaller the better, minimum is zero.</p> <p>The pinball loss is strictly consistent for quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>float</code> <p>The level of the quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> gives the median.</p> <code>0.5</code> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"quantile\"</p> Notes <p>The pinball loss has degree of homogeneity 1 and is given by</p> \\[ S_\\alpha(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) (z - y) \\] <p>The authors do not know where and when the term pinball loss was coined. It is most famously used in quantile regression.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pl = PinballLoss(level=0.9)\n&gt;&gt;&gt; pl(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n0.275\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class PinballLoss(HomogeneousQuantileScore):\n    r\"\"\"Pinball loss.\n\n    The smaller the better, minimum is zero.\n\n    The pinball loss is strictly consistent for quantiles.\n\n    Parameters\n    ----------\n    level : float\n        The level of the quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` gives the median.\n\n    Attributes\n    ----------\n    functional: str\n        \"quantile\"\n\n    Notes\n    -----\n    The pinball loss has degree of homogeneity 1 and is given by\n\n    \\[\n    S_\\alpha(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) (z - y)\n    \\]\n\n    The authors do not know where and when the term *pinball loss* was coined. It is\n    most famously used in quantile regression.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pl = PinballLoss(level=0.9)\n    &gt;&gt;&gt; pl(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])  # doctest: +SKIP\n    0.275\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self, level: float = 0.5) -&gt; None:\n        super().__init__(degree=1, level=level)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.PinballLoss.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.PinballLoss.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 1:\n        # Fast path\n        score = z - y\n    elif self.degree &gt; 1 and self.degree % 2 == 1:\n        # Odd positive degree\n        score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = np.log(z / y)\n    else:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n\n    if self.level == 0.5:\n        return 0.5 * np.abs(score)\n    else:\n        return (np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.PoissonDeviance","title":"<code>PoissonDeviance</code>","text":"<p>Poisson deviance.</p> <p>The smaller the better, minimum is zero.</p> <p>The Poisson deviance is strictly consistent for the mean. It has a degree of homogeneity of 1.</p> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"mean\"</p> Notes <p>\\(S(y, z) = 2(y\\log\\frac{y}{z} - y + z)\\)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pd = PoissonDeviance()\n&gt;&gt;&gt; pd(y_obs=[0, 0, 1, 1], y_pred=[2, 1, 1 , 2])\n1.6534264097200273\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class PoissonDeviance(HomogeneousExpectileScore):\n    r\"\"\"Poisson deviance.\n\n    The smaller the better, minimum is zero.\n\n    The Poisson deviance is strictly consistent for the mean.\n    It has a degree of homogeneity of 1.\n\n    Attributes\n    ----------\n    functional: str\n        \"mean\"\n\n    Notes\n    -----\n    \\(S(y, z) = 2(y\\log\\frac{y}{z} - y + z)\\)\n\n    Examples\n    --------\n    &gt;&gt;&gt; pd = PoissonDeviance()\n    &gt;&gt;&gt; pd(y_obs=[0, 0, 1, 1], y_pred=[2, 1, 1 , 2])  # doctest: +SKIP\n    1.6534264097200273\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self) -&gt; None:\n        super().__init__(degree=1, level=0.5)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.PoissonDeviance.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.PoissonDeviance.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 2:\n        # Fast path\n        score = np.square(z - y)\n    elif self.degree &gt; 1:\n        z_abs = np.abs(z)\n        score = 2 * (\n            (np.power(np.abs(y), self.degree) - np.power(z_abs, self.degree))\n            / (self.degree * (self.degree - 1))\n            - np.sign(z)\n            / (self.degree - 1)\n            * np.power(z_abs, self.degree - 1)\n            * (y - z)\n        )\n    elif self.degree == 1:\n        # Domain: y &gt;= 0 and z &gt; 0\n        if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is y_obs &gt;= 0 and \"\n                \"y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = 2 * (special.xlogy(y, y / z) - y + z)\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        y_z = y / z\n        score = 2 * (y_z - np.log(y_z) - 1)\n    else:  # self.degree &lt; 1\n        # Domain: y &gt;= 0 and z &gt; 0 for 0 &lt; self.degree &lt; 1\n        # Domain: y &gt; 0  and z &gt; 0 else\n        if self.degree &gt; 0:\n            if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt;= 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n        elif not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        # Note: We add 0.0 to be sure we have floating points. Integers are not\n        # allowerd to be raised to a negative power.\n        score = 2 * (\n            (np.power(y + 0.0, self.degree) - np.power(z + 0.0, self.degree))\n            / (self.degree * (self.degree - 1))\n            - 1 / (self.degree - 1) * np.power(z + 0.0, self.degree - 1) * (y - z)\n        )\n\n    if self.level == 0.5:\n        return score\n    else:\n        return 2 * np.abs(np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.SquaredError","title":"<code>SquaredError</code>","text":"<p>Squared error.</p> <p>The smaller the better, minimum is zero.</p> <p>The squared error is strictly consistent for the mean. It has a degree of homogeneity of 2. In the context of probabilistic classification, it is also known as Brier score.</p> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"mean\"</p> Notes <p>\\(S(y, z) = (y - z)^2\\)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; se = SquaredError()\n&gt;&gt;&gt; se(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n0.75\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class SquaredError(HomogeneousExpectileScore):\n    r\"\"\"Squared error.\n\n    The smaller the better, minimum is zero.\n\n    The squared error is strictly consistent for the mean.\n    It has a degree of homogeneity of 2.\n    In the context of probabilistic classification, it is also known as Brier score.\n\n\n    Attributes\n    ----------\n    functional: str\n        \"mean\"\n\n    Notes\n    -----\n    \\(S(y, z) = (y - z)^2\\)\n\n    Examples\n    --------\n    &gt;&gt;&gt; se = SquaredError()\n    &gt;&gt;&gt; se(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])  # doctest: +SKIP\n    0.75\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self) -&gt; None:\n        super().__init__(degree=2, level=0.5)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.SquaredError.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.SquaredError.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 2:\n        # Fast path\n        score = np.square(z - y)\n    elif self.degree &gt; 1:\n        z_abs = np.abs(z)\n        score = 2 * (\n            (np.power(np.abs(y), self.degree) - np.power(z_abs, self.degree))\n            / (self.degree * (self.degree - 1))\n            - np.sign(z)\n            / (self.degree - 1)\n            * np.power(z_abs, self.degree - 1)\n            * (y - z)\n        )\n    elif self.degree == 1:\n        # Domain: y &gt;= 0 and z &gt; 0\n        if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is y_obs &gt;= 0 and \"\n                \"y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = 2 * (special.xlogy(y, y / z) - y + z)\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        y_z = y / z\n        score = 2 * (y_z - np.log(y_z) - 1)\n    else:  # self.degree &lt; 1\n        # Domain: y &gt;= 0 and z &gt; 0 for 0 &lt; self.degree &lt; 1\n        # Domain: y &gt; 0  and z &gt; 0 else\n        if self.degree &gt; 0:\n            if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt;= 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n        elif not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        # Note: We add 0.0 to be sure we have floating points. Integers are not\n        # allowerd to be raised to a negative power.\n        score = 2 * (\n            (np.power(y + 0.0, self.degree) - np.power(z + 0.0, self.degree))\n            / (self.degree * (self.degree - 1))\n            - 1 / (self.degree - 1) * np.power(z + 0.0, self.degree - 1) * (y - z)\n        )\n\n    if self.level == 0.5:\n        return score\n    else:\n        return 2 * np.abs(np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.decompose","title":"<code>decompose(y_obs, y_pred, weights=None, *, scoring_function, functional=None, level=None)</code>","text":"<p>Additive decomposition of scores.</p> <p>The score is decomposed as <code>score = miscalibration - discrimination + uncertainty</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <code>scoring_function</code> <code>callable</code> <p>A scoring function with signature roughly <code>fun(y_obs, y_pred, weights) -&gt; float</code>.</p> required <code>functional</code> <code>str or None</code> <p>The target functional which <code>y_pred</code> aims to predict. If <code>None</code>, then it will be inferred from <code>scoring_function.functional</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>None</code> <code>level</code> <code>float or None</code> <p>Functionals like expectiles and quantiles have a level (often called alpha). If <code>None</code>, then it will be inferred from <code>scoring_function.level</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>decomposition</code> <code>DataFrame</code> <p>The resulting score decomposition as a dataframe with columns:</p> <ul> <li><code>miscalibration</code></li> <li><code>discrimination</code></li> <li><code>uncertainty</code></li> <li><code>score</code>: the average score</li> </ul> <code>If `y_pred` contains several predictions, i.e. it is 2-dimension with shape</code> <code>`(n_obs, n_pred)` and `n_pred &gt;1`, then there is the additional column:</code> <ul> <li><code>model</code></li> </ul> Notes <p>To be precise, this function returns the decomposition of the score in terms of auto-miscalibration, auto-discrimination (or resolution) and uncertainy (or entropy), see <code>[FLM2022]</code> and references therein. The key element is to estimate the recalibrated predictions, i.e. \\(T(Y|m(X))\\) for the target functional \\(T\\) and model predictions \\(m(X)\\). This is accomplished by isotonic regression, <code>[Dimitriadis2021]</code> and <code>[Gneiting2021]</code>.</p> References <code>[FLM2022]</code> <p>T. Fissler, C. Lorentzen, and M. Mayer. \"Model Comparison and Calibration Assessment\". (2022) arxiv:2202.12780.</p> <code>[Dimitriadis2021]</code> <p>T. Dimitriadis, T. Gneiting, and A. I. Jordan. \"Stable reliability diagrams for probabilistic classifiers\". (2021) doi:10.1073/pnas.2016191118</p> <code>[Gneiting2021]</code> <p>T. Gneiting and J. Resin. \"Regression Diagnostics meets Forecast Evaluation: Conditional Calibration, Reliability Diagrams, and Coefficient of Determination\". (2021). arXiv:2108.03210.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; decompose(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1, 2],\n... scoring_function=SquaredError())\nshape: (1, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 miscalibration \u2506 discrimination \u2506 uncertainty \u2506 score \u2502\n\u2502 ---            \u2506 ---            \u2506 ---         \u2506 ---   \u2502\n\u2502 f64            \u2506 f64            \u2506 f64         \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.625          \u2506 0.125          \u2506 0.25        \u2506 0.75  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def decompose(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    scoring_function: Callable[..., Any],  # TODO: make type hint stricter\n    functional: Optional[str] = None,\n    level: Optional[float] = None,\n) -&gt; pl.DataFrame:\n    r\"\"\"Additive decomposition of scores.\n\n    The score is decomposed as\n    `score = miscalibration - discrimination + uncertainty`.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n    scoring_function : callable\n        A scoring function with signature roughly\n        `fun(y_obs, y_pred, weights) -&gt; float`.\n    functional : str or None\n        The target functional which `y_pred` aims to predict.\n        If `None`, then it will be inferred from `scoring_function.functional`.\n        Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n    level : float or None\n        Functionals like expectiles and quantiles have a level (often called alpha).\n        If `None`, then it will be inferred from `scoring_function.level`.\n\n    Returns\n    -------\n    decomposition : polars.DataFrame\n        The resulting score decomposition as a dataframe with columns:\n\n        - `miscalibration`\n        - `discrimination`\n        - `uncertainty`\n        - `score`: the average score\n\n    If `y_pred` contains several predictions, i.e. it is 2-dimension with shape\n    `(n_obs, n_pred)` and `n_pred &gt;1`, then there is the additional column:\n\n        - `model`\n\n    Notes\n    -----\n    To be precise, this function returns the decomposition of the score in terms of\n    auto-miscalibration, auto-discrimination (or resolution) and uncertainy (or\n    entropy), see `[FLM2022]` and references therein.\n    The key element is to estimate the recalibrated predictions, i.e. \\(T(Y|m(X))\\) for\n    the target functional \\(T\\) and model predictions \\(m(X)\\).\n    This is accomplished by isotonic regression, `[Dimitriadis2021]` and\n    `[Gneiting2021]`.\n\n    References\n    ----------\n    `[FLM2022]`\n\n    :   T. Fissler, C. Lorentzen, and M. Mayer.\n        \"Model Comparison and Calibration Assessment\". (2022)\n        [arxiv:2202.12780](https://arxiv.org/abs/2202.12780).\n\n    `[Dimitriadis2021]`\n\n    :   T. Dimitriadis, T. Gneiting, and A. I. Jordan.\n        \"Stable reliability diagrams for probabilistic classifiers\". (2021)\n        [doi:10.1073/pnas.2016191118](https://doi.org/10.1073/pnas.2016191118)\n\n    `[Gneiting2021]`\n\n    :   T. Gneiting and J. Resin.\n        \"Regression Diagnostics meets Forecast Evaluation: Conditional Calibration,\n        Reliability Diagrams, and Coefficient of Determination\". (2021).\n        [arXiv:2108.03210](https://arxiv.org/abs/2108.03210).\n\n    Examples\n    --------\n    &gt;&gt;&gt; decompose(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1, 2],\n    ... scoring_function=SquaredError())\n    shape: (1, 4)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 miscalibration \u2506 discrimination \u2506 uncertainty \u2506 score \u2502\n    \u2502 ---            \u2506 ---            \u2506 ---         \u2506 ---   \u2502\n    \u2502 f64            \u2506 f64            \u2506 f64         \u2506 f64   \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 0.625          \u2506 0.125          \u2506 0.25        \u2506 0.75  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    if functional is None:\n        if hasattr(scoring_function, \"functional\"):\n            functional = scoring_function.functional\n        else:\n            msg = (\n                \"You set functional=None, but scoring_function has no attribute \"\n                \"functional.\"\n            )\n            raise ValueError(msg)\n    if level is None:\n        level = 0.5\n        if functional in (\"expectile\", \"quantile\"):\n            if hasattr(scoring_function, \"level\"):\n                level = float(scoring_function.level)\n            else:\n                msg = (\n                    \"You set level=None, but scoring_function has no attribute \"\n                    \"level.\"\n                )\n                raise ValueError(msg)\n\n    allowed_functionals = (\"mean\", \"median\", \"expectile\", \"quantile\")\n    if functional not in allowed_functionals:\n        msg = (\n            f\"The functional must be one of {allowed_functionals}, got \"\n            f\"{functional}.\"\n        )\n        raise ValueError(msg)\n    if functional in (\"expectile\", \"quantile\") and (level &lt;= 0 or level &gt;= 1):\n        msg = f\"The level must fulfil 0 &lt; level &lt; 1, got {level}.\"\n        raise ValueError(msg)\n\n    validate_same_first_dimension(y_obs, y_pred)\n    n_pred = length_of_second_dimension(y_pred)\n    pred_names, _ = get_sorted_array_names(y_pred)\n    y_o = np.asarray(y_obs)\n\n    if weights is None:\n        w = None\n    else:\n        validate_same_first_dimension(weights, y_o)\n        w = np.asarray(weights)  # needed to satisfy mypy\n        if w.ndim &gt; 1:\n            msg = f\"The array weights must be 1-dimensional, got weights.ndim={w.ndim}.\"\n            raise ValueError(msg)\n\n    if functional == \"mean\":\n        iso = IsotonicRegression_skl(y_min=None, y_max=None)\n        marginal = np.average(y_o, weights=w)\n    else:\n        iso = IsotonicRegression(functional=functional, level=level)\n        if functional == \"expectile\":\n            marginal = expectile(y_o, alpha=level, weights=w)\n        elif functional == \"quantile\":\n            marginal = 0.5 * (\n                quantile_lower(y_o, level=level) + quantile_upper(y_o, level=level)\n            )\n\n    if y_o[0] == marginal == y_o[-1]:\n        # y_o is constant. We need to check if y_o is allowed as argument to y_pred.\n        # For instance for the poisson deviance, y_o = 0 is allowed. But 0 is forbidden\n        # as a prediction.\n        try:\n            scoring_function(y_o[0], marginal)\n        except ValueError as exc:\n            msg = (\n                \"Your y_obs is constant and lies outside the allowed range of y_pred \"\n                \"of your scoring function. Therefore, the score decomposition cannot \"\n                \"be applied.\"\n            )\n            raise ValueError(msg) from exc\n\n    # The recalibrated versions, further down, could contain min(y_obs) and that could\n    # be outside of the valid domain, e.g. y_pred = 0 for the Poisson deviance where\n    # y_obs=0 is allowed. We detect that here:\n    y_min = np.amin(y_o)\n    y_min_allowed = True\n    try:\n        scoring_function(y_o[:1], np.array([y_min]), None if w is None else w[:1])\n    except ValueError:\n        y_min_allowed = False\n\n    marginal = np.full_like(y_o, fill_value=marginal, dtype=float)\n    score_marginal = scoring_function(y_o, marginal, w)\n\n    df_list = []\n    for i in range(len(pred_names)):\n        # Loop over columns of y_pred.\n        x = y_pred if n_pred == 0 else get_second_dimension(y_pred, i)\n        iso.fit(x, y_o, sample_weight=w)\n        recalibrated = np.squeeze(iso.predict(x))\n        if not y_min_allowed and recalibrated[0] &lt;= y_min:\n            # Oh dear, this needs quite some extra work:\n            # First index of value greater than y_min\n            idx1 = np.argmax(recalibrated &gt; y_min)\n            val1 = recalibrated[idx1]\n            # First index of value greater than the value at idx1.\n            idx2 = np.argmax(recalibrated &gt; val1)\n            # Note that val1 may already be the largest value of the array =&gt; idx2 = 0.\n            if idx2 == 0:\n                idx2 = recalibrated.shape[0]\n            # We merge the first 2 blocks of the isotonic regression as it violates\n            # our domain requirements.\n            re2 = recalibrated[:idx2]\n            w2 = None if w is None else w[:idx2]\n            if functional == \"mean\":\n                recalibrated[:idx2] = np.average(re2, weights=w2)\n            elif functional == \"expectile\":\n                recalibrated[:idx2] = expectile(re2, alpha=level, weights=w2)\n            elif functional == \"quantile\":\n                # Note, no scoring function known that could end up here.\n                lower = quantile_lower(re2, level=level)\n                upper = quantile_upper(re2, level=level)\n                recalibrated[:idx2] = 0.5 * (lower + upper)\n\n        score = scoring_function(y_o, x, w)\n        try:\n            score_recalibrated = scoring_function(y_o, recalibrated, w)\n        except ValueError as exc:\n            msg = (\n                \"The recalibrated predictions obtained from isotonic regression are \"\n                \"very likely outside the allowed range of y_pred of your scoring \"\n                \"function. Therefore, the score decomposition cannot be applied.\"\n            )\n            raise ValueError(msg) from exc\n\n        df = pl.DataFrame(\n            {\n                \"model\": pred_names[i],\n                \"miscalibration\": score - score_recalibrated,\n                \"discrimination\": score_marginal - score_recalibrated,\n                \"uncertainty\": score_marginal,\n                \"score\": score,\n            }\n        )\n        df_list.append(df)\n\n    df = pl.concat(df_list)\n\n    # Remove column \"model\" for a single model.\n    if n_pred &lt;= 1:\n        df = df.drop(\"model\")\n\n    return df\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/#model_diagnostics.scoring.plot_murphy_diagram","title":"<code>plot_murphy_diagram(y_obs, y_pred, weights=None, *, etas=100, functional='mean', level=0.5, ax=None)</code>","text":"<p>Plot a Murphy diagram.</p> <p>A Murphy diagram plots the scores of elementary scoring functions <code>ElementaryScore</code> over a range of their free parameter <code>eta</code>. This shows, if a model dominates all others over a wide class of scoring functions or if the ranking is very much dependent on the choice of scoring function. See Notes for further details.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <code>etas</code> <code>int or array - like</code> <p>If an integer is given, equidistant points between min and max y values are generater. If an array-like is given, those points are used.</p> <code>100</code> <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile of quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> <code>ax</code> <code>Axes</code> <p>Axes object to draw the plot onto, otherwise uses the current Axes.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <p>Either the matplotlib axes or the plotly figure. This is configurable by setting the <code>plot_backend</code> via <code>model_diagnostics.set_config</code> or <code>model_diagnostics.config_context</code>.</p> Notes <p> For details, refer to  For details, refer to <code>[Ehm2015]</code>.</p> References <code>[Ehm2015]</code> <p>W. Ehm, T. Gneiting, A. Jordan, F. Kr\u00fcger. \"Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet Representations, and Forecast Rankings\". arxiv:1503.08195.</p> Source code in <code>src/model_diagnostics/scoring/plots.py</code> <pre><code>def plot_murphy_diagram(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    etas: Union[int, npt.ArrayLike] = 100,\n    functional: str = \"mean\",\n    level: float = 0.5,\n    ax: Optional[mpl.axes.Axes] = None,\n):\n    r\"\"\"Plot a Murphy diagram.\n\n    A Murphy diagram plots the scores of elementary scoring functions `ElementaryScore`\n    over a range of their free parameter `eta`. This shows, if a model dominates all\n    others over a wide class of scoring functions or if the ranking is very much\n    dependent on the choice of scoring function.\n    See [Notes](#notes) for further details.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n    etas : int or array-like\n        If an integer is given, equidistant points between min and max y values are\n        generater. If an array-like is given, those points are used.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n    level : float\n        The level of the expectile of quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n    ax : matplotlib.axes.Axes\n        Axes object to draw the plot onto, otherwise uses the current Axes.\n\n    Returns\n    -------\n    ax :\n        Either the matplotlib axes or the plotly figure. This is configurable by\n        setting the `plot_backend` via\n        [`model_diagnostics.set_config`][model_diagnostics.set_config] or\n        [`model_diagnostics.config_context`][model_diagnostics.config_context].\n\n    Notes\n    -----\n    [](){#notes}\n    For details, refer to `[Ehm2015]`.\n\n    References\n    ----------\n    `[Ehm2015]`\n\n    :   W. Ehm, T. Gneiting, A. Jordan, F. Kr\u00fcger.\n        \"Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet\n        Representations, and Forecast Rankings\".\n        [arxiv:1503.08195](https://arxiv.org/abs/1503.08195).\n    \"\"\"\n    if ax is None:\n        plot_backend = get_config()[\"plot_backend\"]\n        if plot_backend == \"matplotlib\":\n            ax = plt.gca()\n        else:\n            import plotly.graph_objects as go\n\n            fig = ax = go.Figure()\n    elif isinstance(ax, mpl.axes.Axes):\n        plot_backend = \"matplotlib\"\n    elif is_plotly_figure(ax):\n        import plotly.graph_objects as go\n\n        plot_backend = \"plotly\"\n        fig = ax\n    else:\n        msg = (\n            \"The ax argument must be None, a matplotlib Axes or a plotly Figure, \"\n            f\"got {type(ax)}.\"\n        )\n        raise ValueError(msg)\n\n    if (n_cols := length_of_second_dimension(y_obs)) &gt; 0:\n        if n_cols == 1:\n            y_obs = get_second_dimension(y_obs, 0)\n        else:\n            msg = (\n                f\"Array-like y_obs has more than 2 dimensions, y_obs.shape[1]={n_cols}\"\n            )\n            raise ValueError(msg)\n\n    y_pred_min, y_pred_max = get_array_min_max(y_pred)\n    y_obs_min, y_obs_max = get_array_min_max(y_obs)\n    y_min, y_max = min(y_pred_min, y_obs_min), max(y_pred_max, y_obs_max)\n\n    if y_min == y_max:\n        msg = \"All values y_obs and y_pred are one single and same value.\"\n        raise ValueError(msg)\n    elif isinstance(etas, numbers.Integral):\n        etas = np.linspace(y_min, y_max, num=etas, endpoint=True)\n    else:\n        etas = np.asarray(etas).astype(float)\n        if etas.ndim &gt; 1:\n            etas = etas.reshape(max(etas.shape))\n\n    def elementary_score(y_obs, y_pred, weights, eta):\n        sf = ElementaryScore(eta, functional=functional, level=level)\n        return sf(y_obs=y_obs, y_pred=y_pred, weights=weights)\n\n    n_pred = length_of_second_dimension(y_pred)\n    pred_names, _ = get_sorted_array_names(y_pred)\n\n    for i in range(len(pred_names)):\n        y_pred_i = y_pred if n_pred == 0 else get_second_dimension(y_pred, i)\n\n        y_plot = [\n            elementary_score(y_obs=y_obs, y_pred=y_pred_i, weights=weights, eta=eta)\n            for eta in etas\n        ]\n        label = pred_names[i] if n_pred &gt;= 2 else None\n        if plot_backend == \"matplotlib\":\n            ax.plot(etas, y_plot, label=label)\n        else:\n            fig.add_scatter(\n                x=etas,\n                y=y_plot,\n                mode=\"lines\",\n                line={\"color\": get_plotly_color(i)},\n                name=label,\n            )\n\n    xlabel = \"eta\"\n    ylabel = \"score\"\n    title = \"Murphy Diagram\"\n    if n_pred &lt;= 1 and len(pred_names[0]) &gt; 0:\n        title = title + \" \" + pred_names[0]\n\n    if plot_backend == \"matplotlib\":\n        if n_pred &gt;= 2:\n            ax.legend()\n        ax.set_title(title)\n        ax.set(xlabel=xlabel, ylabel=ylabel)\n    else:\n        if n_pred &lt;= 1:\n            fig.update_layout(showlegend=False)\n        fig.update_layout(xaxis_title=xlabel, yaxis_title=ylabel, title=title)\n\n    return ax\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/plots/","title":"plots","text":""},{"location":"reference/model_diagnostics/scoring/plots/#model_diagnostics.scoring.plots","title":"<code>plots</code>","text":""},{"location":"reference/model_diagnostics/scoring/plots/#model_diagnostics.scoring.plots.plot_murphy_diagram","title":"<code>plot_murphy_diagram(y_obs, y_pred, weights=None, *, etas=100, functional='mean', level=0.5, ax=None)</code>","text":"<p>Plot a Murphy diagram.</p> <p>A Murphy diagram plots the scores of elementary scoring functions <code>ElementaryScore</code> over a range of their free parameter <code>eta</code>. This shows, if a model dominates all others over a wide class of scoring functions or if the ranking is very much dependent on the choice of scoring function. See Notes for further details.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable. For binary classification, y_obs is expected to be in the interval [0, 1].</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values, e.g. for the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <code>etas</code> <code>int or array - like</code> <p>If an integer is given, equidistant points between min and max y values are generater. If an array-like is given, those points are used.</p> <code>100</code> <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile of quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> <code>ax</code> <code>Axes</code> <p>Axes object to draw the plot onto, otherwise uses the current Axes.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <p>Either the matplotlib axes or the plotly figure. This is configurable by setting the <code>plot_backend</code> via <code>model_diagnostics.set_config</code> or <code>model_diagnostics.config_context</code>.</p> Notes <p> For details, refer to  For details, refer to <code>[Ehm2015]</code>.</p> References <code>[Ehm2015]</code> <p>W. Ehm, T. Gneiting, A. Jordan, F. Kr\u00fcger. \"Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet Representations, and Forecast Rankings\". arxiv:1503.08195.</p> Source code in <code>src/model_diagnostics/scoring/plots.py</code> <pre><code>def plot_murphy_diagram(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    etas: Union[int, npt.ArrayLike] = 100,\n    functional: str = \"mean\",\n    level: float = 0.5,\n    ax: Optional[mpl.axes.Axes] = None,\n):\n    r\"\"\"Plot a Murphy diagram.\n\n    A Murphy diagram plots the scores of elementary scoring functions `ElementaryScore`\n    over a range of their free parameter `eta`. This shows, if a model dominates all\n    others over a wide class of scoring functions or if the ranking is very much\n    dependent on the choice of scoring function.\n    See [Notes](#notes) for further details.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n        For binary classification, y_obs is expected to be in the interval [0, 1].\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values, e.g. for the conditional expectation of the response,\n        `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n    etas : int or array-like\n        If an integer is given, equidistant points between min and max y values are\n        generater. If an array-like is given, those points are used.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n    level : float\n        The level of the expectile of quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n    ax : matplotlib.axes.Axes\n        Axes object to draw the plot onto, otherwise uses the current Axes.\n\n    Returns\n    -------\n    ax :\n        Either the matplotlib axes or the plotly figure. This is configurable by\n        setting the `plot_backend` via\n        [`model_diagnostics.set_config`][model_diagnostics.set_config] or\n        [`model_diagnostics.config_context`][model_diagnostics.config_context].\n\n    Notes\n    -----\n    [](){#notes}\n    For details, refer to `[Ehm2015]`.\n\n    References\n    ----------\n    `[Ehm2015]`\n\n    :   W. Ehm, T. Gneiting, A. Jordan, F. Kr\u00fcger.\n        \"Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet\n        Representations, and Forecast Rankings\".\n        [arxiv:1503.08195](https://arxiv.org/abs/1503.08195).\n    \"\"\"\n    if ax is None:\n        plot_backend = get_config()[\"plot_backend\"]\n        if plot_backend == \"matplotlib\":\n            ax = plt.gca()\n        else:\n            import plotly.graph_objects as go\n\n            fig = ax = go.Figure()\n    elif isinstance(ax, mpl.axes.Axes):\n        plot_backend = \"matplotlib\"\n    elif is_plotly_figure(ax):\n        import plotly.graph_objects as go\n\n        plot_backend = \"plotly\"\n        fig = ax\n    else:\n        msg = (\n            \"The ax argument must be None, a matplotlib Axes or a plotly Figure, \"\n            f\"got {type(ax)}.\"\n        )\n        raise ValueError(msg)\n\n    if (n_cols := length_of_second_dimension(y_obs)) &gt; 0:\n        if n_cols == 1:\n            y_obs = get_second_dimension(y_obs, 0)\n        else:\n            msg = (\n                f\"Array-like y_obs has more than 2 dimensions, y_obs.shape[1]={n_cols}\"\n            )\n            raise ValueError(msg)\n\n    y_pred_min, y_pred_max = get_array_min_max(y_pred)\n    y_obs_min, y_obs_max = get_array_min_max(y_obs)\n    y_min, y_max = min(y_pred_min, y_obs_min), max(y_pred_max, y_obs_max)\n\n    if y_min == y_max:\n        msg = \"All values y_obs and y_pred are one single and same value.\"\n        raise ValueError(msg)\n    elif isinstance(etas, numbers.Integral):\n        etas = np.linspace(y_min, y_max, num=etas, endpoint=True)\n    else:\n        etas = np.asarray(etas).astype(float)\n        if etas.ndim &gt; 1:\n            etas = etas.reshape(max(etas.shape))\n\n    def elementary_score(y_obs, y_pred, weights, eta):\n        sf = ElementaryScore(eta, functional=functional, level=level)\n        return sf(y_obs=y_obs, y_pred=y_pred, weights=weights)\n\n    n_pred = length_of_second_dimension(y_pred)\n    pred_names, _ = get_sorted_array_names(y_pred)\n\n    for i in range(len(pred_names)):\n        y_pred_i = y_pred if n_pred == 0 else get_second_dimension(y_pred, i)\n\n        y_plot = [\n            elementary_score(y_obs=y_obs, y_pred=y_pred_i, weights=weights, eta=eta)\n            for eta in etas\n        ]\n        label = pred_names[i] if n_pred &gt;= 2 else None\n        if plot_backend == \"matplotlib\":\n            ax.plot(etas, y_plot, label=label)\n        else:\n            fig.add_scatter(\n                x=etas,\n                y=y_plot,\n                mode=\"lines\",\n                line={\"color\": get_plotly_color(i)},\n                name=label,\n            )\n\n    xlabel = \"eta\"\n    ylabel = \"score\"\n    title = \"Murphy Diagram\"\n    if n_pred &lt;= 1 and len(pred_names[0]) &gt; 0:\n        title = title + \" \" + pred_names[0]\n\n    if plot_backend == \"matplotlib\":\n        if n_pred &gt;= 2:\n            ax.legend()\n        ax.set_title(title)\n        ax.set(xlabel=xlabel, ylabel=ylabel)\n    else:\n        if n_pred &lt;= 1:\n            fig.update_layout(showlegend=False)\n        fig.update_layout(xaxis_title=xlabel, yaxis_title=ylabel, title=title)\n\n    return ax\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/","title":"scoring","text":""},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring","title":"<code>scoring</code>","text":"<p>The scoring module provides scoring functions, also known as loss functions, and a score decomposition. Each scoring function is implemented as a class that needs to be instantiated before calling the <code>__call__</code> methode, e.g. <code>SquaredError()(y_obs=[1], y_pred=[2])</code>.</p>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.ElementaryScore","title":"<code>ElementaryScore</code>","text":"<p>Elementary scoring function.</p> <p>The smaller the better.</p> <p>The elementary scoring function is consistent for the specified <code>functional</code> for all values of <code>eta</code> and is the main ingredient for Murphy diagrams. See Notes for further details.</p> <p>Parameters:</p> Name Type Description Default <code>eta</code> <code>float</code> <p>Free parameter.</p> required <code>functional</code> <code>str</code> <p>The functional that is induced by the identification function <code>V</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>'mean'</code> <code>level</code> <code>float</code> <p>The level of the expectile of quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> and <code>functional=\"expectile\"</code> gives the mean. <code>level=0.5</code> and <code>functional=\"quantile\"</code> gives the median.</p> <code>0.5</code> Notes <p> The elementary scoring or loss function is given by The elementary scoring or loss function is given by</p> \\[ S_\\eta(y, z) = (\\mathbf{1}\\{\\eta \\le z\\} - \\mathbf{1}\\{\\eta \\le y\\}) V(y, \\eta) \\] <p>with identification functions \\(V\\) for the given <code>functional</code> \\(T\\) . If allows for the mixture or Choquet representation</p> \\[ S(y, z) = \\int S_\\eta(y, z) \\,dH(\\eta) \\] <p>for some locally finite measure \\(H\\). It follows that the scoring function \\(S\\) is consistent for \\(T\\).</p> References <code>[Jordan2022]</code> <p>A.I. Jordan, A. M\u00fchlemann, J.F. Ziegel. \"Characterizing the optimal solutions to the isotonic regression problem for identifiable functionals\". (2022) doi:10.1007/s10463-021-00808-0</p> <code>[GneitingResin2022]</code> <p>T. Gneiting, J. Resin. \"Regression Diagnostics meets Forecast Evaluation: Conditional Calibration, Reliability Diagrams, and Coefficient of Determination\". arxiv:2108.03210</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; el_score = ElementaryScore(eta=2, functional=\"mean\")\n&gt;&gt;&gt; el_score(y_obs=[1, 2, 2, 1], y_pred=[4, 1, 2, 3])\n0.5\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class ElementaryScore(_BaseScoringFunction):\n    r\"\"\"Elementary scoring function.\n\n    The smaller the better.\n\n    The elementary scoring function is consistent for the specified `functional` for\n    all values of `eta` and is the main ingredient for Murphy diagrams.\n    See [Notes](#notes) for further details.\n\n    Parameters\n    ----------\n    eta : float\n        Free parameter.\n    functional : str\n        The functional that is induced by the identification function `V`. Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n    level : float\n        The level of the expectile of quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` and `functional=\"expectile\"` gives the mean.\n        `level=0.5` and `functional=\"quantile\"` gives the median.\n\n    Notes\n    -----\n    [](){#notes}\n    The elementary scoring or loss function is given by\n\n    \\[\n    S_\\eta(y, z) = (\\mathbf{1}\\{\\eta \\le z\\} - \\mathbf{1}\\{\\eta \\le y\\})\n    V(y, \\eta)\n    \\]\n\n    with [identification functions]\n    [model_diagnostics.calibration.identification_function]\n    \\(V\\) for the given `functional` \\(T\\) . If allows for the mixture or Choquet\n    representation\n\n    \\[\n    S(y, z) = \\int S_\\eta(y, z) \\,dH(\\eta)\n    \\]\n\n    for some locally finite measure \\(H\\). It follows that the scoring function \\(S\\)\n    is consistent for \\(T\\).\n\n    References\n    ----------\n    `[Jordan2022]`\n\n    :   A.I. Jordan, A. M\u00fchlemann, J.F. Ziegel.\n        \"Characterizing the optimal solutions to the isotonic regression problem for\n        identifiable functionals\". (2022)\n        [doi:10.1007/s10463-021-00808-0](https://doi.org/10.1007/s10463-021-00808-0)\n\n    `[GneitingResin2022]`\n\n    :   T. Gneiting, J. Resin.\n        \"Regression Diagnostics meets Forecast Evaluation: Conditional Calibration,\n        Reliability Diagrams, and Coefficient of Determination\".\n        [arxiv:2108.03210](https://arxiv.org/abs/2108.03210)\n\n    Examples\n    --------\n    &gt;&gt;&gt; el_score = ElementaryScore(eta=2, functional=\"mean\")\n    &gt;&gt;&gt; el_score(y_obs=[1, 2, 2, 1], y_pred=[4, 1, 2, 3])  # doctest: +SKIP\n    0.5\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(\n        self, eta: float, functional: str = \"mean\", level: float = 0.5\n    ) -&gt; None:\n        self.eta = eta\n        self._functional = functional\n        if level &lt;= 0 or level &gt;= 1:\n            msg = f\"Argument level must fulfil 0 &lt; level &lt; 1, got {level}.\"\n            raise ValueError(msg)\n        self.level = level\n\n    @property\n    def functional(self):\n        return self._functional\n\n    def score_per_obs(\n        self,\n        y_obs: npt.ArrayLike,\n        y_pred: npt.ArrayLike,\n    ) -&gt; np.ndarray:\n        \"\"\"Score per observation.\n\n        Parameters\n        ----------\n        y_obs : array-like of shape (n_obs)\n            Observed values of the response variable.\n        y_pred : array-like of shape (n_obs)\n            Predicted values of the `functional` of interest, e.g. the conditional\n            expectation of the response, `E(Y|X)`.\n\n        Returns\n        -------\n        score_per_obs : ndarray\n            Values of the scoring function for each observation.\n        \"\"\"\n        y: np.ndarray\n        z: np.ndarray\n        y, z = validate_2_arrays(y_obs, y_pred)\n\n        eta = self.eta\n        eta_term = np.less_equal(eta, z).astype(float) - np.less_equal(eta, y)\n        return eta_term * identification_function(\n            y_obs=y_obs,\n            y_pred=np.full(y.shape, fill_value=float(eta)),\n            functional=self.functional,\n            level=self.level,\n        )\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.ElementaryScore.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.ElementaryScore.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    eta = self.eta\n    eta_term = np.less_equal(eta, z).astype(float) - np.less_equal(eta, y)\n    return eta_term * identification_function(\n        y_obs=y_obs,\n        y_pred=np.full(y.shape, fill_value=float(eta)),\n        functional=self.functional,\n        level=self.level,\n    )\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.GammaDeviance","title":"<code>GammaDeviance</code>","text":"<p>Gamma deviance.</p> <p>The smaller the better, minimum is zero.</p> <p>The Gamma deviance is strictly consistent for the mean. It has a degree of homogeneity of 0 and is therefore insensitive to a change of units or multiplication of <code>y_obs</code> and <code>y_pred</code> by the same positive constant.</p> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"mean\"</p> Notes <p>\\(S(y, z) = 2(\\frac{y}{z} -\\log\\frac{y}{z} - 1)\\)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gd = GammaDeviance()\n&gt;&gt;&gt; gd(y_obs=[3, 2, 1, 1], y_pred=[2, 1, 1 , 2])\n0.2972674459459178\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class GammaDeviance(HomogeneousExpectileScore):\n    r\"\"\"Gamma deviance.\n\n    The smaller the better, minimum is zero.\n\n    The Gamma deviance is strictly consistent for the mean.\n    It has a degree of homogeneity of 0 and is therefore insensitive to a change of\n    units or multiplication of `y_obs` and `y_pred` by the same positive constant.\n\n    Attributes\n    ----------\n    functional: str\n        \"mean\"\n\n    Notes\n    -----\n    \\(S(y, z) = 2(\\frac{y}{z} -\\log\\frac{y}{z} - 1)\\)\n\n    Examples\n    --------\n    &gt;&gt;&gt; gd = GammaDeviance()\n    &gt;&gt;&gt; gd(y_obs=[3, 2, 1, 1], y_pred=[2, 1, 1 , 2])  # doctest: +SKIP\n    0.2972674459459178\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self) -&gt; None:\n        super().__init__(degree=0, level=0.5)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.GammaDeviance.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.GammaDeviance.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 2:\n        # Fast path\n        score = np.square(z - y)\n    elif self.degree &gt; 1:\n        z_abs = np.abs(z)\n        score = 2 * (\n            (np.power(np.abs(y), self.degree) - np.power(z_abs, self.degree))\n            / (self.degree * (self.degree - 1))\n            - np.sign(z)\n            / (self.degree - 1)\n            * np.power(z_abs, self.degree - 1)\n            * (y - z)\n        )\n    elif self.degree == 1:\n        # Domain: y &gt;= 0 and z &gt; 0\n        if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is y_obs &gt;= 0 and \"\n                \"y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = 2 * (special.xlogy(y, y / z) - y + z)\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        y_z = y / z\n        score = 2 * (y_z - np.log(y_z) - 1)\n    else:  # self.degree &lt; 1\n        # Domain: y &gt;= 0 and z &gt; 0 for 0 &lt; self.degree &lt; 1\n        # Domain: y &gt; 0  and z &gt; 0 else\n        if self.degree &gt; 0:\n            if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt;= 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n        elif not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        # Note: We add 0.0 to be sure we have floating points. Integers are not\n        # allowerd to be raised to a negative power.\n        score = 2 * (\n            (np.power(y + 0.0, self.degree) - np.power(z + 0.0, self.degree))\n            / (self.degree * (self.degree - 1))\n            - 1 / (self.degree - 1) * np.power(z + 0.0, self.degree - 1) * (y - z)\n        )\n\n    if self.level == 0.5:\n        return score\n    else:\n        return 2 * np.abs(np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.HomogeneousExpectileScore","title":"<code>HomogeneousExpectileScore</code>","text":"<p>Homogeneous scoring function of degree h for expectiles.</p> <p>The smaller the better, minimum is zero.</p> <p>Up to a multiplicative constant, these are the only scoring functions that are strictly consistent for expectiles at level alpha and homogeneous functions. The possible additive constant is chosen such that the minimal function value equals zero.</p> <p>Note that the \u00bd-expectile (level alpha=0.5) equals the mean.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>float</code> <p>Degree of homogeneity.</p> <code>2</code> <code>level</code> <code>float</code> <p>The level of the expectile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> gives the mean.</p> <code>0.5</code> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"mean\" if <code>level=0.5</code>, else \"expectile\"</p> Notes <p>The homogeneous score of degree \\(h\\) is given by</p> \\[ S_\\alpha^h(y, z) = 2 |\\mathbf{1}\\{z \\ge y\\} - \\alpha| \\frac{2}{h(h-1)} \\left(|y|^h - |z|^h - h \\operatorname{sign}(z) |z|^{h-1} (y-z)\\right) \\] <p>Note that the first term, \\(2 |\\mathbf{1}\\{z \\ge y\\} - \\alpha|\\) equals 1 for \\(\\alpha=0.5\\). There are important domain restrictions and limits:</p> <ul> <li> <p>\\(h&gt;1\\): All real numbers \\(y\\) and \\(z\\) are allowed.</p> <p>Special case \\(h=2, \\alpha=\\frac{1}{2}\\) equals the squared error, aka Normal deviance \\(S(y, z) = (y - z)^2\\).</p> </li> <li> <p>\\(0 &lt; h \\leq 1\\): Only \\(y \\geq 0\\), \\(z&gt;0\\) are allowed.</p> <p>Special case \\(h=1, \\alpha=\\frac{1}{2}\\) (by taking the limit) equals the Poisson deviance \\(S(y, z) = 2(y\\log\\frac{y}{z} - y + z)\\).</p> </li> <li> <p>\\(h \\leq 0\\): Only \\(y&gt;0\\), \\(z&gt;0\\) are allowed.</p> <p>Special case \\(h=0, \\alpha=\\frac{1}{2}\\) (by taking the limit) equals the Gamma deviance \\(S(y, z) = 2(\\frac{y}{z} -\\log\\frac{y}{z} - 1)\\).</p> </li> </ul> <p>For the common domains, \\(S_{\\frac{1}{2}}^h\\) equals the Tweedie deviance with the following relation between the degree of homogeneity \\(h\\) and the Tweedie power \\(p\\): \\(h = 2-p\\).</p> References <code>[Gneiting2011]</code> <p>T. Gneiting. \"Making and Evaluating Point Forecasts\". (2011) doi:10.1198/jasa.2011.r10138 arxiv:0912.0902</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hes = HomogeneousExpectileScore(degree=2, level=0.1)\n&gt;&gt;&gt; hes(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n0.95\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class HomogeneousExpectileScore(_BaseScoringFunction):\n    r\"\"\"Homogeneous scoring function of degree h for expectiles.\n\n    The smaller the better, minimum is zero.\n\n    Up to a multiplicative constant, these are the only scoring functions that are\n    strictly consistent for expectiles at level alpha and homogeneous functions.\n    The possible additive constant is chosen such that the minimal function value\n    equals zero.\n\n    Note that the 1/2-expectile (level alpha=0.5) equals the mean.\n\n    Parameters\n    ----------\n    degree : float\n        Degree of homogeneity.\n    level : float\n        The level of the expectile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` gives the mean.\n\n    Attributes\n    ----------\n    functional: str\n        \"mean\" if `level=0.5`, else \"expectile\"\n\n    Notes\n    -----\n    The homogeneous score of degree \\(h\\) is given by\n\n    \\[\n    S_\\alpha^h(y, z) = 2 |\\mathbf{1}\\{z \\ge y\\} - \\alpha| \\frac{2}{h(h-1)}\n    \\left(|y|^h - |z|^h - h \\operatorname{sign}(z) |z|^{h-1} (y-z)\\right)\n    \\]\n\n    Note that the first term, \\(2 |\\mathbf{1}\\{z \\ge y\\} - \\alpha|\\) equals 1 for\n    \\(\\alpha=0.5\\).\n    There are important domain restrictions and limits:\n\n    - \\(h&gt;1\\): All real numbers \\(y\\) and \\(z\\) are allowed.\n\n        Special case \\(h=2, \\alpha=\\frac{1}{2}\\) equals the squared error, aka Normal\n        deviance \\(S(y, z) = (y - z)^2\\).\n\n    - \\(0 &lt; h \\leq 1\\): Only \\(y \\geq 0\\), \\(z&gt;0\\) are allowed.\n\n        Special case \\(h=1, \\alpha=\\frac{1}{2}\\) (by taking the limit) equals the\n        Poisson deviance \\(S(y, z) = 2(y\\log\\frac{y}{z} - y + z)\\).\n\n    - \\(h \\leq 0\\): Only \\(y&gt;0\\), \\(z&gt;0\\) are allowed.\n\n        Special case \\(h=0, \\alpha=\\frac{1}{2}\\) (by taking the limit) equals the Gamma\n        deviance \\(S(y, z) = 2(\\frac{y}{z} -\\log\\frac{y}{z} - 1)\\).\n\n    For the common domains, \\(S_{\\frac{1}{2}}^h\\) equals the\n    [Tweedie deviance](https://en.wikipedia.org/wiki/Tweedie_distribution) with the\n    following relation between the degree of homogeneity \\(h\\) and the Tweedie\n    power \\(p\\): \\(h = 2-p\\).\n\n    References\n    ----------\n    `[Gneiting2011]`\n\n    :   T. Gneiting.\n        \"Making and Evaluating Point Forecasts\". (2011)\n        [doi:10.1198/jasa.2011.r10138](https://doi.org/10.1198/jasa.2011.r10138)\n        [arxiv:0912.0902](https://arxiv.org/abs/0912.0902)\n\n    Examples\n    --------\n    &gt;&gt;&gt; hes = HomogeneousExpectileScore(degree=2, level=0.1)\n    &gt;&gt;&gt; hes(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])  # doctest: +SKIP\n    0.95\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self, degree: float = 2, level: float = 0.5) -&gt; None:\n        self.degree = degree\n        if level &lt;= 0 or level &gt;= 1:\n            msg = f\"Argument level must fulfil 0 &lt; level &lt; 1, got {level}.\"\n            raise ValueError(msg)\n        self.level = level\n\n    @property\n    def functional(self):\n        if self.level == 0.5:\n            return \"mean\"\n        else:\n            return \"expectile\"\n\n    def score_per_obs(\n        self,\n        y_obs: npt.ArrayLike,\n        y_pred: npt.ArrayLike,\n    ) -&gt; np.ndarray:\n        \"\"\"Score per observation.\n\n        Parameters\n        ----------\n        y_obs : array-like of shape (n_obs)\n            Observed values of the response variable.\n        y_pred : array-like of shape (n_obs)\n            Predicted values of the `functional` of interest, e.g. the conditional\n            expectation of the response, `E(Y|X)`.\n\n        Returns\n        -------\n        score_per_obs : ndarray\n            Values of the scoring function for each observation.\n        \"\"\"\n        y: np.ndarray\n        z: np.ndarray\n        y, z = validate_2_arrays(y_obs, y_pred)\n\n        if self.degree == 2:\n            # Fast path\n            score = np.square(z - y)\n        elif self.degree &gt; 1:\n            z_abs = np.abs(z)\n            score = 2 * (\n                (np.power(np.abs(y), self.degree) - np.power(z_abs, self.degree))\n                / (self.degree * (self.degree - 1))\n                - np.sign(z)\n                / (self.degree - 1)\n                * np.power(z_abs, self.degree - 1)\n                * (y - z)\n            )\n        elif self.degree == 1:\n            # Domain: y &gt;= 0 and z &gt; 0\n            if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is y_obs &gt;= 0 and \"\n                    \"y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n            score = 2 * (special.xlogy(y, y / z) - y + z)\n        elif self.degree == 0:\n            # Domain: y &gt; 0 and z &gt; 0.\n            if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt; 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n            y_z = y / z\n            score = 2 * (y_z - np.log(y_z) - 1)\n        else:  # self.degree &lt; 1\n            # Domain: y &gt;= 0 and z &gt; 0 for 0 &lt; self.degree &lt; 1\n            # Domain: y &gt; 0  and z &gt; 0 else\n            if self.degree &gt; 0:\n                if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                    msg = (\n                        f\"Valid domain for degree={self.degree} is \"\n                        \"y_obs &gt;= 0 and y_pred &gt; 0.\"\n                    )\n                    raise ValueError(msg)\n            elif not np.all((y &gt; 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt; 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n            # Note: We add 0.0 to be sure we have floating points. Integers are not\n            # allowerd to be raised to a negative power.\n            score = 2 * (\n                (np.power(y + 0.0, self.degree) - np.power(z + 0.0, self.degree))\n                / (self.degree * (self.degree - 1))\n                - 1 / (self.degree - 1) * np.power(z + 0.0, self.degree - 1) * (y - z)\n            )\n\n        if self.level == 0.5:\n            return score\n        else:\n            return 2 * np.abs(np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.HomogeneousExpectileScore.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.HomogeneousExpectileScore.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 2:\n        # Fast path\n        score = np.square(z - y)\n    elif self.degree &gt; 1:\n        z_abs = np.abs(z)\n        score = 2 * (\n            (np.power(np.abs(y), self.degree) - np.power(z_abs, self.degree))\n            / (self.degree * (self.degree - 1))\n            - np.sign(z)\n            / (self.degree - 1)\n            * np.power(z_abs, self.degree - 1)\n            * (y - z)\n        )\n    elif self.degree == 1:\n        # Domain: y &gt;= 0 and z &gt; 0\n        if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is y_obs &gt;= 0 and \"\n                \"y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = 2 * (special.xlogy(y, y / z) - y + z)\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        y_z = y / z\n        score = 2 * (y_z - np.log(y_z) - 1)\n    else:  # self.degree &lt; 1\n        # Domain: y &gt;= 0 and z &gt; 0 for 0 &lt; self.degree &lt; 1\n        # Domain: y &gt; 0  and z &gt; 0 else\n        if self.degree &gt; 0:\n            if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt;= 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n        elif not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        # Note: We add 0.0 to be sure we have floating points. Integers are not\n        # allowerd to be raised to a negative power.\n        score = 2 * (\n            (np.power(y + 0.0, self.degree) - np.power(z + 0.0, self.degree))\n            / (self.degree * (self.degree - 1))\n            - 1 / (self.degree - 1) * np.power(z + 0.0, self.degree - 1) * (y - z)\n        )\n\n    if self.level == 0.5:\n        return score\n    else:\n        return 2 * np.abs(np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.HomogeneousQuantileScore","title":"<code>HomogeneousQuantileScore</code>","text":"<p>Homogeneous scoring function of degree h for quantiles.</p> <p>The smaller the better, minimum is zero.</p> <p>Up to a multiplicative constant, these are the only scoring funtions that are strictly consistent for quantiles at level alpha and homogeneous functions. The possible additive constant is chosen such that the minimal function value equals zero.</p> <p>Note that the \u00bd-quantile (level alpha=0.5) equals the median.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>float</code> <p>Degree of homogeneity.</p> <code>2</code> <code>level</code> <code>float</code> <p>The level of the quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> gives the median.</p> <code>0.5</code> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"quantile\"</p> Notes <p>The homogeneous score of degree \\(h\\) is given by</p> \\[ S_\\alpha^h(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) \\frac{z^h - y^h}{h} \\] <p>There are important domain restrictions and limits:</p> <ul> <li> <p>\\(h\\) positive odd integer: All real numbers \\(y\\) and \\(z\\) are allowed.</p> <ul> <li>Special case \\(h=1\\) equals the pinball loss,   \\(S(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) (z - y)\\).</li> <li>Special case \\(h=1, \\alpha=\\frac{1}{2}\\) equals half the absolute error   \\(S(y, z) = \\frac{1}{2}|z - y|\\).</li> </ul> </li> <li> <p>\\(h\\) real valued: Only \\(y&gt;0\\), \\(z&gt;0\\) are allowed.</p> <p>Special case \\(h=0\\) (by taking the limit) equals \\(S(y, z) = |\\mathbf{1}\\{z \\ge y\\} - \\alpha| \\log\\frac{z}{y}\\).</p> </li> </ul> References <code>[Gneiting2011]</code> <p>T. Gneiting. \"Making and Evaluating Point Forecasts\". (2011) doi:10.1198/jasa.2011.r10138 arxiv:0912.0902</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hqs = HomogeneousQuantileScore(degree=3, level=0.1)\n&gt;&gt;&gt; hqs(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n0.6083333333333334\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class HomogeneousQuantileScore(_BaseScoringFunction):\n    r\"\"\"Homogeneous scoring function of degree h for quantiles.\n\n    The smaller the better, minimum is zero.\n\n    Up to a multiplicative constant, these are the only scoring funtions that are\n    strictly consistent for quantiles at level alpha and homogeneous functions.\n    The possible additive constant is chosen such that the minimal function value\n    equals zero.\n\n    Note that the 1/2-quantile (level alpha=0.5) equals the median.\n\n    Parameters\n    ----------\n    degree : float\n        Degree of homogeneity.\n    level : float\n        The level of the quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` gives the median.\n\n    Attributes\n    ----------\n    functional: str\n        \"quantile\"\n\n    Notes\n    -----\n    The homogeneous score of degree \\(h\\) is given by\n\n    \\[\n    S_\\alpha^h(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) \\frac{z^h - y^h}{h}\n    \\]\n\n    There are important domain restrictions and limits:\n\n    - \\(h\\) positive odd integer: All real numbers \\(y\\) and \\(z\\) are allowed.\n\n        - Special case \\(h=1\\) equals the pinball loss,\n          \\(S(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) (z - y)\\).\n        - Special case \\(h=1, \\alpha=\\frac{1}{2}\\) equals half the absolute error\n          \\(S(y, z) = \\frac{1}{2}|z - y|\\).\n\n    - \\(h\\) real valued: Only \\(y&gt;0\\), \\(z&gt;0\\) are allowed.\n\n        Special case \\(h=0\\) (by taking the limit) equals\n        \\(S(y, z) = |\\mathbf{1}\\{z \\ge y\\} - \\alpha| \\log\\frac{z}{y}\\).\n\n    References\n    ----------\n    `[Gneiting2011]`\n\n    :   T. Gneiting.\n        \"Making and Evaluating Point Forecasts\". (2011)\n        [doi:10.1198/jasa.2011.r10138](https://doi.org/10.1198/jasa.2011.r10138)\n        [arxiv:0912.0902](https://arxiv.org/abs/0912.0902)\n\n    Examples\n    --------\n    &gt;&gt;&gt; hqs = HomogeneousQuantileScore(degree=3, level=0.1)\n    &gt;&gt;&gt; hqs(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])  # doctest: +SKIP\n    0.6083333333333334\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self, degree: float = 2, level: float = 0.5) -&gt; None:\n        self.degree = degree\n        if level &lt;= 0 or level &gt;= 1:\n            msg = f\"Argument level must fulfil 0 &lt; level &lt; 1, got {level}.\"\n            raise ValueError(msg)\n        self.level = level\n\n    @property\n    def functional(self):\n        return \"quantile\"\n\n    def score_per_obs(\n        self,\n        y_obs: npt.ArrayLike,\n        y_pred: npt.ArrayLike,\n    ) -&gt; np.ndarray:\n        \"\"\"Score per observation.\n\n        Parameters\n        ----------\n        y_obs : array-like of shape (n_obs)\n            Observed values of the response variable.\n        y_pred : array-like of shape (n_obs)\n            Predicted values of the `functional` of interest, e.g. the conditional\n            expectation of the response, `E(Y|X)`.\n\n        Returns\n        -------\n        score_per_obs : ndarray\n            Values of the scoring function for each observation.\n        \"\"\"\n        y: np.ndarray\n        z: np.ndarray\n        y, z = validate_2_arrays(y_obs, y_pred)\n\n        if self.degree == 1:\n            # Fast path\n            score = z - y\n        elif self.degree &gt; 1 and self.degree % 2 == 1:\n            # Odd positive degree\n            score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n        elif self.degree == 0:\n            # Domain: y &gt; 0 and z &gt; 0.\n            if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt; 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n            score = np.log(z / y)\n        else:\n            # Domain: y &gt; 0 and z &gt; 0.\n            if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt; 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n            score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n\n        if self.level == 0.5:\n            return 0.5 * np.abs(score)\n        else:\n            return (np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.HomogeneousQuantileScore.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.HomogeneousQuantileScore.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 1:\n        # Fast path\n        score = z - y\n    elif self.degree &gt; 1 and self.degree % 2 == 1:\n        # Odd positive degree\n        score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = np.log(z / y)\n    else:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n\n    if self.level == 0.5:\n        return 0.5 * np.abs(score)\n    else:\n        return (np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.LogLoss","title":"<code>LogLoss</code>","text":"<p>Log loss.</p> <p>The smaller the better, minimum is zero.</p> <p>The log loss is a strictly consistent scoring function for the mean for observations and predictions in the range 0 to 1. It is also referred to as (half the) Bernoulli deviance, (half the) Binomial log-likelihood, logistic loss and binary cross-entropy. Its minimal function value is zero.</p> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"mean\"</p> Notes <p>The log loss for \\(y,z \\in [0,1]\\) is given by</p> \\[ S(y, z) = - y \\log\\frac{z}{y} - (1 - y) \\log\\frac{1-z}{1-y} \\] <p>If one restricts to \\(y\\in \\{0, 1\\}\\), this simplifies to</p> \\[ S(y, z) = - y \\log(z) - (1 - y) \\log(1-z) \\] <p>Examples:</p> <pre><code>&gt;&gt;&gt; ll = LogLoss()\n&gt;&gt;&gt; ll(y_obs=[0, 0.5, 1, 1], y_pred=[0.1, 0.2, 0.8 , 0.9], weights=[1, 2, 1, 1])\n0.17603033705165635\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class LogLoss(_BaseScoringFunction):\n    r\"\"\"Log loss.\n\n    The smaller the better, minimum is zero.\n\n    The log loss is a strictly consistent scoring function for the mean for\n    observations and predictions in the range 0 to 1.\n    It is also referred to as (half the) Bernoulli deviance,\n    (half the) Binomial log-likelihood, logistic loss and binary cross-entropy.\n    Its minimal function value is zero.\n\n    Attributes\n    ----------\n    functional: str\n        \"mean\"\n\n    Notes\n    -----\n    The log loss for \\(y,z \\in [0,1]\\) is given by\n\n    \\[\n    S(y, z) = - y \\log\\frac{z}{y} - (1 - y) \\log\\frac{1-z}{1-y}\n    \\]\n\n    If one restricts to \\(y\\in \\{0, 1\\}\\), this simplifies to\n\n    \\[\n    S(y, z) = - y \\log(z) - (1 - y) \\log(1-z)\n    \\]\n\n    Examples\n    --------\n    &gt;&gt;&gt; ll = LogLoss()\n    &gt;&gt;&gt; ll(y_obs=[0, 0.5, 1, 1], y_pred=[0.1, 0.2, 0.8 , 0.9], weights=[1, 2, 1, 1])  # doctest: +SKIP\n    0.17603033705165635\n    \"\"\"  # noqa: E501\n\n    # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    @property\n    def functional(self):\n        return \"mean\"\n\n    def score_per_obs(\n        self,\n        y_obs: npt.ArrayLike,\n        y_pred: npt.ArrayLike,\n    ) -&gt; np.ndarray:\n        \"\"\"Score per observation.\n\n        Parameters\n        ----------\n        y_obs : array-like of shape (n_obs)\n            Observed values of the response variable.\n        y_pred : array-like of shape (n_obs)\n            Predicted values of the `functional` of interest, e.g. the conditional\n            expectation of the response, `E(Y|X)`.\n\n        Returns\n        -------\n        score_per_obs : ndarray\n            Values of the scoring function for each observation.\n        \"\"\"\n        y: np.ndarray\n        z: np.ndarray\n        y, z = validate_2_arrays(y_obs, y_pred)\n\n        score = -special.xlogy(y, z) - special.xlogy(1 - y, 1 - z)\n        if np.any((y &gt; 0) &amp; (y &lt; 1)):\n            score += special.xlogy(y, y) + special.xlogy(1 - y, 1 - y)\n        return score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.LogLoss.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.LogLoss.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    score = -special.xlogy(y, z) - special.xlogy(1 - y, 1 - z)\n    if np.any((y &gt; 0) &amp; (y &lt; 1)):\n        score += special.xlogy(y, y) + special.xlogy(1 - y, 1 - y)\n    return score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.PinballLoss","title":"<code>PinballLoss</code>","text":"<p>Pinball loss.</p> <p>The smaller the better, minimum is zero.</p> <p>The pinball loss is strictly consistent for quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>float</code> <p>The level of the quantile. (Often called \\(\\alpha\\).) It must be <code>0 &lt; level &lt; 1</code>. <code>level=0.5</code> gives the median.</p> <code>0.5</code> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"quantile\"</p> Notes <p>The pinball loss has degree of homogeneity 1 and is given by</p> \\[ S_\\alpha(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) (z - y) \\] <p>The authors do not know where and when the term pinball loss was coined. It is most famously used in quantile regression.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pl = PinballLoss(level=0.9)\n&gt;&gt;&gt; pl(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n0.275\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class PinballLoss(HomogeneousQuantileScore):\n    r\"\"\"Pinball loss.\n\n    The smaller the better, minimum is zero.\n\n    The pinball loss is strictly consistent for quantiles.\n\n    Parameters\n    ----------\n    level : float\n        The level of the quantile. (Often called \\(\\alpha\\).)\n        It must be `0 &lt; level &lt; 1`.\n        `level=0.5` gives the median.\n\n    Attributes\n    ----------\n    functional: str\n        \"quantile\"\n\n    Notes\n    -----\n    The pinball loss has degree of homogeneity 1 and is given by\n\n    \\[\n    S_\\alpha(y, z) = (\\mathbf{1}\\{z \\ge y\\} - \\alpha) (z - y)\n    \\]\n\n    The authors do not know where and when the term *pinball loss* was coined. It is\n    most famously used in quantile regression.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pl = PinballLoss(level=0.9)\n    &gt;&gt;&gt; pl(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])  # doctest: +SKIP\n    0.275\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self, level: float = 0.5) -&gt; None:\n        super().__init__(degree=1, level=level)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.PinballLoss.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.PinballLoss.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 1:\n        # Fast path\n        score = z - y\n    elif self.degree &gt; 1 and self.degree % 2 == 1:\n        # Odd positive degree\n        score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = np.log(z / y)\n    else:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = (np.power(z, self.degree) - np.power(y, self.degree)) / self.degree\n\n    if self.level == 0.5:\n        return 0.5 * np.abs(score)\n    else:\n        return (np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.PoissonDeviance","title":"<code>PoissonDeviance</code>","text":"<p>Poisson deviance.</p> <p>The smaller the better, minimum is zero.</p> <p>The Poisson deviance is strictly consistent for the mean. It has a degree of homogeneity of 1.</p> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"mean\"</p> Notes <p>\\(S(y, z) = 2(y\\log\\frac{y}{z} - y + z)\\)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pd = PoissonDeviance()\n&gt;&gt;&gt; pd(y_obs=[0, 0, 1, 1], y_pred=[2, 1, 1 , 2])\n1.6534264097200273\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class PoissonDeviance(HomogeneousExpectileScore):\n    r\"\"\"Poisson deviance.\n\n    The smaller the better, minimum is zero.\n\n    The Poisson deviance is strictly consistent for the mean.\n    It has a degree of homogeneity of 1.\n\n    Attributes\n    ----------\n    functional: str\n        \"mean\"\n\n    Notes\n    -----\n    \\(S(y, z) = 2(y\\log\\frac{y}{z} - y + z)\\)\n\n    Examples\n    --------\n    &gt;&gt;&gt; pd = PoissonDeviance()\n    &gt;&gt;&gt; pd(y_obs=[0, 0, 1, 1], y_pred=[2, 1, 1 , 2])  # doctest: +SKIP\n    1.6534264097200273\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self) -&gt; None:\n        super().__init__(degree=1, level=0.5)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.PoissonDeviance.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.PoissonDeviance.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 2:\n        # Fast path\n        score = np.square(z - y)\n    elif self.degree &gt; 1:\n        z_abs = np.abs(z)\n        score = 2 * (\n            (np.power(np.abs(y), self.degree) - np.power(z_abs, self.degree))\n            / (self.degree * (self.degree - 1))\n            - np.sign(z)\n            / (self.degree - 1)\n            * np.power(z_abs, self.degree - 1)\n            * (y - z)\n        )\n    elif self.degree == 1:\n        # Domain: y &gt;= 0 and z &gt; 0\n        if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is y_obs &gt;= 0 and \"\n                \"y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = 2 * (special.xlogy(y, y / z) - y + z)\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        y_z = y / z\n        score = 2 * (y_z - np.log(y_z) - 1)\n    else:  # self.degree &lt; 1\n        # Domain: y &gt;= 0 and z &gt; 0 for 0 &lt; self.degree &lt; 1\n        # Domain: y &gt; 0  and z &gt; 0 else\n        if self.degree &gt; 0:\n            if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt;= 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n        elif not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        # Note: We add 0.0 to be sure we have floating points. Integers are not\n        # allowerd to be raised to a negative power.\n        score = 2 * (\n            (np.power(y + 0.0, self.degree) - np.power(z + 0.0, self.degree))\n            / (self.degree * (self.degree - 1))\n            - 1 / (self.degree - 1) * np.power(z + 0.0, self.degree - 1) * (y - z)\n        )\n\n    if self.level == 0.5:\n        return score\n    else:\n        return 2 * np.abs(np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.SquaredError","title":"<code>SquaredError</code>","text":"<p>Squared error.</p> <p>The smaller the better, minimum is zero.</p> <p>The squared error is strictly consistent for the mean. It has a degree of homogeneity of 2. In the context of probabilistic classification, it is also known as Brier score.</p> <p>Attributes:</p> Name Type Description <code>functional</code> <code>str</code> <p>\"mean\"</p> Notes <p>\\(S(y, z) = (y - z)^2\\)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; se = SquaredError()\n&gt;&gt;&gt; se(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])\n0.75\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>class SquaredError(HomogeneousExpectileScore):\n    r\"\"\"Squared error.\n\n    The smaller the better, minimum is zero.\n\n    The squared error is strictly consistent for the mean.\n    It has a degree of homogeneity of 2.\n    In the context of probabilistic classification, it is also known as Brier score.\n\n\n    Attributes\n    ----------\n    functional: str\n        \"mean\"\n\n    Notes\n    -----\n    \\(S(y, z) = (y - z)^2\\)\n\n    Examples\n    --------\n    &gt;&gt;&gt; se = SquaredError()\n    &gt;&gt;&gt; se(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1 , 2])  # doctest: +SKIP\n    0.75\n    \"\"\"  # FIXME: numpy 2.0.0, doctest skip should not be necessary.\n\n    def __init__(self) -&gt; None:\n        super().__init__(degree=2, level=0.5)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.SquaredError.__call__","title":"<code>__call__(y_obs, y_pred, weights=None)</code>","text":"<p>Mean or average score.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The average score.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def __call__(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n) -&gt; np.floating[Any]:\n    \"\"\"Mean or average score.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n\n    Returns\n    -------\n    score : float\n        The average score.\n    \"\"\"\n    return np.average(self.score_per_obs(y_obs, y_pred), weights=weights)\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.SquaredError.score_per_obs","title":"<code>score_per_obs(y_obs, y_pred)</code>","text":"<p>Score per observation.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <p>Returns:</p> Name Type Description <code>score_per_obs</code> <code>ndarray</code> <p>Values of the scoring function for each observation.</p> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def score_per_obs(\n    self,\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n) -&gt; np.ndarray:\n    \"\"\"Score per observation.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n\n    Returns\n    -------\n    score_per_obs : ndarray\n        Values of the scoring function for each observation.\n    \"\"\"\n    y: np.ndarray\n    z: np.ndarray\n    y, z = validate_2_arrays(y_obs, y_pred)\n\n    if self.degree == 2:\n        # Fast path\n        score = np.square(z - y)\n    elif self.degree &gt; 1:\n        z_abs = np.abs(z)\n        score = 2 * (\n            (np.power(np.abs(y), self.degree) - np.power(z_abs, self.degree))\n            / (self.degree * (self.degree - 1))\n            - np.sign(z)\n            / (self.degree - 1)\n            * np.power(z_abs, self.degree - 1)\n            * (y - z)\n        )\n    elif self.degree == 1:\n        # Domain: y &gt;= 0 and z &gt; 0\n        if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is y_obs &gt;= 0 and \"\n                \"y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        score = 2 * (special.xlogy(y, y / z) - y + z)\n    elif self.degree == 0:\n        # Domain: y &gt; 0 and z &gt; 0.\n        if not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        y_z = y / z\n        score = 2 * (y_z - np.log(y_z) - 1)\n    else:  # self.degree &lt; 1\n        # Domain: y &gt;= 0 and z &gt; 0 for 0 &lt; self.degree &lt; 1\n        # Domain: y &gt; 0  and z &gt; 0 else\n        if self.degree &gt; 0:\n            if not np.all((y &gt;= 0) &amp; (z &gt; 0)):\n                msg = (\n                    f\"Valid domain for degree={self.degree} is \"\n                    \"y_obs &gt;= 0 and y_pred &gt; 0.\"\n                )\n                raise ValueError(msg)\n        elif not np.all((y &gt; 0) &amp; (z &gt; 0)):\n            msg = (\n                f\"Valid domain for degree={self.degree} is \"\n                \"y_obs &gt; 0 and y_pred &gt; 0.\"\n            )\n            raise ValueError(msg)\n        # Note: We add 0.0 to be sure we have floating points. Integers are not\n        # allowerd to be raised to a negative power.\n        score = 2 * (\n            (np.power(y + 0.0, self.degree) - np.power(z + 0.0, self.degree))\n            / (self.degree * (self.degree - 1))\n            - 1 / (self.degree - 1) * np.power(z + 0.0, self.degree - 1) * (y - z)\n        )\n\n    if self.level == 0.5:\n        return score\n    else:\n        return 2 * np.abs(np.greater_equal(z, y) - self.level) * score\n</code></pre>"},{"location":"reference/model_diagnostics/scoring/scoring/#model_diagnostics.scoring.scoring.decompose","title":"<code>decompose(y_obs, y_pred, weights=None, *, scoring_function, functional=None, level=None)</code>","text":"<p>Additive decomposition of scores.</p> <p>The score is decomposed as <code>score = miscalibration - discrimination + uncertainty</code>.</p> <p>Parameters:</p> Name Type Description Default <code>y_obs</code> <code>array-like of shape (n_obs)</code> <p>Observed values of the response variable.</p> required <code>y_pred</code> <code>array-like of shape (n_obs) or (n_obs, n_models)</code> <p>Predicted values of the <code>functional</code> of interest, e.g. the conditional expectation of the response, <code>E(Y|X)</code>.</p> required <code>weights</code> <code>array-like of shape (n_obs) or None</code> <p>Case weights.</p> <code>None</code> <code>scoring_function</code> <code>callable</code> <p>A scoring function with signature roughly <code>fun(y_obs, y_pred, weights) -&gt; float</code>.</p> required <code>functional</code> <code>str or None</code> <p>The target functional which <code>y_pred</code> aims to predict. If <code>None</code>, then it will be inferred from <code>scoring_function.functional</code>. Options are:</p> <ul> <li><code>\"mean\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"median\"</code>. Argument <code>level</code> is neglected.</li> <li><code>\"expectile\"</code></li> <li><code>\"quantile\"</code></li> </ul> <code>None</code> <code>level</code> <code>float or None</code> <p>Functionals like expectiles and quantiles have a level (often called alpha). If <code>None</code>, then it will be inferred from <code>scoring_function.level</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>decomposition</code> <code>DataFrame</code> <p>The resulting score decomposition as a dataframe with columns:</p> <ul> <li><code>miscalibration</code></li> <li><code>discrimination</code></li> <li><code>uncertainty</code></li> <li><code>score</code>: the average score</li> </ul> <code>If `y_pred` contains several predictions, i.e. it is 2-dimension with shape</code> <code>`(n_obs, n_pred)` and `n_pred &gt;1`, then there is the additional column:</code> <ul> <li><code>model</code></li> </ul> Notes <p>To be precise, this function returns the decomposition of the score in terms of auto-miscalibration, auto-discrimination (or resolution) and uncertainy (or entropy), see <code>[FLM2022]</code> and references therein. The key element is to estimate the recalibrated predictions, i.e. \\(T(Y|m(X))\\) for the target functional \\(T\\) and model predictions \\(m(X)\\). This is accomplished by isotonic regression, <code>[Dimitriadis2021]</code> and <code>[Gneiting2021]</code>.</p> References <code>[FLM2022]</code> <p>T. Fissler, C. Lorentzen, and M. Mayer. \"Model Comparison and Calibration Assessment\". (2022) arxiv:2202.12780.</p> <code>[Dimitriadis2021]</code> <p>T. Dimitriadis, T. Gneiting, and A. I. Jordan. \"Stable reliability diagrams for probabilistic classifiers\". (2021) doi:10.1073/pnas.2016191118</p> <code>[Gneiting2021]</code> <p>T. Gneiting and J. Resin. \"Regression Diagnostics meets Forecast Evaluation: Conditional Calibration, Reliability Diagrams, and Coefficient of Determination\". (2021). arXiv:2108.03210.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; decompose(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1, 2],\n... scoring_function=SquaredError())\nshape: (1, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 miscalibration \u2506 discrimination \u2506 uncertainty \u2506 score \u2502\n\u2502 ---            \u2506 ---            \u2506 ---         \u2506 ---   \u2502\n\u2502 f64            \u2506 f64            \u2506 f64         \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.625          \u2506 0.125          \u2506 0.25        \u2506 0.75  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>src/model_diagnostics/scoring/scoring.py</code> <pre><code>def decompose(\n    y_obs: npt.ArrayLike,\n    y_pred: npt.ArrayLike,\n    weights: Optional[npt.ArrayLike] = None,\n    *,\n    scoring_function: Callable[..., Any],  # TODO: make type hint stricter\n    functional: Optional[str] = None,\n    level: Optional[float] = None,\n) -&gt; pl.DataFrame:\n    r\"\"\"Additive decomposition of scores.\n\n    The score is decomposed as\n    `score = miscalibration - discrimination + uncertainty`.\n\n    Parameters\n    ----------\n    y_obs : array-like of shape (n_obs)\n        Observed values of the response variable.\n    y_pred : array-like of shape (n_obs) or (n_obs, n_models)\n        Predicted values of the `functional` of interest, e.g. the conditional\n        expectation of the response, `E(Y|X)`.\n    weights : array-like of shape (n_obs) or None\n        Case weights.\n    scoring_function : callable\n        A scoring function with signature roughly\n        `fun(y_obs, y_pred, weights) -&gt; float`.\n    functional : str or None\n        The target functional which `y_pred` aims to predict.\n        If `None`, then it will be inferred from `scoring_function.functional`.\n        Options are:\n\n        - `\"mean\"`. Argument `level` is neglected.\n        - `\"median\"`. Argument `level` is neglected.\n        - `\"expectile\"`\n        - `\"quantile\"`\n    level : float or None\n        Functionals like expectiles and quantiles have a level (often called alpha).\n        If `None`, then it will be inferred from `scoring_function.level`.\n\n    Returns\n    -------\n    decomposition : polars.DataFrame\n        The resulting score decomposition as a dataframe with columns:\n\n        - `miscalibration`\n        - `discrimination`\n        - `uncertainty`\n        - `score`: the average score\n\n    If `y_pred` contains several predictions, i.e. it is 2-dimension with shape\n    `(n_obs, n_pred)` and `n_pred &gt;1`, then there is the additional column:\n\n        - `model`\n\n    Notes\n    -----\n    To be precise, this function returns the decomposition of the score in terms of\n    auto-miscalibration, auto-discrimination (or resolution) and uncertainy (or\n    entropy), see `[FLM2022]` and references therein.\n    The key element is to estimate the recalibrated predictions, i.e. \\(T(Y|m(X))\\) for\n    the target functional \\(T\\) and model predictions \\(m(X)\\).\n    This is accomplished by isotonic regression, `[Dimitriadis2021]` and\n    `[Gneiting2021]`.\n\n    References\n    ----------\n    `[FLM2022]`\n\n    :   T. Fissler, C. Lorentzen, and M. Mayer.\n        \"Model Comparison and Calibration Assessment\". (2022)\n        [arxiv:2202.12780](https://arxiv.org/abs/2202.12780).\n\n    `[Dimitriadis2021]`\n\n    :   T. Dimitriadis, T. Gneiting, and A. I. Jordan.\n        \"Stable reliability diagrams for probabilistic classifiers\". (2021)\n        [doi:10.1073/pnas.2016191118](https://doi.org/10.1073/pnas.2016191118)\n\n    `[Gneiting2021]`\n\n    :   T. Gneiting and J. Resin.\n        \"Regression Diagnostics meets Forecast Evaluation: Conditional Calibration,\n        Reliability Diagrams, and Coefficient of Determination\". (2021).\n        [arXiv:2108.03210](https://arxiv.org/abs/2108.03210).\n\n    Examples\n    --------\n    &gt;&gt;&gt; decompose(y_obs=[0, 0, 1, 1], y_pred=[-1, 1, 1, 2],\n    ... scoring_function=SquaredError())\n    shape: (1, 4)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 miscalibration \u2506 discrimination \u2506 uncertainty \u2506 score \u2502\n    \u2502 ---            \u2506 ---            \u2506 ---         \u2506 ---   \u2502\n    \u2502 f64            \u2506 f64            \u2506 f64         \u2506 f64   \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 0.625          \u2506 0.125          \u2506 0.25        \u2506 0.75  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    if functional is None:\n        if hasattr(scoring_function, \"functional\"):\n            functional = scoring_function.functional\n        else:\n            msg = (\n                \"You set functional=None, but scoring_function has no attribute \"\n                \"functional.\"\n            )\n            raise ValueError(msg)\n    if level is None:\n        level = 0.5\n        if functional in (\"expectile\", \"quantile\"):\n            if hasattr(scoring_function, \"level\"):\n                level = float(scoring_function.level)\n            else:\n                msg = (\n                    \"You set level=None, but scoring_function has no attribute \"\n                    \"level.\"\n                )\n                raise ValueError(msg)\n\n    allowed_functionals = (\"mean\", \"median\", \"expectile\", \"quantile\")\n    if functional not in allowed_functionals:\n        msg = (\n            f\"The functional must be one of {allowed_functionals}, got \"\n            f\"{functional}.\"\n        )\n        raise ValueError(msg)\n    if functional in (\"expectile\", \"quantile\") and (level &lt;= 0 or level &gt;= 1):\n        msg = f\"The level must fulfil 0 &lt; level &lt; 1, got {level}.\"\n        raise ValueError(msg)\n\n    validate_same_first_dimension(y_obs, y_pred)\n    n_pred = length_of_second_dimension(y_pred)\n    pred_names, _ = get_sorted_array_names(y_pred)\n    y_o = np.asarray(y_obs)\n\n    if weights is None:\n        w = None\n    else:\n        validate_same_first_dimension(weights, y_o)\n        w = np.asarray(weights)  # needed to satisfy mypy\n        if w.ndim &gt; 1:\n            msg = f\"The array weights must be 1-dimensional, got weights.ndim={w.ndim}.\"\n            raise ValueError(msg)\n\n    if functional == \"mean\":\n        iso = IsotonicRegression_skl(y_min=None, y_max=None)\n        marginal = np.average(y_o, weights=w)\n    else:\n        iso = IsotonicRegression(functional=functional, level=level)\n        if functional == \"expectile\":\n            marginal = expectile(y_o, alpha=level, weights=w)\n        elif functional == \"quantile\":\n            marginal = 0.5 * (\n                quantile_lower(y_o, level=level) + quantile_upper(y_o, level=level)\n            )\n\n    if y_o[0] == marginal == y_o[-1]:\n        # y_o is constant. We need to check if y_o is allowed as argument to y_pred.\n        # For instance for the poisson deviance, y_o = 0 is allowed. But 0 is forbidden\n        # as a prediction.\n        try:\n            scoring_function(y_o[0], marginal)\n        except ValueError as exc:\n            msg = (\n                \"Your y_obs is constant and lies outside the allowed range of y_pred \"\n                \"of your scoring function. Therefore, the score decomposition cannot \"\n                \"be applied.\"\n            )\n            raise ValueError(msg) from exc\n\n    # The recalibrated versions, further down, could contain min(y_obs) and that could\n    # be outside of the valid domain, e.g. y_pred = 0 for the Poisson deviance where\n    # y_obs=0 is allowed. We detect that here:\n    y_min = np.amin(y_o)\n    y_min_allowed = True\n    try:\n        scoring_function(y_o[:1], np.array([y_min]), None if w is None else w[:1])\n    except ValueError:\n        y_min_allowed = False\n\n    marginal = np.full_like(y_o, fill_value=marginal, dtype=float)\n    score_marginal = scoring_function(y_o, marginal, w)\n\n    df_list = []\n    for i in range(len(pred_names)):\n        # Loop over columns of y_pred.\n        x = y_pred if n_pred == 0 else get_second_dimension(y_pred, i)\n        iso.fit(x, y_o, sample_weight=w)\n        recalibrated = np.squeeze(iso.predict(x))\n        if not y_min_allowed and recalibrated[0] &lt;= y_min:\n            # Oh dear, this needs quite some extra work:\n            # First index of value greater than y_min\n            idx1 = np.argmax(recalibrated &gt; y_min)\n            val1 = recalibrated[idx1]\n            # First index of value greater than the value at idx1.\n            idx2 = np.argmax(recalibrated &gt; val1)\n            # Note that val1 may already be the largest value of the array =&gt; idx2 = 0.\n            if idx2 == 0:\n                idx2 = recalibrated.shape[0]\n            # We merge the first 2 blocks of the isotonic regression as it violates\n            # our domain requirements.\n            re2 = recalibrated[:idx2]\n            w2 = None if w is None else w[:idx2]\n            if functional == \"mean\":\n                recalibrated[:idx2] = np.average(re2, weights=w2)\n            elif functional == \"expectile\":\n                recalibrated[:idx2] = expectile(re2, alpha=level, weights=w2)\n            elif functional == \"quantile\":\n                # Note, no scoring function known that could end up here.\n                lower = quantile_lower(re2, level=level)\n                upper = quantile_upper(re2, level=level)\n                recalibrated[:idx2] = 0.5 * (lower + upper)\n\n        score = scoring_function(y_o, x, w)\n        try:\n            score_recalibrated = scoring_function(y_o, recalibrated, w)\n        except ValueError as exc:\n            msg = (\n                \"The recalibrated predictions obtained from isotonic regression are \"\n                \"very likely outside the allowed range of y_pred of your scoring \"\n                \"function. Therefore, the score decomposition cannot be applied.\"\n            )\n            raise ValueError(msg) from exc\n\n        df = pl.DataFrame(\n            {\n                \"model\": pred_names[i],\n                \"miscalibration\": score - score_recalibrated,\n                \"discrimination\": score_marginal - score_recalibrated,\n                \"uncertainty\": score_marginal,\n                \"score\": score,\n            }\n        )\n        df_list.append(df)\n\n    df = pl.concat(df_list)\n\n    # Remove column \"model\" for a single model.\n    if n_pred &lt;= 1:\n        df = df.drop(\"model\")\n\n    return df\n</code></pre>"}]}